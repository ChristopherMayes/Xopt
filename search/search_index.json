{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#xopt","title":"Xopt","text":"<p>Flexible optimization of arbitrary problems in Python.</p> <p>The goal of this package is to provide advanced algorithmic support for arbitrary  optimization problems (simulations/control systems) with minimal required coding. Users  can easily connect  arbitrary evaluation functions to advanced algorithms with minimal coding with  support for multi-threaded or MPI-enabled execution.</p> <p>Currenty Xopt provides:</p> <ul> <li>Optimization algorithms:</li> <li>Genetic algorithms<ul> <li><code>cnsga</code> Continuous NSGA-II with constraints</li> </ul> </li> <li>Bayesian optimization (BO) algorithms:<ul> <li><code>upper_confidence_bound</code> BO using Upper Confidence Bound acquisition function    (w/ or w/o constraints, serial or parallel)</li> <li><code>expected_improvement</code> BO using Expected Improvement acquisition function    (w/ or w/o constraints, serial or parallel)</li> <li><code>mobo</code> Multi-objective BO (w/ or w/o constraints, serial or parallel)</li> <li><code>bayesian_exploration</code> Autonomous function characterization using Bayesian    Exploration</li> <li><code>mggpo</code> Parallelized hybrid Multi-Generation Multi-Objective Bayesian    optimization</li> <li><code>multi_fidelity</code> Multi-fidelity single or multi objective optimization</li> <li><code>BAX</code> Bayesian algorithm execution using virtual measurements</li> <li>BO customization:</li> <li>Trust region BO</li> <li>Heteroskedastic noise specification</li> <li>Multiple acquisition function optimization stratigies</li> </ul> </li> <li><code>extremum_seeking</code> Extremum seeking time-dependent optimization</li> <li><code>rcds</code> Robust Conjugate Direction Search (RCDS)</li> <li><code>neldermead</code> Nelder-Mead Simplex</li> <li>Sampling algorithms:</li> <li><code>random</code> Uniform random sampling</li> <li>Convenient YAML/JSON based input format</li> <li>Driver programs:</li> <li><code>xopt.mpi.run</code> Parallel MPI execution using this input format</li> </ul> <p>Xopt does not provide:  - your custom simulation via an <code>evaluate</code> function.</p> <p>Rather, Xopt asks you to define this function.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Xopt Overview PDF gives an overview of Xopt's design and  usage.</p> <p>Xopt Built-In Generators provides a list of available algorithms  implemented in the Xopt <code>Generator</code> framework.</p> <p>Simple Bayesian Optimization Example shows  Xopt usage for a simple optimization problem.</p> <p>Xopt IPAC23 paper summarizes the usage of Xopt in particle accelerator physics problems. </p>"},{"location":"#configuring-an-xopt-run","title":"Configuring an Xopt run","text":"<p>Xopt runs can be specified via a YAML file or dictonary input. This requires <code>generator</code>, <code>evaluator</code>, and <code>vocs</code> to be specified, along with optional general options such as <code>max_evaluations</code>. An example to run a multi-objective optimiation of a user-defined function <code>my_function</code> is: <pre><code>generator:\n    name: cnsga\n    population_size: 64\n    population_file: test.csv\n    output_path: .\n\nevaluator:\n    function: my_function\n    function_kwargs:\n      my_arguments: 42\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: \n        y1: MINIMIZE\n        y2: MINIMIZE\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    constants: {a: dummy_constant}\n\nmax_evaluations: 6400    \n</code></pre></p> <p>Xopt can also be used through a simple Python interface. <pre><code>import math\n\nfrom xopt.vocs import VOCS\nfrom xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt import Xopt\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n\n# define the function to optimize\ndef sin_function(input_dict):\n    return {\"f\": math.sin(input_dict[\"x\"])}\n\n# create Xopt evaluator, generator, and Xopt objects\nevaluator = Evaluator(function=sin_function)\ngenerator = UpperConfidenceBoundGenerator(vocs=vocs)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n\n# call X.random_evaluate() to generate + evaluate 3 initial points\nX.random_evaluate(3)\n\n# run optimization for 10 steps\nfor i in range(10):\n    X.step()\n\n# view collected data\nprint(X.data)\n</code></pre></p>"},{"location":"#defining-an-evaluation-function","title":"Defining an evaluation function","text":"<p>Xopt can interface with arbitrary evaluate functions (defined in Python) with the  following form: <pre><code>def evaluate(inputs: dict) -&gt; dict:\n    \"\"\" your code here \"\"\"\n</code></pre> Evaluate functions must accept a dictionary object that at least has the keys  specified in <code>variables, constants</code> and returns a dictionary  containing at least the  keys contained in <code>objectives, constraints</code>. Extra dictionary keys are tracked and  used in the evaluate function but are not modified by xopt.</p>"},{"location":"#using-mpi","title":"Using MPI","text":"<p>Example MPI run, with <code>xopt.yaml</code> as the only user-defined file: <pre><code>mpirun -n 64 python -m mpi4py.futures -m xopt.mpi.run xopt.yaml\n</code></pre></p>"},{"location":"#citing-xopt","title":"Citing Xopt","text":"<p>If you use <code>Xopt</code> for your research, please consider adding the following  citation to your publications. <pre><code>R. Roussel., et al., \"Xopt: A simplified framework for optimization of accelerator problems using advanced algorithms\", \nin Proc. IPAC'23, Venezia.doi:https://doi.org/10.18429/JACoW-14th International Particle Accelerator Conference-THPL164\n</code></pre></p> <p>BibTex entry: <pre><code>@inproceedings{Xopt,\n    title        = {Xopt: A simplified framework for optimization of accelerator problems using advanced algorithms},\n    author       = {R. Roussel and A. Edelen and A. Bartnik and C. Mayes},\n    year         = 2023,\n    month        = {05},\n    booktitle    = {Proc. IPAC'23},\n    publisher    = {JACoW Publishing, Geneva, Switzerland},\n    series       = {IPAC'23 - 14th International Particle Accelerator Conference},\n    number       = 14,\n    pages        = {4796--4799},\n    doi          = {doi:10.18429/jacow-ipac2023-thpl164},\n    isbn         = {978-3-95450-231-8},\n    issn         = {2673-5490},\n    url          = {https://indico.jacow.org/event/41/contributions/2556},\n    paper        = {THPL164},\n    venue        = {Venezia},\n    language     = {english}\n}\n</code></pre></p> <p>Particular versions of Xopt can be cited from Zenodo</p>"},{"location":"algorithms/","title":"Pre-Configured Generators in Xopt","text":"<p>A number of algorithms are implemented in Xopt using the <code>Generator</code> class for  off-the-shelf usage.  Below is a  description of the different generators that are available in Xopt and their target  use cases.</p>"},{"location":"algorithms/#randomgenerator","title":"RandomGenerator","text":"<p>Generates random points in the input space according to <code>VOCS</code>.</p>"},{"location":"algorithms/#bayesian-generators","title":"Bayesian Generators","text":"<p>All of the generators here use Bayesian optimization (BO) type methods to solve single  objective, multi objective and characterization problems. Bayesian generators  incorperate unknown constrianing functions into optimization based on what is  specified in <code>VOCS</code></p> <ul> <li><code>ExpectedImprovementGenerator</code>: implements Expected Improvement single    objective BO. Automatically balances trade-offs between exploration and    exploitation and is thus useful for general purpose optimization. </li> <li><code>UpperConfidenceBoundGenerator</code>: implements Upper Confidence Bound single    objective BO. Requires a hyperparameter <code>beta</code> that explicitly sets the tradeoff    between exploration and exploitation. Default value of <code>beta=2</code> is a good    starting point. Increase $\\beta$ to prioritize exploration and decrease <code>beta</code> to    prioritize exploitation.</li> <li><code>BayesianExplorationGenerator</code>: implements the Bayesian Exploration algorithm    for function characterization. This algorithm selects observation points that    maximize model uncertainty, thus picking points that maximize the information gain    about the target function at each iteration. If the target function is found to be    more sensative to one parameter this generator will adjust sampling frequency to    adapt. Note: specifying <code>vocs.objective[1]</code>   to <code>MAXIMIZE</code> or <code>MINIMIZE</code> does not change the behavior of this generator.</li> <li><code>MOBOGenerator</code>: implements Multi-Objective BO using the    Expected Hypervolume Improvement (EHVI) acquisition function. This is an ideal    general purpose multi-objective optimizer when objective evaluations cannot be    massively parallelized (&lt; 10 parallel evaluations).</li> <li><code>MGGPOGenerator</code>: implements Multi-Generation Gaussian Process Optimization using    the    Expected Hypervolume Improvement (EHVI) acquisition function. This is an ideal    general purpose multi-objective optimizer when objective evaluations can be    massively parallelized (&gt; 10 parallel evaluations) .</li> <li><code>MultiFidelityGenerator</code>: implements Multi-Fidelity BO which can take    advantage of lower fidelity evaluations of objectives and constraints to reduce    the computational cost of solving single or multi-objective optimization problems    in sequential or small scale parallel (&lt; 10 parallel evaluations)    contexts. </li> </ul>"},{"location":"algorithms/#evolutionary-generators","title":"Evolutionary Generators","text":"<ul> <li><code>CNSGAGenerator</code>: implements Continuous Non-dominated Sorted Genetic Algorithm    which as a good general purpose evolutionary algorithm used for solving    multi-objective optimization problems where evaluating the objective is relatively    cheap and massively parallelizable (above 5-10 parallel evaluations).</li> </ul>"},{"location":"algorithms/#extremum-seeking-generators","title":"Extremum Seeking Generators","text":"<ul> <li><code>ExtremumSeekingGenerator</code>: implements the Extremum Seeking algorithm which is    ideal for solving optimization problems that are suceptable to drifts.</li> </ul>"},{"location":"algorithms/#scipy-generators","title":"Scipy Generators","text":"<p>These generators serve as wrappers for algorithms implemented in scipy. - <code>NelderMeadGenerator</code>: implements Nelder-Mead (simplex) optimization.</p>"},{"location":"algorithms/#rcds-generators","title":"RCDS Generators","text":"<ul> <li><code>RCDSGenerator</code>: implements the RCDS algorithm. RCDS could be applied in noisy   online optimization scenarios</li> </ul>"},{"location":"algorithms/#custom-generators","title":"Custom Generators","text":"<p>Any general algorithm can be implemented by subclassing the abstract <code>Generator</code> class and used in the Xopt framework. If you implement a generator for your use case please consider opening a pull request so that we can add it to Xopt!</p>"},{"location":"installation/","title":"Installing Xopt","text":"<p>Installing <code>xopt</code> from the <code>conda-forge</code> channel can be achieved by adding <code>conda-forge</code> to your channels with:</p> <pre><code>conda config --add channels conda-forge\n</code></pre> <p>Once the <code>conda-forge</code> channel has been enabled, <code>xopt</code> can be installed with:</p> <pre><code>conda install xopt\n</code></pre> <p>It is possible to list all of the versions of <code>xopt</code> available on your platform with:</p> <pre><code>conda search xopt --channel conda-forge\n</code></pre>"},{"location":"installation/#developers","title":"Developers","text":"<p>Clone this repository: <pre><code>git clone https://github.com/ChristopherMayes/Xopt.git\n</code></pre></p> <p>Create an environment <code>xopt-dev</code> with all the dependencies: <pre><code>conda env create -f environment.yml\n</code></pre></p> <p>Install as editable: <pre><code>conda activate xopt-dev\npip install --no-dependencies -e .\n</code></pre></p>"},{"location":"installation/#cori-nersc-setup","title":"Cori (NERSC) setup","text":"<p><pre><code>conda install -c conda-forge xopt\n</code></pre> Follow instructions to build mpi4py: https://docs.nersc.gov/programming/high-level-environments/python/ Note that there is a bug in Jupyterhub terminals. Type: <pre><code>module swap PrgEnv-gnu PrgEnv-gnu\n</code></pre> to get the C compiler activated. </p>"},{"location":"api/evaluator/","title":"Evaluator","text":"<p>             Bases: <code>XoptBaseModel</code></p> <p>Xopt Evaluator for handling the parallel execution of an evaluate function.</p>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator--parameters","title":"Parameters","text":"<p>function : Callable     Function to evaluate. function_kwargs : dict, default={}     Any kwargs to pass on to this function. max_workers : int, default=1     Maximum number of workers. executor : NormalExecutor     NormalExecutor or any instantiated Executor object vectorized : bool, default=False     If true,</p> Source code in <code>xopt/evaluator.py</code> <pre><code>class Evaluator(XoptBaseModel):\n    \"\"\"\n    Xopt Evaluator for handling the parallel execution of an evaluate function.\n\n    Parameters\n    ----------\n    function : Callable\n        Function to evaluate.\n    function_kwargs : dict, default={}\n        Any kwargs to pass on to this function.\n    max_workers : int, default=1\n        Maximum number of workers.\n    executor : NormalExecutor\n        NormalExecutor or any instantiated Executor object\n    vectorized : bool, default=False\n        If true,\n    \"\"\"\n\n    function: Callable\n    max_workers: int = Field(1, ge=1)\n    executor: NormalExecutor = Field(exclude=True)  # Do not serialize\n    function_kwargs: dict = Field({})\n    vectorized: bool = Field(False)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"before\")\n    def validate_all(cls, values):\n        f = get_function(values[\"function\"])\n        kwargs = values.get(\"function_kwargs\", {})\n        kwargs = {**get_function_defaults(f), **kwargs}\n        values[\"function\"] = f\n        values[\"function_kwargs\"] = kwargs\n\n        max_workers = values.pop(\"max_workers\", 1)\n\n        executor = values.pop(\"executor\", None)\n        if not executor:\n            if max_workers &gt; 1:\n                executor = ProcessPoolExecutor(max_workers=max_workers)\n            else:\n                executor = DummyExecutor()\n\n        # Cast as a NormalExecutor\n        values[\"executor\"] = NormalExecutor[type(executor)](executor=executor)\n        values[\"max_workers\"] = max_workers\n\n        return values\n\n    def evaluate(self, input: Dict, **kwargs):\n        \"\"\"\n        Evaluate a single input dict using Evaluator.function with\n        Evaluator.function_kwargs.\n\n        Further kwargs are passed to the function.\n\n        Inputs:\n            inputs: dict of inputs to be evaluated\n            **kwargs: additional kwargs to pass to the function\n\n        Returns:\n            function(input, **function_kwargs_updated)\n\n        \"\"\"\n        return self.safe_function(input, **{**self.function_kwargs, **kwargs})\n\n    def evaluate_data(\n        self,\n        input_data: Union[\n            pd.DataFrame,\n            List[Dict[str, float]],\n            Dict[str, List[float]],\n            Dict[str, float],\n        ],\n    ) -&gt; pd.DataFrame:\n        \"\"\"evaluate dataframe of inputs\"\"\"\n        if self.vectorized:\n            output_data = self.safe_function(input_data, **self.function_kwargs)\n        else:\n            # This construction is needed to avoid a pickle error\n            # translate input data into pandas dataframes\n            if not isinstance(input_data, DataFrame):\n                try:\n                    input_data = DataFrame(input_data)\n                except ValueError:\n                    input_data = DataFrame(input_data, index=[0])\n\n            inputs = input_data.to_dict(\"records\")\n\n            funcs = [self.function] * len(inputs)\n            kwargs = [self.function_kwargs] * len(inputs)\n\n            output_data = self.executor.map(\n                safe_function1_for_map,\n                funcs,\n                inputs,\n                kwargs,\n            )\n\n        return pd.concat(\n            [input_data, DataFrame(output_data, index=input_data.index)], axis=1\n        )\n\n    def safe_function(self, *args, **kwargs):\n        \"\"\"\n        Safely call the function, handling exceptions.\n\n        Note that this should not be submitted to fuu\n        \"\"\"\n        return safe_function(self.function, *args, **kwargs)\n\n    def submit(self, input: Dict):\n        \"\"\"submit a single input to the executor\n\n        Parameters\n        ----------\n        input : dict\n\n        Returns\n        -------\n        Future  : Future object\n        \"\"\"\n        if not isinstance(input, dict):\n            raise ValueError(\"input must be a dictionary\")\n        # return self.executor.submit(self.function, input, **self.function_kwargs)\n        # Must call a function outside of the classs\n        # See: https://stackoverflow.com/questions/44144584/typeerror-cant-pickle-thread-lock-objects\n        return self.executor.submit(\n            safe_function, self.function, input, **self.function_kwargs\n        )\n\n    def submit_data(self, input_data: pd.DataFrame):\n        \"\"\"submit dataframe of inputs to executor\"\"\"\n        input_data = pd.DataFrame(input_data)  # cast to dataframe for consistency\n\n        if self.vectorized:\n            # Single submission, cast to numpy array\n            inputs = input_data.to_dict(orient=\"list\")\n            for key, value in inputs.items():\n                inputs[key] = np.array(value)\n            futures = [self.submit(inputs)]  # Single item\n        else:\n            # Do not use iterrows or itertuples.\n            futures = [self.submit(inputs) for inputs in input_data.to_dict(\"records\")]\n\n        return futures\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.evaluate","title":"<code>evaluate(input, **kwargs)</code>","text":"<p>Evaluate a single input dict using Evaluator.function with Evaluator.function_kwargs.</p> <p>Further kwargs are passed to the function.</p> Inputs <p>inputs: dict of inputs to be evaluated **kwargs: additional kwargs to pass to the function</p> <p>Returns:</p> Type Description <p>function(input, **function_kwargs_updated)</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def evaluate(self, input: Dict, **kwargs):\n    \"\"\"\n    Evaluate a single input dict using Evaluator.function with\n    Evaluator.function_kwargs.\n\n    Further kwargs are passed to the function.\n\n    Inputs:\n        inputs: dict of inputs to be evaluated\n        **kwargs: additional kwargs to pass to the function\n\n    Returns:\n        function(input, **function_kwargs_updated)\n\n    \"\"\"\n    return self.safe_function(input, **{**self.function_kwargs, **kwargs})\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.evaluate_data","title":"<code>evaluate_data(input_data)</code>","text":"<p>evaluate dataframe of inputs</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def evaluate_data(\n    self,\n    input_data: Union[\n        pd.DataFrame,\n        List[Dict[str, float]],\n        Dict[str, List[float]],\n        Dict[str, float],\n    ],\n) -&gt; pd.DataFrame:\n    \"\"\"evaluate dataframe of inputs\"\"\"\n    if self.vectorized:\n        output_data = self.safe_function(input_data, **self.function_kwargs)\n    else:\n        # This construction is needed to avoid a pickle error\n        # translate input data into pandas dataframes\n        if not isinstance(input_data, DataFrame):\n            try:\n                input_data = DataFrame(input_data)\n            except ValueError:\n                input_data = DataFrame(input_data, index=[0])\n\n        inputs = input_data.to_dict(\"records\")\n\n        funcs = [self.function] * len(inputs)\n        kwargs = [self.function_kwargs] * len(inputs)\n\n        output_data = self.executor.map(\n            safe_function1_for_map,\n            funcs,\n            inputs,\n            kwargs,\n        )\n\n    return pd.concat(\n        [input_data, DataFrame(output_data, index=input_data.index)], axis=1\n    )\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.safe_function","title":"<code>safe_function(*args, **kwargs)</code>","text":"<p>Safely call the function, handling exceptions.</p> <p>Note that this should not be submitted to fuu</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def safe_function(self, *args, **kwargs):\n    \"\"\"\n    Safely call the function, handling exceptions.\n\n    Note that this should not be submitted to fuu\n    \"\"\"\n    return safe_function(self.function, *args, **kwargs)\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.submit","title":"<code>submit(input)</code>","text":"<p>submit a single input to the executor</p>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.submit--parameters","title":"Parameters","text":"<p>input : dict</p>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.submit--returns","title":"Returns","text":"<p>Future  : Future object</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def submit(self, input: Dict):\n    \"\"\"submit a single input to the executor\n\n    Parameters\n    ----------\n    input : dict\n\n    Returns\n    -------\n    Future  : Future object\n    \"\"\"\n    if not isinstance(input, dict):\n        raise ValueError(\"input must be a dictionary\")\n    # return self.executor.submit(self.function, input, **self.function_kwargs)\n    # Must call a function outside of the classs\n    # See: https://stackoverflow.com/questions/44144584/typeerror-cant-pickle-thread-lock-objects\n    return self.executor.submit(\n        safe_function, self.function, input, **self.function_kwargs\n    )\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.submit_data","title":"<code>submit_data(input_data)</code>","text":"<p>submit dataframe of inputs to executor</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def submit_data(self, input_data: pd.DataFrame):\n    \"\"\"submit dataframe of inputs to executor\"\"\"\n    input_data = pd.DataFrame(input_data)  # cast to dataframe for consistency\n\n    if self.vectorized:\n        # Single submission, cast to numpy array\n        inputs = input_data.to_dict(orient=\"list\")\n        for key, value in inputs.items():\n            inputs[key] = np.array(value)\n        futures = [self.submit(inputs)]  # Single item\n    else:\n        # Do not use iterrows or itertuples.\n        futures = [self.submit(inputs) for inputs in input_data.to_dict(\"records\")]\n\n    return futures\n</code></pre>"},{"location":"api/generators/","title":"Base generator class","text":""},{"location":"api/generators/#xopt.generator.Generator","title":"<code>Generator</code>","text":"<p>             Bases: <code>XoptBaseModel</code>, <code>ABC</code></p> Source code in <code>xopt/generator.py</code> <pre><code>class Generator(XoptBaseModel, ABC):\n    name: ClassVar[str] = Field(description=\"generator name\")\n\n    supports_batch_generation: bool = Field(\n        default=False,\n        description=\"flag that describes if this \"\n        \"generator can generate \"\n        \"batches of points\",\n        frozen=True,\n        exclude=True,\n    )\n    supports_multi_objective: bool = Field(\n        default=False,\n        description=\"flag that describes if this generator can solve multi-objective \"\n        \"problems\",\n        frozen=True,\n        exclude=True,\n    )\n\n    vocs: VOCS = Field(description=\"generator VOCS\", exclude=True)\n    data: Optional[pd.DataFrame] = Field(\n        None, description=\"generator data\", exclude=True\n    )\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    _is_done = False\n\n    @field_validator(\"vocs\", mode=\"after\")\n    def validate_vocs(cls, v, info: ValidationInfo):\n        if v.n_objectives != 1 and not info.data[\"supports_multi_objective\"]:\n            raise ValueError(\"this generator only supports single objective\")\n        return v\n\n    @field_validator(\"data\", mode=\"before\")\n    def validate_data(cls, v):\n        if isinstance(v, dict):\n            try:\n                v = pd.DataFrame(v)\n            except IndexError:\n                v = pd.DataFrame(v, index=[0])\n        return v\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the generator.\n\n        \"\"\"\n        super().__init__(**kwargs)\n        logger.info(f\"Initialized generator {self.name}\")\n\n    @property\n    def is_done(self):\n        return self._is_done\n\n    @abstractmethod\n    def generate(self, n_candidates) -&gt; list[dict]:\n        pass\n\n    def add_data(self, new_data: pd.DataFrame):\n        \"\"\"\n        update dataframe with results from new evaluations.\n\n        This is intended for generators that maintain their own data.\n\n        \"\"\"\n        if self.data is not None:\n            self.data = pd.concat([self.data, new_data], axis=0)\n        else:\n            self.data = new_data\n\n    def model_dump(self, *args, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"overwrite model dump to remove faux class attrs\"\"\"\n\n        res = super().model_dump(*args, **kwargs)\n\n        res.pop(\"supports_batch_generation\", None)\n        res.pop(\"supports_multi_objective\", None)\n\n        return res\n</code></pre>"},{"location":"api/generators/#xopt.generator.Generator.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the generator.</p> Source code in <code>xopt/generator.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the generator.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    logger.info(f\"Initialized generator {self.name}\")\n</code></pre>"},{"location":"api/generators/#xopt.generator.Generator.add_data","title":"<code>add_data(new_data)</code>","text":"<p>update dataframe with results from new evaluations.</p> <p>This is intended for generators that maintain their own data.</p> Source code in <code>xopt/generator.py</code> <pre><code>def add_data(self, new_data: pd.DataFrame):\n    \"\"\"\n    update dataframe with results from new evaluations.\n\n    This is intended for generators that maintain their own data.\n\n    \"\"\"\n    if self.data is not None:\n        self.data = pd.concat([self.data, new_data], axis=0)\n    else:\n        self.data = new_data\n</code></pre>"},{"location":"api/generators/#xopt.generator.Generator.model_dump","title":"<code>model_dump(*args, **kwargs)</code>","text":"<p>overwrite model dump to remove faux class attrs</p> Source code in <code>xopt/generator.py</code> <pre><code>def model_dump(self, *args, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"overwrite model dump to remove faux class attrs\"\"\"\n\n    res = super().model_dump(*args, **kwargs)\n\n    res.pop(\"supports_batch_generation\", None)\n    res.pop(\"supports_multi_objective\", None)\n\n    return res\n</code></pre>"},{"location":"api/vocs/","title":"Vocs","text":"<p>             Bases: <code>XoptBaseModel</code></p> <p>Variables, Objectives, Constraints, and other Settings (VOCS) data structure to describe optimization problems.</p> Source code in <code>xopt/vocs.py</code> <pre><code>class VOCS(XoptBaseModel):\n    \"\"\"\n    Variables, Objectives, Constraints, and other Settings (VOCS) data structure\n    to describe optimization problems.\n    \"\"\"\n\n    variables: Dict[str, conlist(float, min_length=2, max_length=2)] = Field(\n        default={},\n        description=\"input variable names with a list of minimum and maximum values\",\n    )\n    constraints: Dict[\n        str, conlist(Union[float, ConstraintEnum], min_length=2, max_length=2)\n    ] = Field(\n        default={},\n        description=\"constraint names with a list of constraint type and value\",\n    )\n    objectives: Dict[str, ObjectiveEnum] = Field(\n        default={}, description=\"objective names with type of objective\"\n    )\n    constants: Dict[str, Any] = Field(\n        default={}, description=\"constant names and values passed to evaluate function\"\n    )\n    observables: List[str] = Field(\n        default=[],\n        description=\"observation names tracked alongside objectives and constraints\",\n    )\n\n    model_config = ConfigDict(\n        validate_assignment=True, use_enum_values=True, extra=\"forbid\"\n    )\n\n    @classmethod\n    def from_yaml(cls, yaml_text):\n        loaded = yaml.safe_load(yaml_text)\n        return cls(**loaded)\n\n    def as_yaml(self):\n        return yaml.dump(self.model_dump(), default_flow_style=None, sort_keys=False)\n\n    @property\n    def bounds(self):\n        \"\"\"\n        Returns a bounds array (mins, maxs) of shape (2, n_variables)\n        Arrays of lower and upper bounds can be extracted by:\n            mins, maxs = vocs.bounds\n        \"\"\"\n        return np.array([v for _, v in sorted(self.variables.items())]).T\n\n    @property\n    def variable_names(self):\n        \"\"\"Returns a sorted list of variable names\"\"\"\n        return list(sorted(self.variables.keys()))\n\n    @property\n    def objective_names(self):\n        \"\"\"Returns a sorted list of objective names\"\"\"\n        return list(sorted(self.objectives.keys()))\n\n    @property\n    def constraint_names(self):\n        \"\"\"Returns a sorted list of constraint names\"\"\"\n        if self.constraints is None:\n            return []\n        return list(sorted(self.constraints.keys()))\n\n    @property\n    def observable_names(self):\n        return sorted(self.observables)\n\n    @property\n    def output_names(self):\n        \"\"\"\n        Returns a list of expected output keys:\n            (objectives + constraints + observables)\n        Each sub-list is sorted.\n        \"\"\"\n        full_list = self.objective_names\n        for ele in self.constraint_names:\n            if ele not in full_list:\n                full_list += [ele]\n\n        for ele in self.observable_names:\n            if ele not in full_list:\n                full_list += [ele]\n\n        return full_list\n\n    @property\n    def constant_names(self):\n        \"\"\"Returns a sorted list of constraint names\"\"\"\n        if self.constants is None:\n            return []\n        return list(sorted(self.constants.keys()))\n\n    @property\n    def all_names(self):\n        \"\"\"Returns all vocs names (variables, constants, objectives, constraints)\"\"\"\n        return self.variable_names + self.constant_names + self.output_names\n\n    @property\n    def n_variables(self):\n        \"\"\"Returns the number of variables\"\"\"\n        return len(self.variables)\n\n    @property\n    def n_constants(self):\n        \"\"\"Returns the number of constants\"\"\"\n        return len(self.constants)\n\n    @property\n    def n_inputs(self):\n        \"\"\"Returns the number of inputs (variables and constants)\"\"\"\n        return self.n_variables + self.n_constants\n\n    @property\n    def n_objectives(self):\n        \"\"\"Returns the number of objectives\"\"\"\n        return len(self.objectives)\n\n    @property\n    def n_constraints(self):\n        \"\"\"Returns the number of constraints\"\"\"\n        return len(self.constraints)\n\n    @property\n    def n_observables(self):\n        \"\"\"Returns the number of constraints\"\"\"\n        return len(self.observables)\n\n    @property\n    def n_outputs(self):\n        \"\"\"\n        Returns the number of outputs\n            len(objectives + constraints + observables)\n        \"\"\"\n        return len(self.output_names)\n\n    def random_inputs(\n        self,\n        n: int = None,\n        custom_bounds: dict = None,\n        include_constants: bool = True,\n        seed: int = None,\n    ) -&gt; list[dict]:\n        \"\"\"\n        Uniform sampling of the variables.\n\n        Returns a dict of inputs.\n\n        If include_constants, the vocs.constants are added to the dict.\n\n        Optional:\n            n (integer) to make arrays of inputs, of size n.\n            seed (integer) to initialize the random number generator\n\n        \"\"\"\n        inputs = {}\n        if seed is None:\n            rng_sample_function = np.random.random\n        else:\n            rng = np.random.default_rng(seed=seed)\n            rng_sample_function = rng.random\n\n        # get bounds\n        # if custom_bounds is specified then they will be clipped inside\n        # vocs variable bounds\n        if custom_bounds is None:\n            bounds = self.variables\n        else:\n            variable_bounds = pd.DataFrame(self.variables)\n            custom_bounds = pd.DataFrame(custom_bounds)\n            custom_bounds = custom_bounds.clip(\n                variable_bounds.iloc[0], variable_bounds.iloc[1], axis=1\n            )\n            bounds = custom_bounds.to_dict()\n            for k in bounds.keys():\n                bounds[k] = [bounds[k][i] for i in range(2)]\n\n        for key, val in bounds.items():  # No need to sort here\n            a, b = val\n            n = n if n is not None else 1\n            x = rng_sample_function(n)\n            inputs[key] = x * a + (1 - x) * b\n\n        # Constants\n        if include_constants and self.constants is not None:\n            inputs.update(self.constants)\n\n        if n == 1:\n            return [inputs]\n        else:\n            return pd.DataFrame(inputs).to_dict(\"records\")\n\n    def convert_dataframe_to_inputs(\n        self, data: pd.DataFrame, include_constants=True\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Extracts only inputs from a dataframe.\n        This will add constants if `include_constants` is true.\n        \"\"\"\n        # make sure that the df keys only contain vocs variables\n        if not set(self.variable_names) == set(data.keys()):\n            raise ValueError(\n                \"input dataframe column set must equal set of vocs variables\"\n            )\n\n        # only keep the variables\n        inner_copy = data.copy()\n\n        # append constants if requested\n        if include_constants:\n            constants = self.constants\n            if constants is not None:\n                for name, val in constants.items():\n                    inner_copy[name] = val\n\n        return inner_copy\n\n    def convert_numpy_to_inputs(\n        self, inputs: np.ndarray, include_constants=True\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        convert 2D numpy array to list of dicts (inputs) for evaluation\n        Assumes that the columns of the array match correspond to\n        `sorted(self.vocs.variables.keys())\n\n        \"\"\"\n        df = pd.DataFrame(inputs, columns=self.variable_names)\n        return self.convert_dataframe_to_inputs(df, include_constants)\n\n    # Extract optimization data (in correct column order)\n    def variable_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"variable_\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns a dataframe containing variables according to `vocs.variables` in sorted\n        order\n\n        Args:\n            data: Data to be processed.\n            prefix: Prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_variable_data(self.variables, data, prefix=prefix)\n\n    def objective_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"objective_\",\n        return_raw=False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns a dataframe containing objective data transformed according to\n        `vocs.objectives` such that we always assume minimization.\n\n        Args:\n            data: data to be processed.\n            prefix: prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_objective_data(self.objectives, data, prefix, return_raw)\n\n    def constraint_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"constraint_\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns a dataframe containing constraint data transformed according to\n        `vocs.constraints` such that values that satisfy each constraint are negative.\n\n        Args:\n            data: data to be processed.\n            prefix: prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_constraint_data(self.constraints, data, prefix)\n\n    def observable_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"observable_\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns a dataframe containing observable data\n\n        Args:\n            data: data to be processed.\n            prefix: prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_observable_data(self.observable_names, data, prefix)\n\n    def feasibility_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"feasible_\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns a dataframe containing booleans denoting if a constraint is satisfied or\n        not. Returned dataframe also contains a column `feasible` which denotes if\n        all constraints are satisfied.\n\n        Args:\n            data: data to be processed.\n            prefix: prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_feasibility_data(self.constraints, data, prefix)\n\n    def normalize_inputs(self, input_points: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Normalize input data (transform data into the range [0,1]) based on the\n        variable ranges defined in the VOCS.\n\n        Parameters\n        ----------\n        input_points : pd.DataFrame\n            A DataFrame containing input data to be normalized.\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame with input data in the range [0,1] corresponding to the\n            specified variable ranges. Contains columns equal to the intersection\n            between `input_points` and `vocs.variable_names`.\n\n        Notes\n        -----\n\n        If the input DataFrame is empty or no variable information is available in\n        the VOCS, an empty DataFrame is returned.\n\n        \"\"\"\n        normed_data = {}\n        for name in self.variable_names:\n            if name in input_points.columns:\n                width = self.variables[name][1] - self.variables[name][0]\n                normed_data[name] = (\n                    input_points[name] - self.variables[name][0]\n                ) / width\n\n        if len(normed_data):\n            return pd.DataFrame(normed_data)\n        else:\n            return pd.DataFrame([])\n\n    def denormalize_inputs(self, input_points: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Denormalize input data (transform data from the range [0,1]) based on the\n        variable ranges defined in the VOCS.\n\n        Parameters\n        ----------\n        input_points : pd.DataFrame\n            A DataFrame containing normalized input data in the range [0,1].\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame with denormalized input data corresponding to the\n            specified variable ranges. Contains columns equal to the intersection\n            between `input_points` and `vocs.variable_names`.\n\n        Notes\n        -----\n\n        If the input DataFrame is empty or no variable information is available in\n        the VOCS, an empty DataFrame is returned.\n\n        \"\"\"\n        denormed_data = {}\n        for name in self.variable_names:\n            if name in input_points.columns:\n                width = self.variables[name][1] - self.variables[name][0]\n                denormed_data[name] = (\n                    input_points[name] * width + self.variables[name][0]\n                )\n\n        if len(denormed_data):\n            return pd.DataFrame(denormed_data)\n        else:\n            return pd.DataFrame([])\n\n    def validate_input_data(self, input_points: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Validates input data. Raises an error if the input data does not satisfy\n        requirements given by vocs.\n\n        Args:\n            input_points: input data to be validated.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: if input data does not satisfy requirements.\n        \"\"\"\n        validate_input_data(self, input_points)\n\n    def extract_data(self, data: pd.DataFrame, return_raw=False):\n        \"\"\"\n        split dataframe into seperate dataframes for variables, objectives and\n        constraints based on vocs - objective data is transformed based on\n        `vocs.objectives` properties\n\n        Args:\n            data: dataframe to be split\n            return_raw: if True, return untransformed objective data\n\n        Returns:\n            variable_data: dataframe containing variable data\n            objective_data: dataframe containing objective data\n            constraint_data: dataframe containing constraint data\n        \"\"\"\n        variable_data = self.variable_data(data, \"\")\n        objective_data = self.objective_data(data, \"\", return_raw)\n        constraint_data = self.constraint_data(data, \"\")\n        return variable_data, objective_data, constraint_data\n\n    def select_best(self, data: pd.DataFrame, n=1):\n        \"\"\"\n        get the best value and point for a given data set based on vocs\n        - does not work for multi-objective problems\n        - data that violates any constraints is ignored\n\n        Args:\n            data: dataframe to select best point from\n            n: number of best points to return\n\n        Returns:\n            index: index of best point\n            value: value of best point\n        \"\"\"\n        if self.n_objectives != 1:\n            raise NotImplementedError(\n                \"cannot select best point when n_objectives is not 1\"\n            )\n\n        feasible_data = self.feasibility_data(data)\n        ascending_flag = {\"MINIMIZE\": True, \"MAXIMIZE\": False}\n        obj = self.objectives[self.objective_names[0]]\n        obj_name = self.objective_names[0]\n        res = data[feasible_data[\"feasible\"]].sort_values(\n            obj_name, ascending=ascending_flag[obj]\n        )[obj_name][:n]\n\n        return res.index.to_numpy(), res.to_numpy()\n\n    def cumulative_optimum(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns the cumulative optimum for the given DataFrame.\n\n        Parameters\n        ----------\n        data: pd.DataFrame\n            Data for which the cumulative optimum shall be calculated.\n\n        Returns\n        -------\n        pd.DataFrame\n            Cumulative optimum for the given DataFrame.\n\n        \"\"\"\n        if not self.objectives:\n            raise RuntimeError(\"No objectives defined.\")\n        if data.empty:\n            return pd.DataFrame()\n        obj_name = self.objective_names[0]\n        obj = self.objectives[obj_name]\n        get_opt = np.nanmax if obj == \"MAXIMIZE\" else np.nanmin\n        feasible = self.feasibility_data(data)[\"feasible\"]\n        feasible_obj_values = [\n            data[obj_name].values[i] if feasible[i] else np.nan\n            for i in range(len(data))\n        ]\n        cumulative_optimum = np.array(\n            [get_opt(feasible_obj_values[: i + 1]) for i in range(len(data))]\n        )\n        return pd.DataFrame({f\"best_{obj_name}\": cumulative_optimum}, index=data.index)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.all_names","title":"<code>all_names</code>  <code>property</code>","text":"<p>Returns all vocs names (variables, constants, objectives, constraints)</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Returns a bounds array (mins, maxs) of shape (2, n_variables) Arrays of lower and upper bounds can be extracted by:     mins, maxs = vocs.bounds</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.constant_names","title":"<code>constant_names</code>  <code>property</code>","text":"<p>Returns a sorted list of constraint names</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.constraint_names","title":"<code>constraint_names</code>  <code>property</code>","text":"<p>Returns a sorted list of constraint names</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_constants","title":"<code>n_constants</code>  <code>property</code>","text":"<p>Returns the number of constants</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_constraints","title":"<code>n_constraints</code>  <code>property</code>","text":"<p>Returns the number of constraints</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_inputs","title":"<code>n_inputs</code>  <code>property</code>","text":"<p>Returns the number of inputs (variables and constants)</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_objectives","title":"<code>n_objectives</code>  <code>property</code>","text":"<p>Returns the number of objectives</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_observables","title":"<code>n_observables</code>  <code>property</code>","text":"<p>Returns the number of constraints</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_outputs","title":"<code>n_outputs</code>  <code>property</code>","text":"<p>Returns the number of outputs     len(objectives + constraints + observables)</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_variables","title":"<code>n_variables</code>  <code>property</code>","text":"<p>Returns the number of variables</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.objective_names","title":"<code>objective_names</code>  <code>property</code>","text":"<p>Returns a sorted list of objective names</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.output_names","title":"<code>output_names</code>  <code>property</code>","text":"Returns a list of expected output keys <p>(objectives + constraints + observables)</p> <p>Each sub-list is sorted.</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.variable_names","title":"<code>variable_names</code>  <code>property</code>","text":"<p>Returns a sorted list of variable names</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.constraint_data","title":"<code>constraint_data(data, prefix='constraint_')</code>","text":"<p>Returns a dataframe containing constraint data transformed according to <code>vocs.constraints</code> such that values that satisfy each constraint are negative.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Dict], List[Dict]]</code> <p>data to be processed.</p> required <code>prefix</code> <code>str</code> <p>prefix added to column names.</p> <code>'constraint_'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>DataFrame</code> <p>processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def constraint_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"constraint_\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a dataframe containing constraint data transformed according to\n    `vocs.constraints` such that values that satisfy each constraint are negative.\n\n    Args:\n        data: data to be processed.\n        prefix: prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_constraint_data(self.constraints, data, prefix)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.convert_dataframe_to_inputs","title":"<code>convert_dataframe_to_inputs(data, include_constants=True)</code>","text":"<p>Extracts only inputs from a dataframe. This will add constants if <code>include_constants</code> is true.</p> Source code in <code>xopt/vocs.py</code> <pre><code>def convert_dataframe_to_inputs(\n    self, data: pd.DataFrame, include_constants=True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Extracts only inputs from a dataframe.\n    This will add constants if `include_constants` is true.\n    \"\"\"\n    # make sure that the df keys only contain vocs variables\n    if not set(self.variable_names) == set(data.keys()):\n        raise ValueError(\n            \"input dataframe column set must equal set of vocs variables\"\n        )\n\n    # only keep the variables\n    inner_copy = data.copy()\n\n    # append constants if requested\n    if include_constants:\n        constants = self.constants\n        if constants is not None:\n            for name, val in constants.items():\n                inner_copy[name] = val\n\n    return inner_copy\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.convert_numpy_to_inputs","title":"<code>convert_numpy_to_inputs(inputs, include_constants=True)</code>","text":"<p>convert 2D numpy array to list of dicts (inputs) for evaluation Assumes that the columns of the array match correspond to `sorted(self.vocs.variables.keys())</p> Source code in <code>xopt/vocs.py</code> <pre><code>def convert_numpy_to_inputs(\n    self, inputs: np.ndarray, include_constants=True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    convert 2D numpy array to list of dicts (inputs) for evaluation\n    Assumes that the columns of the array match correspond to\n    `sorted(self.vocs.variables.keys())\n\n    \"\"\"\n    df = pd.DataFrame(inputs, columns=self.variable_names)\n    return self.convert_dataframe_to_inputs(df, include_constants)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.cumulative_optimum","title":"<code>cumulative_optimum(data)</code>","text":"<p>Returns the cumulative optimum for the given DataFrame.</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.cumulative_optimum--parameters","title":"Parameters","text":"<p>data: pd.DataFrame     Data for which the cumulative optimum shall be calculated.</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.cumulative_optimum--returns","title":"Returns","text":"<p>pd.DataFrame     Cumulative optimum for the given DataFrame.</p> Source code in <code>xopt/vocs.py</code> <pre><code>def cumulative_optimum(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns the cumulative optimum for the given DataFrame.\n\n    Parameters\n    ----------\n    data: pd.DataFrame\n        Data for which the cumulative optimum shall be calculated.\n\n    Returns\n    -------\n    pd.DataFrame\n        Cumulative optimum for the given DataFrame.\n\n    \"\"\"\n    if not self.objectives:\n        raise RuntimeError(\"No objectives defined.\")\n    if data.empty:\n        return pd.DataFrame()\n    obj_name = self.objective_names[0]\n    obj = self.objectives[obj_name]\n    get_opt = np.nanmax if obj == \"MAXIMIZE\" else np.nanmin\n    feasible = self.feasibility_data(data)[\"feasible\"]\n    feasible_obj_values = [\n        data[obj_name].values[i] if feasible[i] else np.nan\n        for i in range(len(data))\n    ]\n    cumulative_optimum = np.array(\n        [get_opt(feasible_obj_values[: i + 1]) for i in range(len(data))]\n    )\n    return pd.DataFrame({f\"best_{obj_name}\": cumulative_optimum}, index=data.index)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.denormalize_inputs","title":"<code>denormalize_inputs(input_points)</code>","text":"<p>Denormalize input data (transform data from the range [0,1]) based on the variable ranges defined in the VOCS.</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.denormalize_inputs--parameters","title":"Parameters","text":"<p>input_points : pd.DataFrame     A DataFrame containing normalized input data in the range [0,1].</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.denormalize_inputs--returns","title":"Returns","text":"<p>pd.DataFrame     A DataFrame with denormalized input data corresponding to the     specified variable ranges. Contains columns equal to the intersection     between <code>input_points</code> and <code>vocs.variable_names</code>.</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.denormalize_inputs--notes","title":"Notes","text":"<p>If the input DataFrame is empty or no variable information is available in the VOCS, an empty DataFrame is returned.</p> Source code in <code>xopt/vocs.py</code> <pre><code>def denormalize_inputs(self, input_points: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Denormalize input data (transform data from the range [0,1]) based on the\n    variable ranges defined in the VOCS.\n\n    Parameters\n    ----------\n    input_points : pd.DataFrame\n        A DataFrame containing normalized input data in the range [0,1].\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame with denormalized input data corresponding to the\n        specified variable ranges. Contains columns equal to the intersection\n        between `input_points` and `vocs.variable_names`.\n\n    Notes\n    -----\n\n    If the input DataFrame is empty or no variable information is available in\n    the VOCS, an empty DataFrame is returned.\n\n    \"\"\"\n    denormed_data = {}\n    for name in self.variable_names:\n        if name in input_points.columns:\n            width = self.variables[name][1] - self.variables[name][0]\n            denormed_data[name] = (\n                input_points[name] * width + self.variables[name][0]\n            )\n\n    if len(denormed_data):\n        return pd.DataFrame(denormed_data)\n    else:\n        return pd.DataFrame([])\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.extract_data","title":"<code>extract_data(data, return_raw=False)</code>","text":"<p>split dataframe into seperate dataframes for variables, objectives and constraints based on vocs - objective data is transformed based on <code>vocs.objectives</code> properties</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>dataframe to be split</p> required <code>return_raw</code> <p>if True, return untransformed objective data</p> <code>False</code> <p>Returns:</p> Name Type Description <code>variable_data</code> <p>dataframe containing variable data</p> <code>objective_data</code> <p>dataframe containing objective data</p> <code>constraint_data</code> <p>dataframe containing constraint data</p> Source code in <code>xopt/vocs.py</code> <pre><code>def extract_data(self, data: pd.DataFrame, return_raw=False):\n    \"\"\"\n    split dataframe into seperate dataframes for variables, objectives and\n    constraints based on vocs - objective data is transformed based on\n    `vocs.objectives` properties\n\n    Args:\n        data: dataframe to be split\n        return_raw: if True, return untransformed objective data\n\n    Returns:\n        variable_data: dataframe containing variable data\n        objective_data: dataframe containing objective data\n        constraint_data: dataframe containing constraint data\n    \"\"\"\n    variable_data = self.variable_data(data, \"\")\n    objective_data = self.objective_data(data, \"\", return_raw)\n    constraint_data = self.constraint_data(data, \"\")\n    return variable_data, objective_data, constraint_data\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.feasibility_data","title":"<code>feasibility_data(data, prefix='feasible_')</code>","text":"<p>Returns a dataframe containing booleans denoting if a constraint is satisfied or not. Returned dataframe also contains a column <code>feasible</code> which denotes if all constraints are satisfied.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Dict], List[Dict]]</code> <p>data to be processed.</p> required <code>prefix</code> <code>str</code> <p>prefix added to column names.</p> <code>'feasible_'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>DataFrame</code> <p>processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def feasibility_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"feasible_\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a dataframe containing booleans denoting if a constraint is satisfied or\n    not. Returned dataframe also contains a column `feasible` which denotes if\n    all constraints are satisfied.\n\n    Args:\n        data: data to be processed.\n        prefix: prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_feasibility_data(self.constraints, data, prefix)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.normalize_inputs","title":"<code>normalize_inputs(input_points)</code>","text":"<p>Normalize input data (transform data into the range [0,1]) based on the variable ranges defined in the VOCS.</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.normalize_inputs--parameters","title":"Parameters","text":"<p>input_points : pd.DataFrame     A DataFrame containing input data to be normalized.</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.normalize_inputs--returns","title":"Returns","text":"<p>pd.DataFrame     A DataFrame with input data in the range [0,1] corresponding to the     specified variable ranges. Contains columns equal to the intersection     between <code>input_points</code> and <code>vocs.variable_names</code>.</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.normalize_inputs--notes","title":"Notes","text":"<p>If the input DataFrame is empty or no variable information is available in the VOCS, an empty DataFrame is returned.</p> Source code in <code>xopt/vocs.py</code> <pre><code>def normalize_inputs(self, input_points: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Normalize input data (transform data into the range [0,1]) based on the\n    variable ranges defined in the VOCS.\n\n    Parameters\n    ----------\n    input_points : pd.DataFrame\n        A DataFrame containing input data to be normalized.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame with input data in the range [0,1] corresponding to the\n        specified variable ranges. Contains columns equal to the intersection\n        between `input_points` and `vocs.variable_names`.\n\n    Notes\n    -----\n\n    If the input DataFrame is empty or no variable information is available in\n    the VOCS, an empty DataFrame is returned.\n\n    \"\"\"\n    normed_data = {}\n    for name in self.variable_names:\n        if name in input_points.columns:\n            width = self.variables[name][1] - self.variables[name][0]\n            normed_data[name] = (\n                input_points[name] - self.variables[name][0]\n            ) / width\n\n    if len(normed_data):\n        return pd.DataFrame(normed_data)\n    else:\n        return pd.DataFrame([])\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.objective_data","title":"<code>objective_data(data, prefix='objective_', return_raw=False)</code>","text":"<p>Returns a dataframe containing objective data transformed according to <code>vocs.objectives</code> such that we always assume minimization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Dict], List[Dict]]</code> <p>data to be processed.</p> required <code>prefix</code> <code>str</code> <p>prefix added to column names.</p> <code>'objective_'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>DataFrame</code> <p>processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def objective_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"objective_\",\n    return_raw=False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a dataframe containing objective data transformed according to\n    `vocs.objectives` such that we always assume minimization.\n\n    Args:\n        data: data to be processed.\n        prefix: prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_objective_data(self.objectives, data, prefix, return_raw)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.observable_data","title":"<code>observable_data(data, prefix='observable_')</code>","text":"<p>Returns a dataframe containing observable data</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Dict], List[Dict]]</code> <p>data to be processed.</p> required <code>prefix</code> <code>str</code> <p>prefix added to column names.</p> <code>'observable_'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>DataFrame</code> <p>processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def observable_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"observable_\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a dataframe containing observable data\n\n    Args:\n        data: data to be processed.\n        prefix: prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_observable_data(self.observable_names, data, prefix)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.random_inputs","title":"<code>random_inputs(n=None, custom_bounds=None, include_constants=True, seed=None)</code>","text":"<p>Uniform sampling of the variables.</p> <p>Returns a dict of inputs.</p> <p>If include_constants, the vocs.constants are added to the dict.</p> Optional <p>n (integer) to make arrays of inputs, of size n. seed (integer) to initialize the random number generator</p> Source code in <code>xopt/vocs.py</code> <pre><code>def random_inputs(\n    self,\n    n: int = None,\n    custom_bounds: dict = None,\n    include_constants: bool = True,\n    seed: int = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Uniform sampling of the variables.\n\n    Returns a dict of inputs.\n\n    If include_constants, the vocs.constants are added to the dict.\n\n    Optional:\n        n (integer) to make arrays of inputs, of size n.\n        seed (integer) to initialize the random number generator\n\n    \"\"\"\n    inputs = {}\n    if seed is None:\n        rng_sample_function = np.random.random\n    else:\n        rng = np.random.default_rng(seed=seed)\n        rng_sample_function = rng.random\n\n    # get bounds\n    # if custom_bounds is specified then they will be clipped inside\n    # vocs variable bounds\n    if custom_bounds is None:\n        bounds = self.variables\n    else:\n        variable_bounds = pd.DataFrame(self.variables)\n        custom_bounds = pd.DataFrame(custom_bounds)\n        custom_bounds = custom_bounds.clip(\n            variable_bounds.iloc[0], variable_bounds.iloc[1], axis=1\n        )\n        bounds = custom_bounds.to_dict()\n        for k in bounds.keys():\n            bounds[k] = [bounds[k][i] for i in range(2)]\n\n    for key, val in bounds.items():  # No need to sort here\n        a, b = val\n        n = n if n is not None else 1\n        x = rng_sample_function(n)\n        inputs[key] = x * a + (1 - x) * b\n\n    # Constants\n    if include_constants and self.constants is not None:\n        inputs.update(self.constants)\n\n    if n == 1:\n        return [inputs]\n    else:\n        return pd.DataFrame(inputs).to_dict(\"records\")\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.select_best","title":"<code>select_best(data, n=1)</code>","text":"<p>get the best value and point for a given data set based on vocs - does not work for multi-objective problems - data that violates any constraints is ignored</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>dataframe to select best point from</p> required <code>n</code> <p>number of best points to return</p> <code>1</code> <p>Returns:</p> Name Type Description <code>index</code> <p>index of best point</p> <code>value</code> <p>value of best point</p> Source code in <code>xopt/vocs.py</code> <pre><code>def select_best(self, data: pd.DataFrame, n=1):\n    \"\"\"\n    get the best value and point for a given data set based on vocs\n    - does not work for multi-objective problems\n    - data that violates any constraints is ignored\n\n    Args:\n        data: dataframe to select best point from\n        n: number of best points to return\n\n    Returns:\n        index: index of best point\n        value: value of best point\n    \"\"\"\n    if self.n_objectives != 1:\n        raise NotImplementedError(\n            \"cannot select best point when n_objectives is not 1\"\n        )\n\n    feasible_data = self.feasibility_data(data)\n    ascending_flag = {\"MINIMIZE\": True, \"MAXIMIZE\": False}\n    obj = self.objectives[self.objective_names[0]]\n    obj_name = self.objective_names[0]\n    res = data[feasible_data[\"feasible\"]].sort_values(\n        obj_name, ascending=ascending_flag[obj]\n    )[obj_name][:n]\n\n    return res.index.to_numpy(), res.to_numpy()\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.validate_input_data","title":"<code>validate_input_data(input_points)</code>","text":"<p>Validates input data. Raises an error if the input data does not satisfy requirements given by vocs.</p> <p>Parameters:</p> Name Type Description Default <code>input_points</code> <code>DataFrame</code> <p>input data to be validated.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if input data does not satisfy requirements.</p> Source code in <code>xopt/vocs.py</code> <pre><code>def validate_input_data(self, input_points: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Validates input data. Raises an error if the input data does not satisfy\n    requirements given by vocs.\n\n    Args:\n        input_points: input data to be validated.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: if input data does not satisfy requirements.\n    \"\"\"\n    validate_input_data(self, input_points)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.variable_data","title":"<code>variable_data(data, prefix='variable_')</code>","text":"<p>Returns a dataframe containing variables according to <code>vocs.variables</code> in sorted order</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[DataFrame, List[Dict], List[Dict]]</code> <p>Data to be processed.</p> required <code>prefix</code> <code>str</code> <p>Prefix added to column names.</p> <code>'variable_'</code> <p>Returns:</p> Name Type Description <code>result</code> <code>DataFrame</code> <p>processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def variable_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"variable_\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a dataframe containing variables according to `vocs.variables` in sorted\n    order\n\n    Args:\n        data: Data to be processed.\n        prefix: Prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_variable_data(self.variables, data, prefix=prefix)\n</code></pre>"},{"location":"api/xopt/","title":"Xopt","text":"<p>             Bases: <code>XoptBaseModel</code></p> <p>Object to handle a single optimization problem.</p> <p>Xopt is designed for managing a single optimization problem by unifying the definition, configuration, and execution of optimization tasks. It combines the Variables, Objective, Constraints, Statics (VOCS) definition with a generator for candidate generation and an evaluator for objective function evaluations.</p>"},{"location":"api/xopt/#xopt.Xopt--parameters","title":"Parameters","text":"<p>vocs : VOCS     VOCS object for defining the problem's variables, objectives, constraints, and     statics. generator : SerializeAsAny[Generator]     An object responsible for generating candidates for optimization. evaluator : SerializeAsAny[Evaluator]     An object used for evaluating candidates generated by the generator. strict : bool, optional     A flag indicating whether exceptions raised during evaluation should stop the     optimization process. dump_file : str, optional     An optional file path for dumping attributes of the xopt object and the     results of evaluations. max_evaluations : int, optional     An optional maximum number of evaluations to perform. If set, the optimization     process will stop after reaching this limit. data : DataFrame, optional     An optional DataFrame object for storing internal data related to the optimization     process. serialize_torch : bool     A flag indicating whether Torch (PyTorch) models should be serialized when     saving them. serialize_inline : bool     A flag indicating whether Torch models should be stored via binary string     directly inside the main configuration file.</p>"},{"location":"api/xopt/#xopt.Xopt--methods","title":"Methods","text":"<p>step()     Executes one optimization cycle, generating candidates, submitting them for     evaluation, waiting for evaluation results, and updating data storage. run()     Runs the optimization process until the specified stopping criteria are met,     such as reaching the maximum number of evaluations. evaluate(input_dict: Dict)     Evaluates a candidate without storing data. evaluate_data(input_data)     Evaluates a set of candidates, adding the results to the internal DataFrame. add_data(new_data)     Adds new data to the internal DataFrame and the generator's data. reset_data()     Resets the internal data by clearing the DataFrame. random_evaluate(n_samples=1, seed=None, kwargs)     Generates random inputs using the VOCS and evaluates them, adding the data to     Xopt. yaml(kwargs)     Serializes the Xopt configuration to a YAML string. dump(file: str = None, kwargs)     Dumps the Xopt configuration to a specified file. dict(kwargs) -&gt; Dict     Provides a custom dictionary representation of the Xopt configuration. json(**kwargs) -&gt; str     Serializes the Xopt configuration to a JSON string.</p> Source code in <code>xopt/base.py</code> <pre><code>class Xopt(XoptBaseModel):\n    \"\"\"\n    Object to handle a single optimization problem.\n\n    Xopt is designed for managing a single optimization problem by unifying the\n    definition, configuration, and execution of optimization tasks. It combines the\n    Variables, Objective, Constraints, Statics (VOCS) definition with a generator for\n    candidate generation and an evaluator for objective function evaluations.\n\n    Parameters\n    ----------\n    vocs : VOCS\n        VOCS object for defining the problem's variables, objectives, constraints, and\n        statics.\n    generator : SerializeAsAny[Generator]\n        An object responsible for generating candidates for optimization.\n    evaluator : SerializeAsAny[Evaluator]\n        An object used for evaluating candidates generated by the generator.\n    strict : bool, optional\n        A flag indicating whether exceptions raised during evaluation should stop the\n        optimization process.\n    dump_file : str, optional\n        An optional file path for dumping attributes of the xopt object and the\n        results of evaluations.\n    max_evaluations : int, optional\n        An optional maximum number of evaluations to perform. If set, the optimization\n        process will stop after reaching this limit.\n    data : DataFrame, optional\n        An optional DataFrame object for storing internal data related to the optimization\n        process.\n    serialize_torch : bool\n        A flag indicating whether Torch (PyTorch) models should be serialized when\n        saving them.\n    serialize_inline : bool\n        A flag indicating whether Torch models should be stored via binary string\n        directly inside the main configuration file.\n\n    Methods\n    -------\n    step()\n        Executes one optimization cycle, generating candidates, submitting them for\n        evaluation, waiting for evaluation results, and updating data storage.\n    run()\n        Runs the optimization process until the specified stopping criteria are met,\n        such as reaching the maximum number of evaluations.\n    evaluate(input_dict: Dict)\n        Evaluates a candidate without storing data.\n    evaluate_data(input_data)\n        Evaluates a set of candidates, adding the results to the internal DataFrame.\n    add_data(new_data)\n        Adds new data to the internal DataFrame and the generator's data.\n    reset_data()\n        Resets the internal data by clearing the DataFrame.\n    random_evaluate(n_samples=1, seed=None, **kwargs)\n        Generates random inputs using the VOCS and evaluates them, adding the data to\n        Xopt.\n    yaml(**kwargs)\n        Serializes the Xopt configuration to a YAML string.\n    dump(file: str = None, **kwargs)\n        Dumps the Xopt configuration to a specified file.\n    dict(**kwargs) -&gt; Dict\n        Provides a custom dictionary representation of the Xopt configuration.\n    json(**kwargs) -&gt; str\n        Serializes the Xopt configuration to a JSON string.\n    \"\"\"\n\n    vocs: VOCS = Field(description=\"VOCS object for Xopt\")\n    generator: SerializeAsAny[Generator] = Field(\n        description=\"generator object for Xopt\"\n    )\n    evaluator: SerializeAsAny[Evaluator] = Field(\n        description=\"evaluator object for Xopt\"\n    )\n    strict: bool = Field(\n        True,\n        description=\"flag to indicate if exceptions raised during evaluation \"\n        \"should stop Xopt\",\n    )\n    dump_file: Optional[str] = Field(\n        None, description=\"file to dump the results of the evaluations\"\n    )\n    max_evaluations: Optional[int] = Field(\n        None, description=\"maximum number of evaluations to perform\"\n    )\n    data: Optional[DataFrame] = Field(None, description=\"internal DataFrame object\")\n    serialize_torch: bool = Field(\n        False,\n        description=\"flag to indicate that torch models should be serialized \"\n        \"when dumping\",\n    )\n    serialize_inline: bool = Field(\n        False,\n        description=\"flag to indicate if torch models\"\n        \" should be stored inside main config file\",\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_model(cls, data: Any):\n        if isinstance(data, dict):\n            # validate vocs\n            if isinstance(data[\"vocs\"], dict):\n                data[\"vocs\"] = VOCS(**data[\"vocs\"])\n\n            # validate generator\n            if isinstance(data[\"generator\"], dict):\n                name = data[\"generator\"].pop(\"name\")\n                generator_class = get_generator(name)\n                data[\"generator\"] = generator_class.model_validate(\n                    {**data[\"generator\"], \"vocs\": data[\"vocs\"]}\n                )\n            elif isinstance(data[\"generator\"], str):\n                generator_class = get_generator(data[\"generator\"])\n\n                data[\"generator\"] = generator_class.model_validate(\n                    {\"vocs\": data[\"vocs\"]}\n                )\n\n        return data\n\n    @field_validator(\"evaluator\", mode=\"before\")\n    def validate_evaluator(cls, value):\n        if isinstance(value, dict):\n            value = Evaluator(**value)\n\n        return value\n\n    @field_validator(\"data\", mode=\"before\")\n    def validate_data(cls, v, info: ValidationInfo):\n        if isinstance(v, dict):\n            try:\n                v = pd.DataFrame(v)\n                v.index = v.index.astype(np.int64)\n                v = v.sort_index()\n            except IndexError:\n                v = pd.DataFrame(v, index=[0])\n        elif isinstance(v, DataFrame):\n            if not v.index.is_integer():\n                raise ValueError(\"dataframe index must be integer\")\n        # also add data to generator\n        # TODO: find a more robust way of doing this\n        info.data[\"generator\"].add_data(v)\n\n        return v\n\n    @property\n    def n_data(self) -&gt; int:\n        if self.data is None:\n            return 0\n        else:\n            return len(self.data)\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize Xopt.\n\n        Parameters\n        ----------\n        args : tuple\n            Positional arguments; a single YAML string can be passed as the only argument\n            to initialize Xopt.\n        kwargs : dict\n            Keyword arguments for initializing Xopt.\n\n        Raises\n        ------\n        ValueError\n            If both a YAML string and keyword arguments are specified during\n            initialization.\n            If more than one positional argument is provided.\n\n        Notes\n        -----\n        - If a single YAML string is provided in the `args` argument, it is deserialized\n          into keyword arguments using `yaml.safe_load`.\n        - When using the YAML string for initialization, no additional keyword arguments\n          are allowed.\n\n        \"\"\"\n        if len(args) == 1:\n            if len(kwargs) &gt; 0:\n                raise ValueError(\"cannot specify yaml string and kwargs for Xopt init\")\n            super().__init__(**yaml.safe_load(args[0]))\n        elif len(args) &gt; 1:\n            raise ValueError(\n                \"arguments to Xopt must be either a single yaml string \"\n                \"or a keyword arguments passed directly to pydantic\"\n            )\n        else:\n            super().__init__(**kwargs)\n\n    def step(self):\n        \"\"\"\n        Run one optimization cycle.\n\n        This method performs the following steps:\n        - Determines the number of candidates to request from the generator.\n        - Passes the candidate request to the generator.\n        - Submits candidates to the evaluator.\n        - Waits until all evaluations are finished\n        - Updates data storage and generator data storage (if applicable).\n\n        \"\"\"\n        logger.info(\"Running Xopt step\")\n\n        # get number of candidates to generate\n        n_generate = self.evaluator.max_workers\n\n        # generate samples and submit to evaluator\n        logger.debug(f\"Generating {n_generate} candidates\")\n        new_samples = self.generator.generate(n_generate)\n\n        if new_samples is not None:\n            # Evaluate data\n            self.evaluate_data(new_samples)\n\n    def run(self):\n        \"\"\"\n        Run until the maximum number of evaluations is reached or the generator is done.\n\n        \"\"\"\n        while not self.generator.is_done:\n            # Stopping criteria\n            if self.max_evaluations is not None:\n                if self.n_data &gt;= self.max_evaluations:\n                    logger.info(\n                        \"Xopt is done. \"\n                        f\"Max evaluations {self.max_evaluations} reached.\"\n                    )\n                    break\n\n            self.step()\n\n    def evaluate(self, input_dict: Dict):\n        \"\"\"\n        Evaluate a candidate without storing data.\n\n        Parameters\n        ----------\n        input_dict : Dict\n            A dictionary representing the input data for candidate evaluation.\n\n        Returns\n        -------\n        Any\n            The result of the evaluation.\n\n        \"\"\"\n        inputs = deepcopy(input_dict)\n\n        # add constants to input data\n        for name, value in self.vocs.constants.items():\n            inputs[name] = value\n\n        self.vocs.validate_input_data(DataFrame(inputs, index=[0]))\n        return self.evaluator.evaluate(input_dict)\n\n    def evaluate_data(\n        self,\n        input_data: Union[\n            pd.DataFrame,\n            List[Dict[str, float]],\n            Dict[str, List[float]],\n            Dict[str, float],\n        ],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Evaluate data using the evaluator and wait for results.\n\n        This method evaluates a set of candidates and adds the results to the internal\n        DataFrame.\n\n        Parameters\n        ----------\n        input_data : Union[pd.DataFrame, List[Dict[str, float], Dict[str, List[float],\n                        Dict[str, float]]]\n            The input data for evaluation, which can be provided as a DataFrame, a list of\n            dictionaries, or a single dictionary.\n\n        Returns\n        -------\n        pd.DataFrame\n            The results of the evaluations added to the internal DataFrame.\n\n        \"\"\"\n        # translate input data into pandas dataframes\n        if not isinstance(input_data, DataFrame):\n            try:\n                input_data = DataFrame(deepcopy(input_data))\n            except ValueError:\n                input_data = DataFrame(deepcopy(input_data), index=[0])\n\n        logger.debug(f\"Evaluating {len(input_data)} inputs\")\n        self.vocs.validate_input_data(input_data)\n\n        # add constants to input data\n        for name, value in self.vocs.constants.items():\n            input_data[name] = value\n\n        output_data = self.evaluator.evaluate_data(input_data)\n\n        if self.strict:\n            validate_outputs(output_data)\n\n        # explode any list like results if all the output names exist\n        output_data = explode_all_columns(output_data)\n\n        self.add_data(output_data)\n\n        # dump data to file if specified\n        if self.dump_file is not None:\n            self.dump()\n\n        return output_data\n\n    def add_data(self, new_data: pd.DataFrame):\n        \"\"\"\n        Concatenate new data to the internal DataFrame and add it to the generator's\n        data.\n\n        Parameters\n        ----------\n        new_data : pd.DataFrame\n            New data to be added to the internal DataFrame.\n\n        \"\"\"\n        logger.debug(f\"Adding {len(new_data)} new data to internal dataframes\")\n\n        # Set internal dataframe.\n        if self.data is not None:\n            new_data = pd.DataFrame(new_data, copy=True)  # copy for reindexing\n            new_data.index = np.arange(len(self.data), len(self.data) + len(new_data))\n\n            self.data = pd.concat([self.data, new_data], axis=0)\n        else:\n            if new_data.index.dtype != np.int64:\n                new_data.index = new_data.index.astype(np.int64)\n            self.data = new_data\n        self.generator.add_data(new_data)\n\n    def reset_data(self):\n        \"\"\"\n        Reset the internal data by clearing the DataFrame.\n\n        \"\"\"\n        self.data = pd.DataFrame()\n        self.generator.data = pd.DataFrame()\n\n    def remove_data(\n        self, indices: list[int], inplace: bool = True\n    ) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Removes data from the `X.data` data storage attribute.\n\n        Parameters\n        ----------\n        indices: list of integers\n            List of indices specifying the rows (steps) to remove from data.\n\n        inplace: boolean, optional\n            Whether to update data inplace. If False, returns a copy.\n\n        Returns\n        -------\n        pd.DataFrame or None\n            A copy of the internal DataFrame with the specified rows removed\n            or None if inplace is True.\n\n        \"\"\"\n        new_data = self.data.drop(labels=indices)\n        new_data.index = np.arange(len(new_data), dtype=np.int64)\n        if inplace:\n            self.data = new_data\n            self.generator.data = new_data\n        else:\n            return new_data\n\n    def random_evaluate(self, n_samples=1, seed=None, **kwargs):\n        \"\"\"\n        Convenience method to generate random inputs using VOCs and evaluate them.\n\n        This method generates random inputs using the Variables, Objectives,\n        Constraints, and Statics (VOCS) and evaluates them, adding the data to the\n        Xopt object and generator.\n\n        Parameters\n        ----------\n        n_samples : int, optional\n            The number of random samples to generate.\n        seed : int, optional\n            The random seed for reproducibility.\n        **kwargs\n            Additional keyword arguments for generating random inputs.\n\n        Returns\n        -------\n        pd.DataFrame\n            The results of the evaluations added to the internal DataFrame.\n\n        \"\"\"\n        random_inputs = self.vocs.random_inputs(n_samples, seed=seed, **kwargs)\n        result = self.evaluate_data(random_inputs)\n        return result\n\n    def yaml(self, **kwargs):\n        \"\"\"\n        Serialize the Xopt configuration to a YAML string.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments for customizing serialization.\n\n        Returns\n        -------\n        str\n            The Xopt configuration serialized as a YAML string.\n\n        \"\"\"\n        output = json.loads(\n            self.json(\n                serialize_torch=self.serialize_torch,\n                serialize_inline=self.serialize_inline,\n                **kwargs,\n            )\n        )\n        return yaml.dump(output)\n\n    def dump(self, file: str = None, **kwargs):\n        \"\"\"\n        Dump data to a file.\n\n        Parameters\n        ----------\n        file : str, optional\n            The path to the file where the Xopt configuration will be dumped.\n        **kwargs\n            Additional keyword arguments for customizing the dump.\n\n        Raises\n        ------\n        ValueError\n            If no dump file is specified via argument or in the `dump_file` attribute.\n\n        \"\"\"\n        fname = file if file is not None else self.dump_file\n\n        if fname is None:\n            raise ValueError(\n                \"no dump file specified via argument or in `dump_file` attribute\"\n            )\n        else:\n            with open(fname, \"w\") as f:\n                f.write(self.yaml(**kwargs))\n            logger.debug(f\"Dumped state to YAML file: {fname}\")\n\n    def dict(self, **kwargs) -&gt; Dict:\n        \"\"\"\n        Handle custom dictionary generation.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments for customizing the dictionary generation.\n\n        Returns\n        -------\n        Dict\n            A dictionary representation of the Xopt configuration.\n\n        \"\"\"\n        result = super().model_dump(**kwargs)\n        result[\"generator\"] = {\"name\": self.generator.name} | result[\"generator\"]\n        return result\n\n    def json(self, **kwargs) -&gt; str:\n        \"\"\"\n        Handle custom serialization of generators and DataFrames.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments for customizing serialization.\n\n        Returns\n        -------\n        str\n            The Xopt configuration serialized as a JSON string.\n\n        \"\"\"\n        result = super().to_json(**kwargs)\n        dict_result = json.loads(result)\n        dict_result[\"generator\"] = {\"name\": self.generator.name} | dict_result[\n            \"generator\"\n        ]\n        dict_result[\"data\"] = (\n            json.loads(self.data.to_json()) if self.data is not None else None\n        )\n\n        # TODO: implement version checking\n        # dict_result[\"xopt_version\"] = __version__\n\n        return json.dumps(dict_result)\n\n    def __repr__(self):\n        \"\"\"\n        Return information about the Xopt object, including the YAML representation\n        without data.\n\n        Returns\n        -------\n        str\n            A string representation of the Xopt object.\n\n        \"\"\"\n\n        # get dict minus data\n        config = json.loads(self.json())\n        config.pop(\"data\")\n        return f\"\"\"\n            Xopt\n________________________________\nVersion: {__version__}\nData size: {self.n_data}\nConfig as YAML:\n{yaml.dump(config)}\n\"\"\"\n\n    def __str__(self):\n        \"\"\"\n        Return a string representation of the Xopt object.\n\n        Returns\n        -------\n        str\n            A string representation of the Xopt object.\n\n        \"\"\"\n        return self.__repr__()\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize Xopt.</p>"},{"location":"api/xopt/#xopt.Xopt.__init__--parameters","title":"Parameters","text":"<p>args : tuple     Positional arguments; a single YAML string can be passed as the only argument     to initialize Xopt. kwargs : dict     Keyword arguments for initializing Xopt.</p>"},{"location":"api/xopt/#xopt.Xopt.__init__--raises","title":"Raises","text":"<p>ValueError     If both a YAML string and keyword arguments are specified during     initialization.     If more than one positional argument is provided.</p>"},{"location":"api/xopt/#xopt.Xopt.__init__--notes","title":"Notes","text":"<ul> <li>If a single YAML string is provided in the <code>args</code> argument, it is deserialized   into keyword arguments using <code>yaml.safe_load</code>.</li> <li>When using the YAML string for initialization, no additional keyword arguments   are allowed.</li> </ul> Source code in <code>xopt/base.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize Xopt.\n\n    Parameters\n    ----------\n    args : tuple\n        Positional arguments; a single YAML string can be passed as the only argument\n        to initialize Xopt.\n    kwargs : dict\n        Keyword arguments for initializing Xopt.\n\n    Raises\n    ------\n    ValueError\n        If both a YAML string and keyword arguments are specified during\n        initialization.\n        If more than one positional argument is provided.\n\n    Notes\n    -----\n    - If a single YAML string is provided in the `args` argument, it is deserialized\n      into keyword arguments using `yaml.safe_load`.\n    - When using the YAML string for initialization, no additional keyword arguments\n      are allowed.\n\n    \"\"\"\n    if len(args) == 1:\n        if len(kwargs) &gt; 0:\n            raise ValueError(\"cannot specify yaml string and kwargs for Xopt init\")\n        super().__init__(**yaml.safe_load(args[0]))\n    elif len(args) &gt; 1:\n        raise ValueError(\n            \"arguments to Xopt must be either a single yaml string \"\n            \"or a keyword arguments passed directly to pydantic\"\n        )\n    else:\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.__repr__","title":"<code>__repr__()</code>","text":"<p>Return information about the Xopt object, including the YAML representation without data.</p>"},{"location":"api/xopt/#xopt.Xopt.__repr__--returns","title":"Returns","text":"<p>str     A string representation of the Xopt object.</p> Source code in <code>xopt/base.py</code> <pre><code>    def __repr__(self):\n        \"\"\"\n        Return information about the Xopt object, including the YAML representation\n        without data.\n\n        Returns\n        -------\n        str\n            A string representation of the Xopt object.\n\n        \"\"\"\n\n        # get dict minus data\n        config = json.loads(self.json())\n        config.pop(\"data\")\n        return f\"\"\"\n            Xopt\n________________________________\nVersion: {__version__}\nData size: {self.n_data}\nConfig as YAML:\n{yaml.dump(config)}\n\"\"\"\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the Xopt object.</p>"},{"location":"api/xopt/#xopt.Xopt.__str__--returns","title":"Returns","text":"<p>str     A string representation of the Xopt object.</p> Source code in <code>xopt/base.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    Return a string representation of the Xopt object.\n\n    Returns\n    -------\n    str\n        A string representation of the Xopt object.\n\n    \"\"\"\n    return self.__repr__()\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.add_data","title":"<code>add_data(new_data)</code>","text":"<p>Concatenate new data to the internal DataFrame and add it to the generator's data.</p>"},{"location":"api/xopt/#xopt.Xopt.add_data--parameters","title":"Parameters","text":"<p>new_data : pd.DataFrame     New data to be added to the internal DataFrame.</p> Source code in <code>xopt/base.py</code> <pre><code>def add_data(self, new_data: pd.DataFrame):\n    \"\"\"\n    Concatenate new data to the internal DataFrame and add it to the generator's\n    data.\n\n    Parameters\n    ----------\n    new_data : pd.DataFrame\n        New data to be added to the internal DataFrame.\n\n    \"\"\"\n    logger.debug(f\"Adding {len(new_data)} new data to internal dataframes\")\n\n    # Set internal dataframe.\n    if self.data is not None:\n        new_data = pd.DataFrame(new_data, copy=True)  # copy for reindexing\n        new_data.index = np.arange(len(self.data), len(self.data) + len(new_data))\n\n        self.data = pd.concat([self.data, new_data], axis=0)\n    else:\n        if new_data.index.dtype != np.int64:\n            new_data.index = new_data.index.astype(np.int64)\n        self.data = new_data\n    self.generator.add_data(new_data)\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.dict","title":"<code>dict(**kwargs)</code>","text":"<p>Handle custom dictionary generation.</p>"},{"location":"api/xopt/#xopt.Xopt.dict--parameters","title":"Parameters","text":"<p>**kwargs     Additional keyword arguments for customizing the dictionary generation.</p>"},{"location":"api/xopt/#xopt.Xopt.dict--returns","title":"Returns","text":"<p>Dict     A dictionary representation of the Xopt configuration.</p> Source code in <code>xopt/base.py</code> <pre><code>def dict(self, **kwargs) -&gt; Dict:\n    \"\"\"\n    Handle custom dictionary generation.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments for customizing the dictionary generation.\n\n    Returns\n    -------\n    Dict\n        A dictionary representation of the Xopt configuration.\n\n    \"\"\"\n    result = super().model_dump(**kwargs)\n    result[\"generator\"] = {\"name\": self.generator.name} | result[\"generator\"]\n    return result\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.dump","title":"<code>dump(file=None, **kwargs)</code>","text":"<p>Dump data to a file.</p>"},{"location":"api/xopt/#xopt.Xopt.dump--parameters","title":"Parameters","text":"<p>file : str, optional     The path to the file where the Xopt configuration will be dumped. **kwargs     Additional keyword arguments for customizing the dump.</p>"},{"location":"api/xopt/#xopt.Xopt.dump--raises","title":"Raises","text":"<p>ValueError     If no dump file is specified via argument or in the <code>dump_file</code> attribute.</p> Source code in <code>xopt/base.py</code> <pre><code>def dump(self, file: str = None, **kwargs):\n    \"\"\"\n    Dump data to a file.\n\n    Parameters\n    ----------\n    file : str, optional\n        The path to the file where the Xopt configuration will be dumped.\n    **kwargs\n        Additional keyword arguments for customizing the dump.\n\n    Raises\n    ------\n    ValueError\n        If no dump file is specified via argument or in the `dump_file` attribute.\n\n    \"\"\"\n    fname = file if file is not None else self.dump_file\n\n    if fname is None:\n        raise ValueError(\n            \"no dump file specified via argument or in `dump_file` attribute\"\n        )\n    else:\n        with open(fname, \"w\") as f:\n            f.write(self.yaml(**kwargs))\n        logger.debug(f\"Dumped state to YAML file: {fname}\")\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.evaluate","title":"<code>evaluate(input_dict)</code>","text":"<p>Evaluate a candidate without storing data.</p>"},{"location":"api/xopt/#xopt.Xopt.evaluate--parameters","title":"Parameters","text":"<p>input_dict : Dict     A dictionary representing the input data for candidate evaluation.</p>"},{"location":"api/xopt/#xopt.Xopt.evaluate--returns","title":"Returns","text":"<p>Any     The result of the evaluation.</p> Source code in <code>xopt/base.py</code> <pre><code>def evaluate(self, input_dict: Dict):\n    \"\"\"\n    Evaluate a candidate without storing data.\n\n    Parameters\n    ----------\n    input_dict : Dict\n        A dictionary representing the input data for candidate evaluation.\n\n    Returns\n    -------\n    Any\n        The result of the evaluation.\n\n    \"\"\"\n    inputs = deepcopy(input_dict)\n\n    # add constants to input data\n    for name, value in self.vocs.constants.items():\n        inputs[name] = value\n\n    self.vocs.validate_input_data(DataFrame(inputs, index=[0]))\n    return self.evaluator.evaluate(input_dict)\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.evaluate_data","title":"<code>evaluate_data(input_data)</code>","text":"<p>Evaluate data using the evaluator and wait for results.</p> <p>This method evaluates a set of candidates and adds the results to the internal DataFrame.</p>"},{"location":"api/xopt/#xopt.Xopt.evaluate_data--parameters","title":"Parameters","text":"<p>input_data : Union[pd.DataFrame, List[Dict[str, float], Dict[str, List[float],                 Dict[str, float]]]     The input data for evaluation, which can be provided as a DataFrame, a list of     dictionaries, or a single dictionary.</p>"},{"location":"api/xopt/#xopt.Xopt.evaluate_data--returns","title":"Returns","text":"<p>pd.DataFrame     The results of the evaluations added to the internal DataFrame.</p> Source code in <code>xopt/base.py</code> <pre><code>def evaluate_data(\n    self,\n    input_data: Union[\n        pd.DataFrame,\n        List[Dict[str, float]],\n        Dict[str, List[float]],\n        Dict[str, float],\n    ],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Evaluate data using the evaluator and wait for results.\n\n    This method evaluates a set of candidates and adds the results to the internal\n    DataFrame.\n\n    Parameters\n    ----------\n    input_data : Union[pd.DataFrame, List[Dict[str, float], Dict[str, List[float],\n                    Dict[str, float]]]\n        The input data for evaluation, which can be provided as a DataFrame, a list of\n        dictionaries, or a single dictionary.\n\n    Returns\n    -------\n    pd.DataFrame\n        The results of the evaluations added to the internal DataFrame.\n\n    \"\"\"\n    # translate input data into pandas dataframes\n    if not isinstance(input_data, DataFrame):\n        try:\n            input_data = DataFrame(deepcopy(input_data))\n        except ValueError:\n            input_data = DataFrame(deepcopy(input_data), index=[0])\n\n    logger.debug(f\"Evaluating {len(input_data)} inputs\")\n    self.vocs.validate_input_data(input_data)\n\n    # add constants to input data\n    for name, value in self.vocs.constants.items():\n        input_data[name] = value\n\n    output_data = self.evaluator.evaluate_data(input_data)\n\n    if self.strict:\n        validate_outputs(output_data)\n\n    # explode any list like results if all the output names exist\n    output_data = explode_all_columns(output_data)\n\n    self.add_data(output_data)\n\n    # dump data to file if specified\n    if self.dump_file is not None:\n        self.dump()\n\n    return output_data\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.json","title":"<code>json(**kwargs)</code>","text":"<p>Handle custom serialization of generators and DataFrames.</p>"},{"location":"api/xopt/#xopt.Xopt.json--parameters","title":"Parameters","text":"<p>**kwargs     Additional keyword arguments for customizing serialization.</p>"},{"location":"api/xopt/#xopt.Xopt.json--returns","title":"Returns","text":"<p>str     The Xopt configuration serialized as a JSON string.</p> Source code in <code>xopt/base.py</code> <pre><code>def json(self, **kwargs) -&gt; str:\n    \"\"\"\n    Handle custom serialization of generators and DataFrames.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments for customizing serialization.\n\n    Returns\n    -------\n    str\n        The Xopt configuration serialized as a JSON string.\n\n    \"\"\"\n    result = super().to_json(**kwargs)\n    dict_result = json.loads(result)\n    dict_result[\"generator\"] = {\"name\": self.generator.name} | dict_result[\n        \"generator\"\n    ]\n    dict_result[\"data\"] = (\n        json.loads(self.data.to_json()) if self.data is not None else None\n    )\n\n    # TODO: implement version checking\n    # dict_result[\"xopt_version\"] = __version__\n\n    return json.dumps(dict_result)\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.random_evaluate","title":"<code>random_evaluate(n_samples=1, seed=None, **kwargs)</code>","text":"<p>Convenience method to generate random inputs using VOCs and evaluate them.</p> <p>This method generates random inputs using the Variables, Objectives, Constraints, and Statics (VOCS) and evaluates them, adding the data to the Xopt object and generator.</p>"},{"location":"api/xopt/#xopt.Xopt.random_evaluate--parameters","title":"Parameters","text":"<p>n_samples : int, optional     The number of random samples to generate. seed : int, optional     The random seed for reproducibility. **kwargs     Additional keyword arguments for generating random inputs.</p>"},{"location":"api/xopt/#xopt.Xopt.random_evaluate--returns","title":"Returns","text":"<p>pd.DataFrame     The results of the evaluations added to the internal DataFrame.</p> Source code in <code>xopt/base.py</code> <pre><code>def random_evaluate(self, n_samples=1, seed=None, **kwargs):\n    \"\"\"\n    Convenience method to generate random inputs using VOCs and evaluate them.\n\n    This method generates random inputs using the Variables, Objectives,\n    Constraints, and Statics (VOCS) and evaluates them, adding the data to the\n    Xopt object and generator.\n\n    Parameters\n    ----------\n    n_samples : int, optional\n        The number of random samples to generate.\n    seed : int, optional\n        The random seed for reproducibility.\n    **kwargs\n        Additional keyword arguments for generating random inputs.\n\n    Returns\n    -------\n    pd.DataFrame\n        The results of the evaluations added to the internal DataFrame.\n\n    \"\"\"\n    random_inputs = self.vocs.random_inputs(n_samples, seed=seed, **kwargs)\n    result = self.evaluate_data(random_inputs)\n    return result\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.remove_data","title":"<code>remove_data(indices, inplace=True)</code>","text":"<p>Removes data from the <code>X.data</code> data storage attribute.</p>"},{"location":"api/xopt/#xopt.Xopt.remove_data--parameters","title":"Parameters","text":"<p>indices: list of integers     List of indices specifying the rows (steps) to remove from data.</p> boolean, optional <p>Whether to update data inplace. If False, returns a copy.</p>"},{"location":"api/xopt/#xopt.Xopt.remove_data--returns","title":"Returns","text":"<p>pd.DataFrame or None     A copy of the internal DataFrame with the specified rows removed     or None if inplace is True.</p> Source code in <code>xopt/base.py</code> <pre><code>def remove_data(\n    self, indices: list[int], inplace: bool = True\n) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Removes data from the `X.data` data storage attribute.\n\n    Parameters\n    ----------\n    indices: list of integers\n        List of indices specifying the rows (steps) to remove from data.\n\n    inplace: boolean, optional\n        Whether to update data inplace. If False, returns a copy.\n\n    Returns\n    -------\n    pd.DataFrame or None\n        A copy of the internal DataFrame with the specified rows removed\n        or None if inplace is True.\n\n    \"\"\"\n    new_data = self.data.drop(labels=indices)\n    new_data.index = np.arange(len(new_data), dtype=np.int64)\n    if inplace:\n        self.data = new_data\n        self.generator.data = new_data\n    else:\n        return new_data\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.reset_data","title":"<code>reset_data()</code>","text":"<p>Reset the internal data by clearing the DataFrame.</p> Source code in <code>xopt/base.py</code> <pre><code>def reset_data(self):\n    \"\"\"\n    Reset the internal data by clearing the DataFrame.\n\n    \"\"\"\n    self.data = pd.DataFrame()\n    self.generator.data = pd.DataFrame()\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.run","title":"<code>run()</code>","text":"<p>Run until the maximum number of evaluations is reached or the generator is done.</p> Source code in <code>xopt/base.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run until the maximum number of evaluations is reached or the generator is done.\n\n    \"\"\"\n    while not self.generator.is_done:\n        # Stopping criteria\n        if self.max_evaluations is not None:\n            if self.n_data &gt;= self.max_evaluations:\n                logger.info(\n                    \"Xopt is done. \"\n                    f\"Max evaluations {self.max_evaluations} reached.\"\n                )\n                break\n\n        self.step()\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.step","title":"<code>step()</code>","text":"<p>Run one optimization cycle.</p> <p>This method performs the following steps: - Determines the number of candidates to request from the generator. - Passes the candidate request to the generator. - Submits candidates to the evaluator. - Waits until all evaluations are finished - Updates data storage and generator data storage (if applicable).</p> Source code in <code>xopt/base.py</code> <pre><code>def step(self):\n    \"\"\"\n    Run one optimization cycle.\n\n    This method performs the following steps:\n    - Determines the number of candidates to request from the generator.\n    - Passes the candidate request to the generator.\n    - Submits candidates to the evaluator.\n    - Waits until all evaluations are finished\n    - Updates data storage and generator data storage (if applicable).\n\n    \"\"\"\n    logger.info(\"Running Xopt step\")\n\n    # get number of candidates to generate\n    n_generate = self.evaluator.max_workers\n\n    # generate samples and submit to evaluator\n    logger.debug(f\"Generating {n_generate} candidates\")\n    new_samples = self.generator.generate(n_generate)\n\n    if new_samples is not None:\n        # Evaluate data\n        self.evaluate_data(new_samples)\n</code></pre>"},{"location":"api/xopt/#xopt.Xopt.yaml","title":"<code>yaml(**kwargs)</code>","text":"<p>Serialize the Xopt configuration to a YAML string.</p>"},{"location":"api/xopt/#xopt.Xopt.yaml--parameters","title":"Parameters","text":"<p>**kwargs     Additional keyword arguments for customizing serialization.</p>"},{"location":"api/xopt/#xopt.Xopt.yaml--returns","title":"Returns","text":"<p>str     The Xopt configuration serialized as a YAML string.</p> Source code in <code>xopt/base.py</code> <pre><code>def yaml(self, **kwargs):\n    \"\"\"\n    Serialize the Xopt configuration to a YAML string.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments for customizing serialization.\n\n    Returns\n    -------\n    str\n        The Xopt configuration serialized as a YAML string.\n\n    \"\"\"\n    output = json.loads(\n        self.json(\n            serialize_torch=self.serialize_torch,\n            serialize_inline=self.serialize_inline,\n            **kwargs,\n        )\n    )\n    return yaml.dump(output)\n</code></pre>"},{"location":"api/generators/bayesian/","title":"Bayesian generators","text":"<p>             Bases: <code>Generator</code>, <code>ABC</code></p> <p>Bayesian Generator for Bayesian Optimization.</p> <p>             Bases: <code>BayesianGenerator</code></p> Source code in <code>xopt/generators/bayesian/bayesian_exploration.py</code> <pre><code>class BayesianExplorationGenerator(BayesianGenerator):\n    name = \"bayesian_exploration\"\n    supports_batch_generation: bool = True\n\n    __doc__ = \"Bayesian exploration generator\\n\" + formatted_base_docstring()\n\n    @field_validator(\"vocs\", mode=\"after\")\n    def validate_vocs(cls, v, info: ValidationInfo):\n        if v.n_objectives != 0:\n            raise ValueError(\"this generator only supports observables\")\n        return v\n\n    def _get_acquisition(self, model):\n        sampler = self._get_sampler(model)\n        qPV = qPosteriorVariance(\n            model,\n            sampler=sampler,\n            objective=self._get_objective(),\n        )\n\n        return qPV\n\n    def _get_objective(self):\n        \"\"\"return exploration objective, which only captures the output of the first\n        model output\"\"\"\n\n        return create_exploration_objective(self.vocs, self._tkwargs)\n</code></pre> <p>             Bases: <code>MultiObjectiveBayesianGenerator</code></p> Source code in <code>xopt/generators/bayesian/mobo.py</code> <pre><code>class MOBOGenerator(MultiObjectiveBayesianGenerator):\n    name = \"mobo\"\n    __doc__ = \"\"\"Implements Multi-Objective Bayesian Optimization using the Expected\n            Hypervolume Improvement acquisition function\"\"\"\n\n    def _get_objective(self):\n        return create_mobo_objective(self.vocs, self._tkwargs)\n\n    def get_acquisition(self, model):\n        \"\"\"\n        Returns a function that can be used to evaluate the acquisition function\n        \"\"\"\n        if model is None:\n            raise ValueError(\"model cannot be None\")\n\n        # get base acquisition function\n        acq = self._get_acquisition(model)\n\n        # apply fixed features if specified in the generator\n        if self.fixed_features is not None:\n            # get input dim\n            dim = len(self.model_input_names)\n            columns = []\n            values = []\n            for name, value in self.fixed_features.items():\n                columns += [self.model_input_names.index(name)]\n                values += [value]\n\n            acq = FixedFeatureAcquisitionFunction(\n                acq_function=acq, d=dim, columns=columns, values=values\n            )\n\n        return acq\n\n    def _get_acquisition(self, model):\n        inputs = self.get_input_data(self.data)\n        sampler = self._get_sampler(model)\n\n        if self.log_transform_acquisition_function:\n            acqclass = qLogNoisyExpectedHypervolumeImprovement\n        else:\n            acqclass = qNoisyExpectedHypervolumeImprovement\n\n        acq = acqclass(\n            model,\n            X_baseline=inputs,\n            constraints=self._get_constraint_callables(),\n            ref_point=self.torch_reference_point,\n            sampler=sampler,\n            objective=self._get_objective(),\n            cache_root=False,\n            prune_baseline=True,\n        )\n\n        return acq\n</code></pre> <p>             Bases: <code>BayesianGenerator</code></p> Source code in <code>xopt/generators/bayesian/upper_confidence_bound.py</code> <pre><code>class UpperConfidenceBoundGenerator(BayesianGenerator):\n    name = \"upper_confidence_bound\"\n    beta: float = Field(2.0, description=\"Beta parameter for UCB optimization\")\n    supports_batch_generation: bool = True\n    __doc__ = (\n        \"\"\"Bayesian optimization generator using Upper Confidence Bound\n\nAttributes\n----------\nbeta : float, default 2.0\n    Beta parameter for UCB optimization, controlling the trade-off between exploration\n    and exploitation. Higher values of beta prioritize exploration.\n\n    \"\"\"\n        + formatted_base_docstring()\n    )\n\n    @field_validator(\"vocs\")\n    def validate_vocs_without_constraints(cls, v):\n        if v.constraints:\n            warnings.warn(\n                f\"Using {cls.__name__} with constraints may lead to numerical issues if the base acquisition \"\n                f\"function has negative values.\"\n            )\n        return v\n\n    @field_validator(\"log_transform_acquisition_function\")\n    def validate_log_transform_acquisition_function(cls, v):\n        if v:\n            raise ValueError(\n                \"Log transform cannot be applied to potentially negative UCB \"\n                \"acquisition function.\"\n            )\n\n    def _get_acquisition(self, model):\n        if self.n_candidates &gt; 1:\n            # MC sampling for generating multiple candidate points\n            sampler = self._get_sampler(model)\n            acq = qUpperConfidenceBound(\n                model,\n                sampler=sampler,\n                objective=self._get_objective(),\n                beta=self.beta,\n            )\n        else:\n            # analytic acquisition function for single candidate generation\n            weights = torch.zeros(self.vocs.n_outputs).to(**self._tkwargs)\n            weights = set_botorch_weights(weights, self.vocs)\n            posterior_transform = ScalarizedPosteriorTransform(weights)\n            acq = UpperConfidenceBound(\n                model, beta=self.beta, posterior_transform=posterior_transform\n            )\n\n        return acq\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator--attributes","title":"Attributes:","text":"<p>name : str     The name of the Bayesian Generator.</p> Optional[Model] <p>The BoTorch model used by the generator to perform optimization.</p> int <p>The number of Monte Carlo samples to use in the optimization process.</p> SerializeAsAny[Optional[TurboController]] <p>The Turbo Controller for trust-region Bayesian Optimization.</p> bool <p>A flag to enable or disable CUDA usage if available.</p> SerializeAsAny[ModelConstructor] <p>The constructor used to generate the model for Bayesian Optimization.</p> SerializeAsAny[NumericalOptimizer] <p>The optimizer used to optimize the acquisition function in Bayesian Optimization.</p> Optional[List[float]] <p>The limits for travel distances between points in normalized space.</p> Optional[Dict[str, float]] <p>The fixed features used in Bayesian Optimization.</p> Optional[pd.DataFrame] <p>A data frame tracking computation time in seconds.</p> Optional[bool] <p>Flag to determine if final acquisition function value should be log-transformed before optimization.</p> Optional[PositiveInt] <p>Number of interpolation points to generate between last observation and next observation, requires n_candidates to be 1.</p> int <p>The number of candidates to generate in each optimization step.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator--methods","title":"Methods:","text":"<p>generate(self, n_candidates: int) -&gt; List[Dict]:     Generate candidates for Bayesian Optimization.</p> <p>add_data(self, new_data: pd.DataFrame):     Add new data to the generator for Bayesian Optimization.</p> <p>train_model(self, data: pd.DataFrame = None, update_internal=True) -&gt; Module:     Train a Bayesian model for Bayesian Optimization.</p> <p>propose_candidates(self, model, n_candidates=1) -&gt; Tensor:     Propose candidates for Bayesian Optimization.</p> <p>get_input_data(self, data: pd.DataFrame) -&gt; torch.Tensor:     Get input data in torch.Tensor format.</p> <p>get_acquisition(self, model) -&gt; AcquisitionFunction:     Get the acquisition function for Bayesian Optimization.</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>class BayesianGenerator(Generator, ABC):\n    \"\"\"Bayesian Generator for Bayesian Optimization.\n\n    Attributes:\n    -----------\n    name : str\n        The name of the Bayesian Generator.\n\n    model : Optional[Model]\n        The BoTorch model used by the generator to perform optimization.\n\n    n_monte_carlo_samples : int\n        The number of Monte Carlo samples to use in the optimization process.\n\n    turbo_controller : SerializeAsAny[Optional[TurboController]]\n        The Turbo Controller for trust-region Bayesian Optimization.\n\n    use_cuda : bool\n        A flag to enable or disable CUDA usage if available.\n\n    gp_constructor : SerializeAsAny[ModelConstructor]\n        The constructor used to generate the model for Bayesian Optimization.\n\n    numerical_optimizer : SerializeAsAny[NumericalOptimizer]\n        The optimizer used to optimize the acquisition function in Bayesian Optimization.\n\n    max_travel_distances : Optional[List[float]]\n        The limits for travel distances between points in normalized space.\n\n    fixed_features : Optional[Dict[str, float]]\n        The fixed features used in Bayesian Optimization.\n\n    computation_time : Optional[pd.DataFrame]\n        A data frame tracking computation time in seconds.\n\n    log_transform_acquisition_function: Optional[bool]\n        Flag to determine if final acquisition function value should be\n        log-transformed before optimization.\n\n    n_interpolate_samples: Optional[PositiveInt]\n        Number of interpolation points to generate between last observation and next\n        observation, requires n_candidates to be 1.\n\n    n_candidates : int\n        The number of candidates to generate in each optimization step.\n\n    Methods:\n    --------\n    generate(self, n_candidates: int) -&gt; List[Dict]:\n        Generate candidates for Bayesian Optimization.\n\n    add_data(self, new_data: pd.DataFrame):\n        Add new data to the generator for Bayesian Optimization.\n\n    train_model(self, data: pd.DataFrame = None, update_internal=True) -&gt; Module:\n        Train a Bayesian model for Bayesian Optimization.\n\n    propose_candidates(self, model, n_candidates=1) -&gt; Tensor:\n        Propose candidates for Bayesian Optimization.\n\n    get_input_data(self, data: pd.DataFrame) -&gt; torch.Tensor:\n        Get input data in torch.Tensor format.\n\n    get_acquisition(self, model) -&gt; AcquisitionFunction:\n        Get the acquisition function for Bayesian Optimization.\n\n    \"\"\"\n\n    name = \"base_bayesian_generator\"\n    model: Optional[Model] = Field(\n        None, description=\"botorch model used by the generator to perform optimization\"\n    )\n    n_monte_carlo_samples: int = Field(\n        128, description=\"number of monte carlo samples to use\"\n    )\n    turbo_controller: SerializeAsAny[Optional[TurboController]] = Field(\n        default=None, description=\"turbo controller for trust-region BO\"\n    )\n    use_cuda: bool = Field(False, description=\"flag to enable cuda usage if available\")\n    gp_constructor: SerializeAsAny[ModelConstructor] = Field(\n        StandardModelConstructor(), description=\"constructor used to generate model\"\n    )\n    numerical_optimizer: SerializeAsAny[NumericalOptimizer] = Field(\n        LBFGSOptimizer(),\n        description=\"optimizer used to optimize the acquisition \" \"function\",\n    )\n    max_travel_distances: Optional[List[float]] = Field(\n        None,\n        description=\"limits for travel distance between points in normalized space\",\n    )\n    fixed_features: Optional[Dict[str, float]] = Field(\n        None, description=\"fixed features used in Bayesian optimization\"\n    )\n    computation_time: Optional[pd.DataFrame] = Field(\n        None,\n        description=\"data frame tracking computation time in seconds\",\n    )\n    log_transform_acquisition_function: Optional[bool] = Field(\n        False,\n        description=\"flag to log transform the acquisition function before optimization\",\n    )\n    n_interpolate_points: Optional[PositiveInt] = None\n\n    n_candidates: int = 1\n\n    @field_validator(\"model\", mode=\"before\")\n    def validate_torch_modules(cls, v):\n        if isinstance(v, str):\n            if v.startswith(\"base64:\"):\n                v = decode_torch_module(v)\n            elif os.path.exists(v):\n                v = torch.load(v)\n        return v\n\n    @field_validator(\"gp_constructor\", mode=\"before\")\n    def validate_gp_constructor(cls, value):\n        constructor_dict = {\"standard\": StandardModelConstructor}\n        if value is None:\n            value = StandardModelConstructor()\n        elif isinstance(value, ModelConstructor):\n            value = value\n        elif isinstance(value, str):\n            if value in constructor_dict:\n                value = constructor_dict[value]()\n            else:\n                raise ValueError(f\"{value} not found\")\n        elif isinstance(value, dict):\n            name = value.pop(\"name\")\n            if name in constructor_dict:\n                value = constructor_dict[name](**value)\n            else:\n                raise ValueError(f\"{value} not found\")\n\n        return value\n\n    @field_validator(\"numerical_optimizer\", mode=\"before\")\n    def validate_numerical_optimizer(cls, value):\n        optimizer_dict = {\"grid\": GridOptimizer, \"LBFGS\": LBFGSOptimizer}\n        if value is None:\n            value = LBFGSOptimizer()\n        elif isinstance(value, NumericalOptimizer):\n            pass\n        elif isinstance(value, str):\n            if value in optimizer_dict:\n                value = optimizer_dict[value]()\n            else:\n                raise ValueError(f\"{value} not found\")\n        elif isinstance(value, dict):\n            name = value.pop(\"name\")\n            if name in optimizer_dict:\n                value = optimizer_dict[name](**value)\n            else:\n                raise ValueError(f\"{value} not found\")\n        return value\n\n    @field_validator(\"turbo_controller\", mode=\"before\")\n    def validate_turbo_controller(cls, value, info: ValidationInfo):\n        \"\"\"note default behavior is no use of turbo\"\"\"\n        optimizer_dict = {\n            \"optimize\": OptimizeTurboController,\n            \"safety\": SafetyTurboController,\n        }\n        if isinstance(value, TurboController):\n            pass\n        elif isinstance(value, str):\n            # create turbo controller from string input\n            if value in optimizer_dict:\n                value = optimizer_dict[value](info.data[\"vocs\"])\n            else:\n                raise ValueError(\n                    f\"{value} not found, available values are \"\n                    f\"{optimizer_dict.keys()}\"\n                )\n        elif isinstance(value, dict):\n            # create turbo controller from dict input\n            if \"name\" not in value:\n                raise ValueError(\"turbo input dict needs to have a `name` attribute\")\n            name = value.pop(\"name\")\n            if name in optimizer_dict:\n                # pop unnecessary elements\n                for ele in [\"dim\"]:\n                    value.pop(ele, None)\n\n                value = optimizer_dict[name](vocs=info.data[\"vocs\"], **value)\n            else:\n                raise ValueError(\n                    f\"{value} not found, available values are \"\n                    f\"{optimizer_dict.keys()}\"\n                )\n        return value\n\n    @field_validator(\"computation_time\", mode=\"before\")\n    def validate_computation_time(cls, value):\n        if isinstance(value, dict):\n            value = pd.DataFrame(value)\n\n        return value\n\n    def add_data(self, new_data: pd.DataFrame):\n        self.data = pd.concat([self.data, new_data], axis=0)\n\n    def generate(self, n_candidates: int) -&gt; list[dict]:\n        \"\"\"\n        Generate candidates using Bayesian Optimization.\n\n        Parameters:\n        -----------\n        n_candidates : int\n            The number of candidates to generate in each optimization step.\n\n        Returns:\n        --------\n        List[Dict]\n            A list of dictionaries containing the generated candidates.\n\n        Raises:\n        -------\n        NotImplementedError\n            If the number of candidates is greater than 1, and the generator does not\n            support batch candidate generation.\n\n        RuntimeError\n            If no data is contained in the generator, the 'add_data' method should be\n            called to add data before generating candidates.\n\n        Notes:\n        ------\n        This method generates candidates for Bayesian Optimization based on the\n        provided number of candidates. It updates the internal model with the current\n        data and calculates the candidates by optimizing the acquisition function.\n        The method returns the generated candidates in the form of a list of dictionaries.\n        \"\"\"\n\n        self.n_candidates = n_candidates\n        if n_candidates &gt; 1 and not self.supports_batch_generation:\n            raise NotImplementedError(\n                \"This Bayesian algorithm does not currently support parallel candidate \"\n                \"generation\"\n            )\n\n        # if no data exists raise error\n        if self.data is None:\n            raise RuntimeError(\n                \"no data contained in generator, call `add_data` \"\n                \"method to add data, see also `Xopt.random_evaluate()`\"\n            )\n\n        else:\n            # dict to track runtimes\n            timing_results = {}\n\n            # update internal model with internal data\n            start_time = time.perf_counter()\n            model = self.train_model(self.data)\n            timing_results[\"training\"] = time.perf_counter() - start_time\n\n            # propose candidates given model\n            start_time = time.perf_counter()\n            candidates = self.propose_candidates(model, n_candidates=n_candidates)\n            timing_results[\"acquisition_optimization\"] = (\n                time.perf_counter() - start_time\n            )\n\n            # post process candidates\n            result = self._process_candidates(candidates)\n\n            # append timing results to dataframe (if it exists)\n            if self.computation_time is not None:\n                self.computation_time = pd.concat(\n                    (\n                        self.computation_time,\n                        pd.DataFrame(timing_results, index=[0]),\n                    ),\n                    ignore_index=True,\n                )\n            else:\n                self.computation_time = pd.DataFrame(timing_results, index=[0])\n\n            if self.n_interpolate_points is not None:\n                if self.n_candidates &gt; 1:\n                    raise RuntimeError(\n                        \"cannot generate interpolated points for \"\n                        \"multiple candidate generation\"\n                    )\n                else:\n                    assert len(result) == 1\n                    result = interpolate_points(\n                        pd.concat(\n                            (self.data.iloc[-1:][self.vocs.variable_names], result),\n                            axis=0,\n                            ignore_index=True,\n                        ),\n                        num_points=self.n_interpolate_points,\n                    )\n\n            return result.to_dict(\"records\")\n\n    def train_model(self, data: pd.DataFrame = None, update_internal=True) -&gt; Module:\n        \"\"\"\n        Returns a ModelListGP containing independent models for the objectives and\n        constraints\n\n        \"\"\"\n        if data is None:\n            data = self.data\n        if data.empty:\n            raise ValueError(\"no data available to build model\")\n\n        # get input bounds\n        variable_bounds = deepcopy(self.vocs.variables)\n\n        # add fixed feature bounds if requested\n        if self.fixed_features is not None:\n            # get bounds for each fixed_feature (vocs bounds take precedent)\n            for key in self.fixed_features:\n                if key not in variable_bounds:\n                    f_data = data[key]\n                    bounds = [f_data.min(), f_data.max()]\n                    if bounds[1] - bounds[0] &lt; 1e-8:\n                        bounds[1] = bounds[0] + 1e-8\n                    variable_bounds[key] = bounds\n\n        _model = self.gp_constructor.build_model(\n            self.model_input_names,\n            self.vocs.output_names,\n            data,\n            {name: variable_bounds[name] for name in self.model_input_names},\n            **self._tkwargs,\n        )\n\n        if update_internal:\n            self.model = _model\n        return _model\n\n    def propose_candidates(self, model, n_candidates=1):\n        \"\"\"\n        given a GP model, propose candidates by numerically optimizing the\n        acquisition function\n\n        \"\"\"\n        # update TurBO state if used with the last `n_candidates` points\n        if self.turbo_controller is not None:\n            self.turbo_controller.update_state(self.data, n_candidates)\n\n        # calculate optimization bounds\n        bounds = self._get_optimization_bounds()\n\n        # get acquisition function\n        acq_funct = self.get_acquisition(model)\n\n        # get candidates\n        candidates = self.numerical_optimizer.optimize(acq_funct, bounds, n_candidates)\n        return candidates\n\n    def get_input_data(self, data: pd.DataFrame) -&gt; torch.Tensor:\n        \"\"\"\n        Convert input data to a torch tensor.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            The input data in the form of a pandas DataFrame.\n\n        Returns:\n        --------\n        torch.Tensor\n            A torch tensor containing the input data.\n\n        Notes:\n        ------\n        This method takes a pandas DataFrame as input data and converts it into a\n        torch tensor. It specifically selects columns corresponding to the model's\n        input names (variables), and the resulting tensor is configured with the data\n        type and device settings from the generator.\n        \"\"\"\n        return torch.tensor(data[self.model_input_names].to_numpy(), **self._tkwargs)\n\n    def get_acquisition(self, model):\n        \"\"\"\n        Define the acquisition function based on the given GP model.\n\n        Parameters:\n        -----------\n        model : Model\n            The BoTorch model to be used for generating the acquisition function.\n\n        Returns:\n        --------\n        acqusition_function : AcqusitionFunction\n\n        Raises:\n        -------\n        ValueError\n            If the provided 'model' is None. A valid model is required to create the\n            acquisition function.\n        \"\"\"\n        if model is None:\n            raise ValueError(\"model cannot be None\")\n\n        # get base acquisition function\n        acq = self._get_acquisition(model)\n\n        # apply constraints if specified in vocs\n        # TODO: replace with direct constrainted acquisition function calls\n        # see SampleReducingMCAcquisitionFunction in botorch for rationale\n        if len(self.vocs.constraints):\n            try:\n                sampler = acq.sampler\n            except AttributeError:\n                sampler = self._get_sampler(model)\n\n            acq = ConstrainedMCAcquisitionFunction(\n                model, acq, self._get_constraint_callables(), sampler=sampler\n            )\n\n        # apply fixed features if specified in the generator\n        if self.fixed_features is not None:\n            # get input dim\n            dim = len(self.model_input_names)\n            columns = []\n            values = []\n            for name, value in self.fixed_features.items():\n                columns += [self.model_input_names.index(name)]\n                values += [value]\n\n            acq = FixedFeatureAcquisitionFunction(\n                acq_function=acq, d=dim, columns=columns, values=values\n            )\n\n        if self.log_transform_acquisition_function:\n            acq = LogAcquisitionFunction(acq)\n\n        return acq\n\n    def get_optimum(self):\n        \"\"\"select the best point(s) given by the\n        model using the Posterior mean\"\"\"\n        c_posterior_mean = ConstrainedMCAcquisitionFunction(\n            self.model,\n            qUpperConfidenceBound(\n                model=self.model, beta=0.0, objective=self._get_objective()\n            ),\n            self._get_constraint_callables(),\n        )\n\n        result = self.numerical_optimizer.optimize(\n            c_posterior_mean, self._get_bounds(), 1\n        )\n\n        return self._process_candidates(result)\n\n    def visualize_model(self, **kwargs):\n        \"\"\"displays the GP models\"\"\"\n        return visualize_generator_model(self, **kwargs)\n\n    def _process_candidates(self, candidates: Tensor):\n        \"\"\"process pytorch candidates from optimizing the acquisition function\"\"\"\n        logger.debug(\"Best candidate from optimize\", candidates)\n\n        if self.fixed_features is not None:\n            results = pd.DataFrame(\n                candidates.detach().cpu().numpy(), columns=self._candidate_names\n            )\n            for name, val in self.fixed_features.items():\n                results[name] = val\n\n        else:\n            results = self.vocs.convert_numpy_to_inputs(\n                candidates.detach().cpu().numpy(), include_constants=False\n            )\n\n        return results\n\n    def _get_sampler(self, model):\n        input_data = self.get_input_data(self.data)\n        sampler = get_sampler(\n            model.posterior(input_data),\n            sample_shape=torch.Size([self.n_monte_carlo_samples]),\n        )\n        return sampler\n\n    @abstractmethod\n    def _get_acquisition(self, model):\n        pass\n\n    def _get_objective(self):\n        \"\"\"return default objective (scalar objective) determined by vocs\"\"\"\n        return create_mc_objective(self.vocs, self._tkwargs)\n\n    def _get_constraint_callables(self):\n        \"\"\"return default objective (scalar objective) determined by vocs\"\"\"\n        constraint_callables = create_constraint_callables(self.vocs)\n        if len(constraint_callables) == 0:\n            constraint_callables = None\n        return constraint_callables\n\n    @property\n    def _tkwargs(self):\n        # set device and data type for generator\n        device = \"cpu\"\n        if self.use_cuda:\n            if torch.cuda.is_available():\n                device = \"cuda\"\n            else:\n                warnings.warn(\n                    \"Cuda requested in generator options but not found on \"\n                    \"machine! Using CPU instead\"\n                )\n\n        return {\"dtype\": torch.double, \"device\": device}\n\n    @property\n    def model_input_names(self):\n        \"\"\"variable names corresponding to trained model\"\"\"\n        variable_names = self.vocs.variable_names\n        if self.fixed_features is not None:\n            for name, val in self.fixed_features.items():\n                if name not in variable_names:\n                    variable_names += [name]\n        return variable_names\n\n    @property\n    def _candidate_names(self):\n        \"\"\"variable names corresponding to generated candidates\"\"\"\n        variable_names = self.vocs.variable_names\n        if self.fixed_features is not None:\n            for name in self.fixed_features:\n                if name in variable_names:\n                    variable_names.remove(name)\n        return variable_names\n\n    def _get_bounds(self):\n        \"\"\"convert bounds from vocs to torch tensors\"\"\"\n        return torch.tensor(self.vocs.bounds, **self._tkwargs)\n\n    def _get_optimization_bounds(self):\n        \"\"\"\n        Get optimization bounds based on the union of several domains.\n\n        Returns:\n        --------\n        torch.Tensor\n            Tensor containing the optimized bounds.\n\n        Notes:\n        ------\n        This method calculates the optimization bounds based on several factors:\n\n        - If 'max_travel_distances' is specified, the bounds are modified to limit\n            the maximum travel distances between points in normalized space.\n        - If 'turbo_controller' is not None, the bounds are updated according to the\n            trust region specified by the controller.\n        - If 'fixed_features' are included in the variable names from the VOCS,\n            the bounds associated with those features are removed.\n\n        \"\"\"\n        bounds = deepcopy(self._get_bounds())\n\n        # if specified modify bounds to limit maximum travel distances\n        if self.max_travel_distances is not None:\n            max_travel_bounds = self._get_max_travel_distances_region(bounds)\n            bounds = rectilinear_domain_union(bounds, max_travel_bounds)\n\n        # if using turbo, update turbo state and set bounds according to turbo state\n        if self.turbo_controller is not None:\n            # set the best value\n            turbo_bounds = self.turbo_controller.get_trust_region(self.model)\n            bounds = rectilinear_domain_union(bounds, turbo_bounds)\n\n        # if fixed features key is in vocs then we need to remove the bounds\n        # associated with that key\n        if self.fixed_features is not None:\n            # grab variable name indices that are NOT in fixed features\n            indicies = []\n            for idx, name in enumerate(self.vocs.variable_names):\n                if name not in self.fixed_features:\n                    indicies += [idx]\n\n            # grab indexed bounds\n            bounds = bounds.T[indicies].T\n\n        return bounds\n\n    def _get_max_travel_distances_region(self, bounds):\n        \"\"\"\n        Calculate the region for maximum travel distances based on the current bounds\n        and the last observation.\n\n        Parameters:\n        -----------\n        bounds : torch.Tensor\n            The optimization bounds based on the union of several domains.\n\n        Returns:\n        --------\n        torch.Tensor\n            The bounds for the maximum travel distances region.\n\n        Raises:\n        -------\n        ValueError\n            If the length of max_travel_distances does not match the number of\n            variables in bounds.\n\n        Notes:\n        ------\n        This method calculates the region in which the next candidates for\n        optimization should be generated based on the maximum travel distances\n        specified. The region is centered around the last observation in the\n        optimization space. The `max_travel_distances` parameter should be a list of\n        maximum travel distances for each variable.\n\n        \"\"\"\n        if len(self.max_travel_distances) != bounds.shape[-1]:\n            raise ValueError(\n                f\"length of max_travel_distances must match the number of \"\n                f\"variables {bounds.shape[-1]}\"\n            )\n\n        # get last point\n        if self.data is None:\n            raise ValueError(\n                \"No data exists to specify max_travel_distances \"\n                \"from, add data first to use during BO\"\n            )\n        last_point = torch.tensor(\n            self.data[self.vocs.variable_names].iloc[-1].to_numpy(), **self._tkwargs\n        )\n\n        # bound lengths based on vocs for normalization\n        lengths = self.vocs.bounds[1, :] - self.vocs.bounds[0, :]\n\n        # get maximum travel distances\n        max_travel_distances = torch.tensor(\n            self.max_travel_distances, **self._tkwargs\n        ) * torch.tensor(lengths, **self._tkwargs)\n        max_travel_bounds = torch.stack(\n            (last_point - max_travel_distances, last_point + max_travel_distances)\n        )\n\n        return max_travel_bounds\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.model_input_names","title":"<code>model_input_names</code>  <code>property</code>","text":"<p>variable names corresponding to trained model</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.generate","title":"<code>generate(n_candidates)</code>","text":"<p>Generate candidates using Bayesian Optimization.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.generate--parameters","title":"Parameters:","text":"<p>n_candidates : int     The number of candidates to generate in each optimization step.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.generate--returns","title":"Returns:","text":"<p>List[Dict]     A list of dictionaries containing the generated candidates.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.generate--raises","title":"Raises:","text":"<p>NotImplementedError     If the number of candidates is greater than 1, and the generator does not     support batch candidate generation.</p> <p>RuntimeError     If no data is contained in the generator, the 'add_data' method should be     called to add data before generating candidates.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.generate--notes","title":"Notes:","text":"<p>This method generates candidates for Bayesian Optimization based on the provided number of candidates. It updates the internal model with the current data and calculates the candidates by optimizing the acquisition function. The method returns the generated candidates in the form of a list of dictionaries.</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def generate(self, n_candidates: int) -&gt; list[dict]:\n    \"\"\"\n    Generate candidates using Bayesian Optimization.\n\n    Parameters:\n    -----------\n    n_candidates : int\n        The number of candidates to generate in each optimization step.\n\n    Returns:\n    --------\n    List[Dict]\n        A list of dictionaries containing the generated candidates.\n\n    Raises:\n    -------\n    NotImplementedError\n        If the number of candidates is greater than 1, and the generator does not\n        support batch candidate generation.\n\n    RuntimeError\n        If no data is contained in the generator, the 'add_data' method should be\n        called to add data before generating candidates.\n\n    Notes:\n    ------\n    This method generates candidates for Bayesian Optimization based on the\n    provided number of candidates. It updates the internal model with the current\n    data and calculates the candidates by optimizing the acquisition function.\n    The method returns the generated candidates in the form of a list of dictionaries.\n    \"\"\"\n\n    self.n_candidates = n_candidates\n    if n_candidates &gt; 1 and not self.supports_batch_generation:\n        raise NotImplementedError(\n            \"This Bayesian algorithm does not currently support parallel candidate \"\n            \"generation\"\n        )\n\n    # if no data exists raise error\n    if self.data is None:\n        raise RuntimeError(\n            \"no data contained in generator, call `add_data` \"\n            \"method to add data, see also `Xopt.random_evaluate()`\"\n        )\n\n    else:\n        # dict to track runtimes\n        timing_results = {}\n\n        # update internal model with internal data\n        start_time = time.perf_counter()\n        model = self.train_model(self.data)\n        timing_results[\"training\"] = time.perf_counter() - start_time\n\n        # propose candidates given model\n        start_time = time.perf_counter()\n        candidates = self.propose_candidates(model, n_candidates=n_candidates)\n        timing_results[\"acquisition_optimization\"] = (\n            time.perf_counter() - start_time\n        )\n\n        # post process candidates\n        result = self._process_candidates(candidates)\n\n        # append timing results to dataframe (if it exists)\n        if self.computation_time is not None:\n            self.computation_time = pd.concat(\n                (\n                    self.computation_time,\n                    pd.DataFrame(timing_results, index=[0]),\n                ),\n                ignore_index=True,\n            )\n        else:\n            self.computation_time = pd.DataFrame(timing_results, index=[0])\n\n        if self.n_interpolate_points is not None:\n            if self.n_candidates &gt; 1:\n                raise RuntimeError(\n                    \"cannot generate interpolated points for \"\n                    \"multiple candidate generation\"\n                )\n            else:\n                assert len(result) == 1\n                result = interpolate_points(\n                    pd.concat(\n                        (self.data.iloc[-1:][self.vocs.variable_names], result),\n                        axis=0,\n                        ignore_index=True,\n                    ),\n                    num_points=self.n_interpolate_points,\n                )\n\n        return result.to_dict(\"records\")\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_acquisition","title":"<code>get_acquisition(model)</code>","text":"<p>Define the acquisition function based on the given GP model.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_acquisition--parameters","title":"Parameters:","text":"<p>model : Model     The BoTorch model to be used for generating the acquisition function.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_acquisition--returns","title":"Returns:","text":"<p>acqusition_function : AcqusitionFunction</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_acquisition--raises","title":"Raises:","text":"<p>ValueError     If the provided 'model' is None. A valid model is required to create the     acquisition function.</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def get_acquisition(self, model):\n    \"\"\"\n    Define the acquisition function based on the given GP model.\n\n    Parameters:\n    -----------\n    model : Model\n        The BoTorch model to be used for generating the acquisition function.\n\n    Returns:\n    --------\n    acqusition_function : AcqusitionFunction\n\n    Raises:\n    -------\n    ValueError\n        If the provided 'model' is None. A valid model is required to create the\n        acquisition function.\n    \"\"\"\n    if model is None:\n        raise ValueError(\"model cannot be None\")\n\n    # get base acquisition function\n    acq = self._get_acquisition(model)\n\n    # apply constraints if specified in vocs\n    # TODO: replace with direct constrainted acquisition function calls\n    # see SampleReducingMCAcquisitionFunction in botorch for rationale\n    if len(self.vocs.constraints):\n        try:\n            sampler = acq.sampler\n        except AttributeError:\n            sampler = self._get_sampler(model)\n\n        acq = ConstrainedMCAcquisitionFunction(\n            model, acq, self._get_constraint_callables(), sampler=sampler\n        )\n\n    # apply fixed features if specified in the generator\n    if self.fixed_features is not None:\n        # get input dim\n        dim = len(self.model_input_names)\n        columns = []\n        values = []\n        for name, value in self.fixed_features.items():\n            columns += [self.model_input_names.index(name)]\n            values += [value]\n\n        acq = FixedFeatureAcquisitionFunction(\n            acq_function=acq, d=dim, columns=columns, values=values\n        )\n\n    if self.log_transform_acquisition_function:\n        acq = LogAcquisitionFunction(acq)\n\n    return acq\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_input_data","title":"<code>get_input_data(data)</code>","text":"<p>Convert input data to a torch tensor.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_input_data--parameters","title":"Parameters:","text":"<p>data : pd.DataFrame     The input data in the form of a pandas DataFrame.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_input_data--returns","title":"Returns:","text":"<p>torch.Tensor     A torch tensor containing the input data.</p>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_input_data--notes","title":"Notes:","text":"<p>This method takes a pandas DataFrame as input data and converts it into a torch tensor. It specifically selects columns corresponding to the model's input names (variables), and the resulting tensor is configured with the data type and device settings from the generator.</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def get_input_data(self, data: pd.DataFrame) -&gt; torch.Tensor:\n    \"\"\"\n    Convert input data to a torch tensor.\n\n    Parameters:\n    -----------\n    data : pd.DataFrame\n        The input data in the form of a pandas DataFrame.\n\n    Returns:\n    --------\n    torch.Tensor\n        A torch tensor containing the input data.\n\n    Notes:\n    ------\n    This method takes a pandas DataFrame as input data and converts it into a\n    torch tensor. It specifically selects columns corresponding to the model's\n    input names (variables), and the resulting tensor is configured with the data\n    type and device settings from the generator.\n    \"\"\"\n    return torch.tensor(data[self.model_input_names].to_numpy(), **self._tkwargs)\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_optimum","title":"<code>get_optimum()</code>","text":"<p>select the best point(s) given by the model using the Posterior mean</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def get_optimum(self):\n    \"\"\"select the best point(s) given by the\n    model using the Posterior mean\"\"\"\n    c_posterior_mean = ConstrainedMCAcquisitionFunction(\n        self.model,\n        qUpperConfidenceBound(\n            model=self.model, beta=0.0, objective=self._get_objective()\n        ),\n        self._get_constraint_callables(),\n    )\n\n    result = self.numerical_optimizer.optimize(\n        c_posterior_mean, self._get_bounds(), 1\n    )\n\n    return self._process_candidates(result)\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.propose_candidates","title":"<code>propose_candidates(model, n_candidates=1)</code>","text":"<p>given a GP model, propose candidates by numerically optimizing the acquisition function</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def propose_candidates(self, model, n_candidates=1):\n    \"\"\"\n    given a GP model, propose candidates by numerically optimizing the\n    acquisition function\n\n    \"\"\"\n    # update TurBO state if used with the last `n_candidates` points\n    if self.turbo_controller is not None:\n        self.turbo_controller.update_state(self.data, n_candidates)\n\n    # calculate optimization bounds\n    bounds = self._get_optimization_bounds()\n\n    # get acquisition function\n    acq_funct = self.get_acquisition(model)\n\n    # get candidates\n    candidates = self.numerical_optimizer.optimize(acq_funct, bounds, n_candidates)\n    return candidates\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.train_model","title":"<code>train_model(data=None, update_internal=True)</code>","text":"<p>Returns a ModelListGP containing independent models for the objectives and constraints</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def train_model(self, data: pd.DataFrame = None, update_internal=True) -&gt; Module:\n    \"\"\"\n    Returns a ModelListGP containing independent models for the objectives and\n    constraints\n\n    \"\"\"\n    if data is None:\n        data = self.data\n    if data.empty:\n        raise ValueError(\"no data available to build model\")\n\n    # get input bounds\n    variable_bounds = deepcopy(self.vocs.variables)\n\n    # add fixed feature bounds if requested\n    if self.fixed_features is not None:\n        # get bounds for each fixed_feature (vocs bounds take precedent)\n        for key in self.fixed_features:\n            if key not in variable_bounds:\n                f_data = data[key]\n                bounds = [f_data.min(), f_data.max()]\n                if bounds[1] - bounds[0] &lt; 1e-8:\n                    bounds[1] = bounds[0] + 1e-8\n                variable_bounds[key] = bounds\n\n    _model = self.gp_constructor.build_model(\n        self.model_input_names,\n        self.vocs.output_names,\n        data,\n        {name: variable_bounds[name] for name in self.model_input_names},\n        **self._tkwargs,\n    )\n\n    if update_internal:\n        self.model = _model\n    return _model\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.validate_turbo_controller","title":"<code>validate_turbo_controller(value, info)</code>","text":"<p>note default behavior is no use of turbo</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>@field_validator(\"turbo_controller\", mode=\"before\")\ndef validate_turbo_controller(cls, value, info: ValidationInfo):\n    \"\"\"note default behavior is no use of turbo\"\"\"\n    optimizer_dict = {\n        \"optimize\": OptimizeTurboController,\n        \"safety\": SafetyTurboController,\n    }\n    if isinstance(value, TurboController):\n        pass\n    elif isinstance(value, str):\n        # create turbo controller from string input\n        if value in optimizer_dict:\n            value = optimizer_dict[value](info.data[\"vocs\"])\n        else:\n            raise ValueError(\n                f\"{value} not found, available values are \"\n                f\"{optimizer_dict.keys()}\"\n            )\n    elif isinstance(value, dict):\n        # create turbo controller from dict input\n        if \"name\" not in value:\n            raise ValueError(\"turbo input dict needs to have a `name` attribute\")\n        name = value.pop(\"name\")\n        if name in optimizer_dict:\n            # pop unnecessary elements\n            for ele in [\"dim\"]:\n                value.pop(ele, None)\n\n            value = optimizer_dict[name](vocs=info.data[\"vocs\"], **value)\n        else:\n            raise ValueError(\n                f\"{value} not found, available values are \"\n                f\"{optimizer_dict.keys()}\"\n            )\n    return value\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.visualize_model","title":"<code>visualize_model(**kwargs)</code>","text":"<p>displays the GP models</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def visualize_model(self, **kwargs):\n    \"\"\"displays the GP models\"\"\"\n    return visualize_generator_model(self, **kwargs)\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.mobo.MOBOGenerator.get_acquisition","title":"<code>get_acquisition(model)</code>","text":"<p>Returns a function that can be used to evaluate the acquisition function</p> Source code in <code>xopt/generators/bayesian/mobo.py</code> <pre><code>def get_acquisition(self, model):\n    \"\"\"\n    Returns a function that can be used to evaluate the acquisition function\n    \"\"\"\n    if model is None:\n        raise ValueError(\"model cannot be None\")\n\n    # get base acquisition function\n    acq = self._get_acquisition(model)\n\n    # apply fixed features if specified in the generator\n    if self.fixed_features is not None:\n        # get input dim\n        dim = len(self.model_input_names)\n        columns = []\n        values = []\n        for name, value in self.fixed_features.items():\n            columns += [self.model_input_names.index(name)]\n            values += [value]\n\n        acq = FixedFeatureAcquisitionFunction(\n            acq_function=acq, d=dim, columns=columns, values=values\n        )\n\n    return acq\n</code></pre>"},{"location":"api/generators/genetic/","title":"Genetic generators","text":"<p>             Bases: <code>Generator</code></p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>class CNSGAGenerator(Generator):\n    name = \"cnsga\"\n    supports_multi_objective: bool = True\n    population_size: int = Field(64, description=\"Population size\")\n    crossover_probability: confloat(ge=0, le=1) = Field(\n        0.9, description=\"Crossover probability\"\n    )\n    mutation_probability: confloat(ge=0, le=1) = Field(\n        1.0, description=\"Mutation probability\"\n    )\n    population_file: Optional[str] = Field(\n        None, description=\"Population file to load (CSV format)\"\n    )\n    output_path: Optional[str] = Field(\n        None, description=\"Output path for population \" \"files\"\n    )\n    _children: List[Dict] = PrivateAttr([])\n    _offspring: Optional[pd.DataFrame] = PrivateAttr(None)\n    population: Optional[pd.DataFrame] = Field(None)\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self._loaded_population = (\n            None  # use these to generate children until the first pop is made\n        )\n\n        # DEAP toolbox (internal)\n        self._toolbox = cnsga_toolbox(self.vocs, selection=\"auto\")\n\n        if self.population_file is not None:\n            self.load_population_csv(self.population_file)\n\n        if self.output_path is not None:\n            assert os.path.isdir(self.output_path), \"Output directory does not exist\"\n\n        # if data is not None:\n        #    self.population = cnsga_select(data, n_pop, vocs, self.toolbox)\n\n    def create_children(self) -&gt; List[Dict]:\n        # No population, so create random children\n        if self.population is None:\n            # Special case when pop is loaded from file\n            if self._loaded_population is None:\n                return self.vocs.random_inputs(self.n_pop, include_constants=False)\n            else:\n                pop = self._loaded_population\n        else:\n            pop = self.population\n\n        # Use population to create children\n        inputs = cnsga_variation(\n            pop,\n            self.vocs,\n            self._toolbox,\n            crossover_probability=self.crossover_probability,\n            mutation_probability=self.mutation_probability,\n        )\n        return inputs.to_dict(orient=\"records\")\n\n    def add_data(self, new_data: pd.DataFrame):\n        if new_data is not None:\n            self._offspring = pd.concat([self._offspring, new_data])\n\n            # Next generation\n            if len(self._offspring) &gt;= self.n_pop:\n                candidates = pd.concat([self.population, self._offspring])\n                self.population = cnsga_select(\n                    candidates, self.n_pop, self.vocs, self._toolbox\n                )\n\n                if self.output_path is not None:\n                    self.write_offspring()\n                    self.write_population()\n\n                self._children = []  # reset children\n                self._offspring = None  # reset offspring\n\n    def generate(self, n_candidates) -&gt; list[dict]:\n        \"\"\"\n        generate `n_candidates` candidates\n\n        \"\"\"\n\n        # Make sure we have enough children to fulfill the request\n        while len(self._children) &lt; n_candidates:\n            self._children.extend(self.create_children())\n\n        return [self._children.pop() for _ in range(n_candidates)]\n\n    def write_offspring(self, filename=None):\n        \"\"\"\n        Write the current offspring to a CSV file.\n\n        Similar to write_population\n        \"\"\"\n        if self._offspring is None:\n            logger.warning(\"No offspring to write\")\n            return\n\n        if filename is None:\n            filename = f\"{self.name}_offspring_{xopt.utils.isotime(include_microseconds=True)}.csv\"\n            filename = os.path.join(self.output_path, filename)\n\n        self._offspring.to_csv(filename, index_label=\"xopt_index\")\n\n    def write_population(self, filename=None):\n        \"\"\"\n        Write the current population to a CSV file.\n\n        Similar to write_offspring\n        \"\"\"\n        if self.population is None:\n            logger.warning(\"No population to write\")\n            return\n\n        if filename is None:\n            filename = f\"{self.name}_population_{xopt.utils.isotime(include_microseconds=True)}.csv\"\n            filename = os.path.join(self.output_path, filename)\n\n        self.population.to_csv(filename, index_label=\"xopt_index\")\n\n    def load_population_csv(self, filename):\n        \"\"\"\n        Read a population from a CSV file.\n        These will be reverted back to children for re-evaluation.\n        \"\"\"\n        pop = pd.read_csv(filename, index_col=\"xopt_index\")\n        self._loaded_population = pop\n        # This is a list of dicts\n        self._children = self.vocs.convert_dataframe_to_inputs(\n            pop[self.vocs.variable_names], include_constants=False\n        ).to_dict(orient=\"records\")\n        logger.info(f\"Loaded population of len {len(pop)} from file: {filename}\")\n\n    @property\n    def n_pop(self):\n        \"\"\"\n        Convenience name for `options.population_size`\n        \"\"\"\n        return self.population_size\n</code></pre>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.n_pop","title":"<code>n_pop</code>  <code>property</code>","text":"<p>Convenience name for <code>options.population_size</code></p>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.generate","title":"<code>generate(n_candidates)</code>","text":"<p>generate <code>n_candidates</code> candidates</p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>def generate(self, n_candidates) -&gt; list[dict]:\n    \"\"\"\n    generate `n_candidates` candidates\n\n    \"\"\"\n\n    # Make sure we have enough children to fulfill the request\n    while len(self._children) &lt; n_candidates:\n        self._children.extend(self.create_children())\n\n    return [self._children.pop() for _ in range(n_candidates)]\n</code></pre>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.load_population_csv","title":"<code>load_population_csv(filename)</code>","text":"<p>Read a population from a CSV file. These will be reverted back to children for re-evaluation.</p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>def load_population_csv(self, filename):\n    \"\"\"\n    Read a population from a CSV file.\n    These will be reverted back to children for re-evaluation.\n    \"\"\"\n    pop = pd.read_csv(filename, index_col=\"xopt_index\")\n    self._loaded_population = pop\n    # This is a list of dicts\n    self._children = self.vocs.convert_dataframe_to_inputs(\n        pop[self.vocs.variable_names], include_constants=False\n    ).to_dict(orient=\"records\")\n    logger.info(f\"Loaded population of len {len(pop)} from file: {filename}\")\n</code></pre>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.write_offspring","title":"<code>write_offspring(filename=None)</code>","text":"<p>Write the current offspring to a CSV file.</p> <p>Similar to write_population</p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>def write_offspring(self, filename=None):\n    \"\"\"\n    Write the current offspring to a CSV file.\n\n    Similar to write_population\n    \"\"\"\n    if self._offspring is None:\n        logger.warning(\"No offspring to write\")\n        return\n\n    if filename is None:\n        filename = f\"{self.name}_offspring_{xopt.utils.isotime(include_microseconds=True)}.csv\"\n        filename = os.path.join(self.output_path, filename)\n\n    self._offspring.to_csv(filename, index_label=\"xopt_index\")\n</code></pre>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.write_population","title":"<code>write_population(filename=None)</code>","text":"<p>Write the current population to a CSV file.</p> <p>Similar to write_offspring</p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>def write_population(self, filename=None):\n    \"\"\"\n    Write the current population to a CSV file.\n\n    Similar to write_offspring\n    \"\"\"\n    if self.population is None:\n        logger.warning(\"No population to write\")\n        return\n\n    if filename is None:\n        filename = f\"{self.name}_population_{xopt.utils.isotime(include_microseconds=True)}.csv\"\n        filename = os.path.join(self.output_path, filename)\n\n    self.population.to_csv(filename, index_label=\"xopt_index\")\n</code></pre>"},{"location":"api/generators/scipy/","title":"SciPy generators","text":"<p>             Bases: <code>Generator</code></p> <p>Nelder-Mead algorithm from SciPy in Xopt's Generator form. Converted to use a state machine to resume in exactly the last state.</p> Source code in <code>xopt/generators/scipy/neldermead.py</code> <pre><code>class NelderMeadGenerator(Generator):\n    \"\"\"\n    Nelder-Mead algorithm from SciPy in Xopt's Generator form.\n    Converted to use a state machine to resume in exactly the last state.\n    \"\"\"\n\n    name = \"neldermead\"\n\n    initial_point: Optional[Dict[str, float]] = None  # replaces x0 argument\n    initial_simplex: Optional[\n        Dict[str, Union[List[float], np.ndarray]]\n    ] = None  # This overrides the use of initial_point\n    # Same as scipy.optimize._optimize._minimize_neldermead\n    adaptive: bool = Field(\n        True, description=\"Change hyperparameters based on dimensionality\"\n    )\n    xatol: float = Field(1e-4, description=\"Tolerance in x value\")\n    fatol: float = Field(1e-4, description=\"Tolerance in function value\")\n    current_state: SimplexState = SimplexState()\n    future_state: Optional[SimplexState] = None\n\n    # Internal data structures\n    x: Optional[np.ndarray] = None\n    y: Optional[float] = None\n    is_done_bool: bool = False\n\n    _initial_simplex = None\n    _saved_options: Dict = None\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # Initialize the first candidate if not given\n        if self.initial_point is None:\n            self.initial_point = self.vocs.random_inputs()[0]\n\n        self._saved_options = self.model_dump(\n            exclude={\"current_state\", \"future_state\"}\n        ).copy()  # Used to keep track of changed options\n\n        if self.initial_simplex:\n            self._initial_simplex = np.array(\n                [self.initial_simplex[k] for k in self.vocs.variable_names]\n            ).T\n        else:\n            self._initial_simplex = None\n\n    @property\n    def x0(self):\n        \"\"\"Raw internal initial point for convenience\"\"\"\n        return np.array([self.initial_point[k] for k in self.vocs.variable_names])\n\n    @property\n    def is_done(self):\n        return self.is_done_bool\n\n    def add_data(self, new_data: pd.DataFrame):\n        if len(new_data) == 0:\n            # empty data, i.e. no steps yet\n            assert self.future_state is None\n            return\n\n        self.data = pd.concat([self.data, new_data], axis=0)\n\n        # Complicated part - need to determine if data corresponds to result of last gen\n        ndata = len(self.data)\n        ngen = self.current_state.ngen\n        if ndata == ngen:\n            # just resuming\n            # print(f'Resuming with {ngen=}')\n            return\n        else:\n            # Must have made at least 1 step, require future_state\n            assert self.future_state is not None\n\n            # new data -&gt; advance state machine 1 step\n            assert ndata == self.future_state.ngen == ngen + 1\n            self.current_state = self.future_state\n            self.future_state = None\n\n            # Can have multiple points if resuming from file, grab last one\n            new_data_df = self.vocs.objective_data(new_data)\n            res = new_data_df.iloc[-1:, :].to_numpy()\n            assert np.shape(res) == (1, 1), f\"Bad last point {res}\"\n\n            yt = res[0, 0].item()\n            if np.isinf(yt) or np.isnan(yt):\n                self.is_done_bool = True\n                return\n\n            self.y = yt\n            # print(f'Added data {self.y=}')\n\n    def generate(self, n_candidates: int) -&gt; Optional[list[dict]]:\n        if self.is_done:\n            return None\n\n        if n_candidates != 1:\n            raise NotImplementedError(\n                \"simplex can only produce one candidate at a time\"\n            )\n\n        if self.current_state.N is None:\n            # fresh start\n            pass\n        else:\n            n_inputs = len(self.data)\n            if self.current_state.ngen == n_inputs:\n                # We are in a state where result of last point is known\n                pass\n            else:\n                pass\n\n        results = self._call_algorithm()\n        if results is None:\n            self.is_done_bool = True\n            return None\n\n        x, state_extra = results\n        assert len(state_extra) == len(STATE_KEYS)\n        stateobj = SimplexState(**{k: v for k, v in zip(STATE_KEYS, state_extra)})\n        # print(\"State:\", stateobj)\n        self.future_state = stateobj\n\n        inputs = dict(zip(self.vocs.variable_names, x))\n        if self.vocs.constants is not None:\n            inputs.update(self.vocs.constants)\n\n        return [inputs]\n\n    def _call_algorithm(self):\n        results = _neldermead_generator(\n            self.x0,\n            state=self.current_state,\n            lastval=self.y,\n            adaptive=self.adaptive,\n            xatol=self.xatol,\n            fatol=self.fatol,\n            initial_simplex=self._initial_simplex,\n            bounds=self.vocs.bounds,\n        )\n\n        self.y = None\n        return results\n\n    @property\n    def simplex(self):\n        \"\"\"\n        Returns the simplex in the current state.\n        \"\"\"\n        sim = self.current_state.sim\n        return dict(zip(self.vocs.variable_names, sim.T))\n</code></pre>"},{"location":"api/generators/scipy/#xopt.generators.scipy.neldermead.NelderMeadGenerator.simplex","title":"<code>simplex</code>  <code>property</code>","text":"<p>Returns the simplex in the current state.</p>"},{"location":"api/generators/scipy/#xopt.generators.scipy.neldermead.NelderMeadGenerator.x0","title":"<code>x0</code>  <code>property</code>","text":"<p>Raw internal initial point for convenience</p>"},{"location":"examples/basic/checkpointing_and_restarts/","title":"Checkpointing and Restarts","text":"In\u00a0[1]: Copied! <pre># Import the class\nfrom xopt import Xopt\n\n# Make a proper input file.\nYAML = \"\"\"\ndump_file: dump.yml\ngenerator:\n    name: random\n\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n    function_kwargs:\n        a: 999\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    constants: {a: dummy_constant}\n\n\"\"\"\n</pre> # Import the class from xopt import Xopt  # Make a proper input file. YAML = \"\"\" dump_file: dump.yml generator:     name: random  evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK     function_kwargs:         a: 999  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     constants: {a: dummy_constant}  \"\"\" In\u00a0[2]: Copied! <pre># create Xopt object.\nX = Xopt.from_yaml(YAML)\n\n# take 10 steps and view data\nfor _ in range(10):\n    X.step()\n\nX.data\n</pre> # create Xopt object. X = Xopt.from_yaml(YAML)  # take 10 steps and view data for _ in range(10):     X.step()  X.data Out[2]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 0 2.753363 2.631444 dummy_constant 2.753363 2.631444 13.411995 9.620699 0.000052 False 1 1.707470 1.995531 dummy_constant 1.707470 1.995531 5.865320 3.694595 0.000032 False 2 0.276053 0.369853 dummy_constant 0.276053 0.369853 -0.719824 0.067091 0.000031 False 3 2.344382 0.591074 dummy_constant 2.344382 0.591074 4.914443 3.410040 0.000030 False 4 2.748114 2.726328 dummy_constant 2.748114 2.726328 13.885196 10.010552 0.000030 False 5 2.205413 2.660812 dummy_constant 2.205413 2.660812 10.935996 7.577543 0.000028 False 6 3.080429 0.234430 dummy_constant 3.080429 0.234430 8.509198 6.729143 0.000029 False 7 0.872056 0.430030 dummy_constant 0.872056 0.430030 -0.104618 0.143321 0.000034 False 8 3.036472 0.750300 dummy_constant 3.036472 0.750300 8.857343 6.496341 0.000032 False 9 0.287512 0.704728 dummy_constant 0.287512 0.704728 -0.520331 0.087065 0.000033 False In\u00a0[3]: Copied! <pre>X2 = Xopt.from_file(\"dump.yml\")\nX2\n</pre> X2 = Xopt.from_file(\"dump.yml\") X2 Out[3]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 10\nConfig as YAML:\ndump_file: dump.yml\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    a: 999\n    raise_probability: 0\n    random_sleep: 0\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  name: random\n  supports_batch_generation: true\n  supports_multi_objective: true\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives:\n    y1: MINIMIZE\n    y2: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[4]: Copied! <pre>for _ in range(10):\n    X2.step()\n\nX2.data\n</pre> for _ in range(10):     X2.step()  X2.data Out[4]: a c1 c2 x1 x2 xopt_error xopt_runtime y1 y2 0 dummy_constant 13.411995 9.620699 2.753363 2.631444 False 0.000052 2.753363 2.631444 1 dummy_constant 5.865320 3.694595 1.707470 1.995531 False 0.000032 1.707470 1.995531 2 dummy_constant -0.719824 0.067091 0.276053 0.369853 False 0.000031 0.276053 0.369853 3 dummy_constant 4.914443 3.410040 2.344382 0.591074 False 0.000030 2.344382 0.591074 4 dummy_constant 13.885196 10.010552 2.748114 2.726328 False 0.000030 2.748114 2.726328 5 dummy_constant 10.935996 7.577543 2.205413 2.660812 False 0.000028 2.205413 2.660812 6 dummy_constant 8.509198 6.729143 3.080429 0.234430 False 0.000029 3.080429 0.234430 7 dummy_constant -0.104618 0.143321 0.872056 0.430030 False 0.000034 0.872056 0.430030 8 dummy_constant 8.857343 6.496341 3.036472 0.750300 False 0.000032 3.036472 0.750300 9 dummy_constant -0.520331 0.087065 0.287512 0.704728 False 0.000033 0.287512 0.704728 10 dummy_constant 6.306542 5.050761 0.117934 2.714675 False 0.000041 0.117934 2.714675 11 dummy_constant 3.868849 2.316016 1.563942 1.588138 False 0.000035 1.563942 1.588138 12 dummy_constant 6.530028 5.281945 0.078597 2.759284 False 0.000041 0.078597 2.759284 13 dummy_constant 11.830742 8.321016 2.683717 2.384781 False 0.000033 2.683717 2.384781 14 dummy_constant 5.864065 4.636691 2.626027 0.158384 False 0.000035 2.626027 0.158384 15 dummy_constant 12.632168 8.981753 2.711862 2.522231 False 0.000037 2.711862 2.522231 16 dummy_constant 2.210009 1.540217 0.339284 1.730605 False 0.000040 0.339284 1.730605 17 dummy_constant 4.849825 2.934639 1.888128 1.503862 False 0.000035 1.888128 1.503862 18 dummy_constant 1.735037 1.154253 1.569426 0.602862 False 0.000039 1.569426 0.602862 19 dummy_constant 4.900578 2.949902 1.415859 1.952964 False 0.000034 1.415859 1.952964 In\u00a0[5]: Copied! <pre># clean up\n</pre> # clean up"},{"location":"examples/basic/checkpointing_and_restarts/#checkpointing-and-restarts","title":"Checkpointing and Restarts\u00b6","text":"<p>If <code>dump_file</code> is provided Xopt will save the data and the Xopt configuration in a yaml file. This can be used directly to create a new Xopt object.</p>"},{"location":"examples/basic/checkpointing_and_restarts/#checkpoints","title":"Checkpoints\u00b6","text":"<p>Since we specified a dump file Xopt will dump the data and all of the options required to create a new Xopt object that continues the run.</p>"},{"location":"examples/basic/checkpointing_and_restarts/#create-xopt-object-from-dump-file","title":"Create Xopt object from dump file\u00b6","text":""},{"location":"examples/basic/xopt_basic/","title":"Xopt basic example","text":"In\u00a0[1]: Copied! <pre>from xopt import Evaluator\n\ndef evaluate_function(inputs: dict) -&gt; dict:\n    objective_value = inputs[\"x1\"]**2 + inputs[\"x2\"]**2\n    constraint_value = -inputs[\"x1\"]**2 - inputs[\"x2\"]**2 + 1\n    return {\"f\": objective_value, \"g\": constraint_value}\n\nevaluator = Evaluator(function=evaluate_function)\n</pre> from xopt import Evaluator  def evaluate_function(inputs: dict) -&gt; dict:     objective_value = inputs[\"x1\"]**2 + inputs[\"x2\"]**2     constraint_value = -inputs[\"x1\"]**2 - inputs[\"x2\"]**2 + 1     return {\"f\": objective_value, \"g\": constraint_value}  evaluator = Evaluator(function=evaluate_function) In\u00a0[2]: Copied! <pre>from xopt import VOCS\nimport math\n\nvocs = VOCS(\n    variables = {\n        \"x1\": [0, math.pi],\n        \"x2\": [0, math.pi]\n    },\n    objectives = {\"f\": \"MINIMIZE\"},\n    constraints = {\"g\": [\"LESS_THAN\", 0]}\n)\n</pre> from xopt import VOCS import math  vocs = VOCS(     variables = {         \"x1\": [0, math.pi],         \"x2\": [0, math.pi]     },     objectives = {\"f\": \"MINIMIZE\"},     constraints = {\"g\": [\"LESS_THAN\", 0]} ) In\u00a0[3]: Copied! <pre>from xopt.generators import list_available_generators\nlist_available_generators()\n</pre> from xopt.generators import list_available_generators list_available_generators() Out[3]: <pre>['random',\n 'mggpo',\n 'neldermead',\n 'latin_hypercube',\n 'upper_confidence_bound',\n 'mobo',\n 'bayesian_exploration',\n 'time_dependent_upper_confidence_bound',\n 'expected_improvement',\n 'multi_fidelity',\n 'cnsga',\n 'extremum_seeking',\n 'rcds']</pre> <p>Here we will use the simplest generator that is defined by Xopt, random number generation.</p> In\u00a0[4]: Copied! <pre>from xopt.generators import get_generator \n# get the docstring for the random generator\nprint(get_generator(\"random\").__doc__)\n\n# use the get generator method to get the random number generator\ngenerator = get_generator(\"random\")(vocs=vocs)\n</pre> from xopt.generators import get_generator  # get the docstring for the random generator print(get_generator(\"random\").__doc__)  # use the get generator method to get the random number generator generator = get_generator(\"random\")(vocs=vocs) <pre>\n    Random number generator.\n    \n</pre> In\u00a0[5]: Copied! <pre>from xopt import Xopt\nX = Xopt(vocs=vocs, generator=generator, evaluator=evaluator)\n</pre> from xopt import Xopt X = Xopt(vocs=vocs, generator=generator, evaluator=evaluator) In\u00a0[6]: Copied! <pre># Make a proper input file.\nYAML = \"\"\"\nevaluator:\n    function: __main__.evaluate_function\n\ngenerator:\n    name: random\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {f: MINIMIZE}\n    constraints:\n        g: [LESS_THAN, 0]\n\n\"\"\"\n</pre> # Make a proper input file. YAML = \"\"\" evaluator:     function: __main__.evaluate_function  generator:     name: random  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {f: MINIMIZE}     constraints:         g: [LESS_THAN, 0]  \"\"\" In\u00a0[7]: Copied! <pre># create Xopt object.\nX_from_yaml = Xopt.from_yaml(YAML)\n</pre> # create Xopt object. X_from_yaml = Xopt.from_yaml(YAML) In\u00a0[8]: Copied! <pre># Convenient representation of the state.\nX\n</pre> # Convenient representation of the state. X Out[8]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: __main__.evaluate_function\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  name: random\n  supports_batch_generation: true\n  supports_multi_objective: true\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints:\n    g:\n    - LESS_THAN\n    - 0.0\n  objectives:\n    f: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 3.141592653589793\n    x2:\n    - 0.0\n    - 3.141592653589793\n</pre> In\u00a0[9]: Copied! <pre># randomly evaluate some points and add data to Xopt object\nX.random_evaluate(5)\n</pre> # randomly evaluate some points and add data to Xopt object X.random_evaluate(5) Out[9]: x1 x2 f g xopt_runtime xopt_error 0 0.660042 0.949303 1.336831 -0.336831 3.717000e-06 False 1 1.028965 0.454876 1.265682 -0.265682 1.383000e-06 False 2 2.225963 1.911716 8.609566 -7.609566 1.042000e-06 False 3 2.757545 1.410221 9.592775 -8.592775 1.363000e-06 False 4 2.097186 2.467250 10.485512 -9.485512 8.020000e-07 False In\u00a0[10]: Copied! <pre># evaluate some points additionally\npoints = {\"x1\": [1.0, 0.5, 2.25],\"x2\":[0,1.75,0.6]}\nX.evaluate_data(points)\n</pre> # evaluate some points additionally points = {\"x1\": [1.0, 0.5, 2.25],\"x2\":[0,1.75,0.6]} X.evaluate_data(points) Out[10]: x1 x2 f g xopt_runtime xopt_error 0 1.00 0.00 1.0000 0.0000 0.000003 False 1 0.50 1.75 3.3125 -2.3125 0.000002 False 2 2.25 0.60 5.4225 -4.4225 0.000001 False In\u00a0[11]: Copied! <pre># examine the data stored in Xopt\nX.data\n</pre> # examine the data stored in Xopt X.data Out[11]: x1 x2 f g xopt_runtime xopt_error 0 0.660042 0.949303 1.336831 -0.336831 3.717000e-06 False 1 1.028965 0.454876 1.265682 -0.265682 1.383000e-06 False 2 2.225963 1.911716 8.609566 -7.609566 1.042000e-06 False 3 2.757545 1.410221 9.592775 -8.592775 1.363000e-06 False 4 2.097186 2.467250 10.485512 -9.485512 8.020000e-07 False 5 1.000000 0.000000 1.000000 0.000000 2.946000e-06 False 6 0.500000 1.750000 3.312500 -2.312500 1.573000e-06 False 7 2.250000 0.600000 5.422500 -4.422500 1.162000e-06 False In\u00a0[12]: Copied! <pre># Take one step (generate a single point)\nX.step()\n</pre> # Take one step (generate a single point) X.step() In\u00a0[13]: Copied! <pre># examine the results\nX.data\n</pre> # examine the results X.data Out[13]: x1 x2 f g xopt_runtime xopt_error 0 0.660042 0.949303 1.336831 -0.336831 3.717000e-06 False 1 1.028965 0.454876 1.265682 -0.265682 1.383000e-06 False 2 2.225963 1.911716 8.609566 -7.609566 1.042000e-06 False 3 2.757545 1.410221 9.592775 -8.592775 1.363000e-06 False 4 2.097186 2.467250 10.485512 -9.485512 8.020000e-07 False 5 1.000000 0.000000 1.000000 0.000000 2.946000e-06 False 6 0.500000 1.750000 3.312500 -2.312500 1.573000e-06 False 7 2.250000 0.600000 5.422500 -4.422500 1.162000e-06 False 8 0.512703 2.621081 7.132930 -6.132930 1.704200e-05 False In\u00a0[14]: Copied! <pre># take a couple of steps and examine the results\nfor _ in range(10):\n    X.step()\nX.data\n</pre> # take a couple of steps and examine the results for _ in range(10):     X.step() X.data Out[14]: x1 x2 f g xopt_runtime xopt_error 0 0.660042 0.949303 1.336831 -0.336831 3.717000e-06 False 1 1.028965 0.454876 1.265682 -0.265682 1.383000e-06 False 2 2.225963 1.911716 8.609566 -7.609566 1.042000e-06 False 3 2.757545 1.410221 9.592775 -8.592775 1.363000e-06 False 4 2.097186 2.467250 10.485512 -9.485512 8.020000e-07 False 5 1.000000 0.000000 1.000000 0.000000 2.946000e-06 False 6 0.500000 1.750000 3.312500 -2.312500 1.573000e-06 False 7 2.250000 0.600000 5.422500 -4.422500 1.162000e-06 False 8 0.512703 2.621081 7.132930 -6.132930 1.704200e-05 False 9 1.802564 2.638108 10.208852 -9.208852 1.285400e-05 False 10 3.045649 2.293191 14.534701 -13.534701 1.238400e-05 False 11 2.062728 2.679771 11.436021 -10.436021 1.321400e-05 False 12 1.468462 0.755478 2.727128 -1.727128 1.347500e-05 False 13 1.367284 1.618035 4.487503 -3.487503 1.350600e-05 False 14 2.472766 3.138805 15.966670 -14.966670 4.162700e-05 False 15 1.685061 2.267897 7.982785 -6.982785 1.361600e-05 False 16 1.067109 2.973331 9.979419 -8.979419 1.339500e-05 False 17 3.102418 2.011511 13.671173 -12.671173 1.348500e-05 False 18 2.303972 1.355493 7.145645 -6.145645 1.353600e-05 False In\u00a0[15]: Copied! <pre>import matplotlib.pyplot as plt\n\n# view objective values\nX.data.plot(y=X.vocs.objective_names)\n\n# view variables values\nX.data.plot(*X.vocs.variable_names, kind=\"scatter\")\n\n# you can also normalize the variables\nX.vocs.normalize_inputs(X.data).plot(*X.vocs.variable_names, kind=\"scatter\")\n</pre> import matplotlib.pyplot as plt  # view objective values X.data.plot(y=X.vocs.objective_names)  # view variables values X.data.plot(*X.vocs.variable_names, kind=\"scatter\")  # you can also normalize the variables X.vocs.normalize_inputs(X.data).plot(*X.vocs.variable_names, kind=\"scatter\") Out[15]: <pre>&lt;Axes: xlabel='x1', ylabel='x2'&gt;</pre>"},{"location":"examples/basic/xopt_basic/#xopt-basic-example","title":"Xopt basic example\u00b6","text":"<p>Xopt optimization problems can be defined via one of two methods:</p> <ul> <li>a yaml text file (for limiting the amount of python script writing and/or setting up simulation runs)</li> <li>a simple python script (for those who prefer to use python directly)</li> </ul> <p>Here we will demonstrate how both of these techniques can be used to solve a relatively simple  constrained optimization problem.</p> <p>$n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objective:</p> <ul> <li>$f(x) = \\sum_i x_i$</li> </ul> <p>Constraint:</p> <ul> <li>$g(x) = -x_1^2 -x_2^2 + 1 \\le 0$</li> </ul>"},{"location":"examples/basic/xopt_basic/#xopt-components","title":"Xopt Components\u00b6","text":"<p>The definition of the Xopt object requires 3 parts, listed below:</p> <ul> <li>The <code>Evaluator</code> object, which evaluates input points using the arbitrary function specified by the <code>function</code> property.</li> <li>The <code>Generator</code> object, which, when given data that has been evaluated, generates future points to evaluate using the evaluator.</li> <li>The <code>VOCS</code> (variables, objectives, constraints, statics) object, which specifies the input domain, the objectives, constraints and constants passed to the evaluator function.</li> </ul>"},{"location":"examples/basic/xopt_basic/#defining-xopt-components-using-python","title":"Defining Xopt components using python\u00b6","text":"<p>We first examine how one would create and configure and Xopt optimization run using python. This can also be done via a YAML file (see the next section).</p>"},{"location":"examples/basic/xopt_basic/#define-the-objective-function-and-the-evaluator","title":"Define the objective function and the evaluator\u00b6","text":"<p>Note that the objective function takes in a dict of variable values and returns a dict of objective return values. The keys of the input and output dictionaries must contain the keys we will specify in VOCS (see below).</p>"},{"location":"examples/basic/xopt_basic/#define-vocs","title":"Define VOCS\u00b6","text":"<p>Here we define the names and ranges of input parameters, the names and settings of objectives, and the names and settings of constraints. Note that the keys here should be referenced in the evaluate function above.</p>"},{"location":"examples/basic/xopt_basic/#define-the-generator","title":"Define the Generator\u00b6","text":"<p>First lets see which generators are available for use.</p>"},{"location":"examples/basic/xopt_basic/#combine-into-xopt-object","title":"Combine into Xopt object\u00b6","text":""},{"location":"examples/basic/xopt_basic/#defining-xopt-object-from-yaml-file","title":"Defining Xopt object from yaml file\u00b6","text":"<p>Alternatively, it might be more useful to define the Xopt object from a text file or YAML string. We replicate the code above with the YAML file below.</p>"},{"location":"examples/basic/xopt_basic/#introspection","title":"Introspection\u00b6","text":"<p>Objects in Xopt can be printed to a string or dumped to a text file for easy introspection of attributes and current configuration.</p>"},{"location":"examples/basic/xopt_basic/#evaluating-randomly-generated-or-fixed-inputs","title":"Evaluating randomly generated or fixed inputs.\u00b6","text":"<p>The main Xopt object has a variety of means for evaluating random or fixed points. This is often used to initialize optimization, but can be used independently of any generator. Results from evaluations are stored in the <code>data</code> attribute. Data can also be explictly added to the Xopt object (and by extension the generator attached to the xopt object by calling <code>X.add_data()</code>.</p>"},{"location":"examples/basic/xopt_basic/#optimization","title":"Optimization\u00b6","text":"<p>Xopt conducts a single iteration of optimization by calling <code>X.step()</code>. Inside this function Xopt will generate a point (or set of points) using the generator object, then send the point to be evaluated by the evaluator. Results will be stored in the data attribute.</p>"},{"location":"examples/basic/xopt_basic/#visualization","title":"Visualization\u00b6","text":"<p>Finally, we can visualize the objectives and variables to monitor optimization or visualize the results</p>"},{"location":"examples/basic/xopt_evaluator/","title":"Xopt Evaluator Basic Usage","text":"In\u00a0[1]: Copied! <pre># needed for macos\nimport platform\nif platform.system() == \"Darwin\": import multiprocessing;multiprocessing.set_start_method(\"fork\")\n</pre> # needed for macos import platform if platform.system() == \"Darwin\": import multiprocessing;multiprocessing.set_start_method(\"fork\")  In\u00a0[2]: Copied! <pre>from xopt import Xopt, Evaluator, VOCS\nfrom xopt.generators.random import RandomGenerator\n\nimport pandas as pd\n\nfrom time import sleep\nfrom numpy.random import randint\n\nfrom typing import Dict\n\nimport numpy as np\nnp.random.seed(666) # for reproducibility\n</pre> from xopt import Xopt, Evaluator, VOCS from xopt.generators.random import RandomGenerator  import pandas as pd  from time import sleep from numpy.random import randint  from typing import Dict  import numpy as np np.random.seed(666) # for reproducibility <p>Define a custom function <code>f(inputs: Dict) -&gt; outputs: Dict</code>.</p> In\u00a0[3]: Copied! <pre>def f(inputs: Dict, enable_errors=True) -&gt; Dict:\n\n    sleep(randint(1, 5)*.1)  # simulate computation time\n    # Make some occasional errors\n    if enable_errors and np.any(inputs[\"x\"] &gt; 0.8):\n        raise ValueError(\"x &gt; 0.8\")\n\n    return {\"f1\": inputs[\"x\"] ** 2 + inputs[\"y\"] ** 2}\n</pre> def f(inputs: Dict, enable_errors=True) -&gt; Dict:      sleep(randint(1, 5)*.1)  # simulate computation time     # Make some occasional errors     if enable_errors and np.any(inputs[\"x\"] &gt; 0.8):         raise ValueError(\"x &gt; 0.8\")      return {\"f1\": inputs[\"x\"] ** 2 + inputs[\"y\"] ** 2} <p>Define variables, objectives, constraints, and other settings (VOCS)</p> In\u00a0[4]: Copied! <pre>vocs = VOCS(variables={\"x\": [0, 1], \"y\": [0, 1]}, objectives={\"f1\": \"MINIMIZE\"})\nvocs\n</pre> vocs = VOCS(variables={\"x\": [0, 1], \"y\": [0, 1]}, objectives={\"f1\": \"MINIMIZE\"}) vocs   Out[4]: <pre>VOCS(variables={'x': [0.0, 1.0], 'y': [0.0, 1.0]}, constraints={}, objectives={'f1': 'MINIMIZE'}, constants={}, observables=[])</pre> <p>This can be used to make some random inputs for evaluating the function.</p> In\u00a0[5]: Copied! <pre>in1 = vocs.random_inputs()[0]\n\nf(in1, enable_errors=False)\n</pre> in1 = vocs.random_inputs()[0]  f(in1, enable_errors=False) Out[5]: <pre>{'f1': array([0.11401572])}</pre> In\u00a0[6]: Copied! <pre># Add in occasional errors. \ntry:\n    f({\"x\": 1, \"y\": 0})\nexcept Exception as ex:\n    print(f\"Caught error in f: {ex}\")\n</pre> # Add in occasional errors.  try:     f({\"x\": 1, \"y\": 0}) except Exception as ex:     print(f\"Caught error in f: {ex}\") <pre>Caught error in f: x &gt; 0.8\n</pre> In\u00a0[7]: Copied! <pre># Create Evaluator\nev = Evaluator(function=f)\n</pre> # Create Evaluator ev = Evaluator(function=f) In\u00a0[8]: Copied! <pre># Single input evaluation\nev.evaluate(in1)\n</pre> # Single input evaluation ev.evaluate(in1) Out[8]: <pre>{'f1': array([0.11401572]),\n 'xopt_runtime': 0.2003169939999907,\n 'xopt_error': False}</pre> In\u00a0[9]: Copied! <pre># Dataframe evaluation\nin10 = pd.DataFrame({\n    \"x\":np.linspace(0,1,10),\n    \"y\":np.linspace(0,1,10)\n})\nev.evaluate_data(in10)\n</pre> # Dataframe evaluation in10 = pd.DataFrame({     \"x\":np.linspace(0,1,10),     \"y\":np.linspace(0,1,10) }) ev.evaluate_data(in10)  Out[9]: x y f1 xopt_runtime xopt_error xopt_error_str 0 0.000000 0.000000 0.000000 0.300409 False NaN 1 0.111111 0.111111 0.024691 0.100190 False NaN 2 0.222222 0.222222 0.098765 0.200295 False NaN 3 0.333333 0.333333 0.222222 0.400505 False NaN 4 0.444444 0.444444 0.395062 0.300408 False NaN 5 0.555556 0.555556 0.617284 0.400492 False NaN 6 0.666667 0.666667 0.888889 0.100190 False NaN 7 0.777778 0.777778 1.209877 0.400518 False NaN 8 0.888889 0.888889 NaN 0.100539 True Traceback (most recent call last):\\n  File \"/u... 9 1.000000 1.000000 NaN 0.300479 True Traceback (most recent call last):\\n  File \"/u... In\u00a0[10]: Copied! <pre># Dataframe evaluation, vectorized\nev.vectorized = True\nev.evaluate_data(in10)\n</pre> # Dataframe evaluation, vectorized ev.vectorized = True ev.evaluate_data(in10)  Out[10]: x y xopt_runtime xopt_error xopt_error_str 0 0.000000 0.000000 0.200599 True Traceback (most recent call last):\\n  File \"/u... 1 0.111111 0.111111 0.200599 True Traceback (most recent call last):\\n  File \"/u... 2 0.222222 0.222222 0.200599 True Traceback (most recent call last):\\n  File \"/u... 3 0.333333 0.333333 0.200599 True Traceback (most recent call last):\\n  File \"/u... 4 0.444444 0.444444 0.200599 True Traceback (most recent call last):\\n  File \"/u... 5 0.555556 0.555556 0.200599 True Traceback (most recent call last):\\n  File \"/u... 6 0.666667 0.666667 0.200599 True Traceback (most recent call last):\\n  File \"/u... 7 0.777778 0.777778 0.200599 True Traceback (most recent call last):\\n  File \"/u... 8 0.888889 0.888889 0.200599 True Traceback (most recent call last):\\n  File \"/u... 9 1.000000 1.000000 0.200599 True Traceback (most recent call last):\\n  File \"/u... In\u00a0[11]: Copied! <pre>from concurrent.futures import ProcessPoolExecutor\nMAX_WORKERS = 10\n</pre> from concurrent.futures import ProcessPoolExecutor MAX_WORKERS = 10 In\u00a0[12]: Copied! <pre># Create Executor instance\nexecutor = ProcessPoolExecutor(max_workers=MAX_WORKERS)\nexecutor\n</pre> # Create Executor instance executor = ProcessPoolExecutor(max_workers=MAX_WORKERS) executor Out[12]: <pre>&lt;concurrent.futures.process.ProcessPoolExecutor at 0x7fa90713d700&gt;</pre> In\u00a0[13]: Copied! <pre># Dask (Optional)\n# from dask.distributed import Client\n# import logging\n# client = Client( silence_logs=logging.ERROR)\n# executor = client.get_executor()\n# client\n</pre> # Dask (Optional) # from dask.distributed import Client # import logging # client = Client( silence_logs=logging.ERROR) # executor = client.get_executor() # client In\u00a0[14]: Copied! <pre># This calls `executor.map`\nev = Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS)\n</pre> # This calls `executor.map` ev = Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS) In\u00a0[15]: Copied! <pre># This will run in parallel\nev.evaluate_data(in10)\n</pre> # This will run in parallel ev.evaluate_data(in10) Out[15]: x y f1 xopt_runtime xopt_error xopt_error_str 0 0.000000 0.000000 0.000000 0.400931 False NaN 1 0.111111 0.111111 0.024691 0.400837 False NaN 2 0.222222 0.222222 0.098765 0.400854 False NaN 3 0.333333 0.333333 0.222222 0.400846 False NaN 4 0.444444 0.444444 0.395062 0.400905 False NaN 5 0.555556 0.555556 0.617284 0.401084 False NaN 6 0.666667 0.666667 0.888889 0.401409 False NaN 7 0.777778 0.777778 1.209877 0.400448 False NaN 8 0.888889 0.888889 NaN 0.401222 True Traceback (most recent call last):\\n  File \"/u... 9 1.000000 1.000000 NaN 0.401222 True Traceback (most recent call last):\\n  File \"/u... In\u00a0[16]: Copied! <pre>X = Xopt(generator=RandomGenerator(vocs=vocs), evaluator=Evaluator(function=f),\n         vocs=vocs)\nX.strict = False\n\n# Evaluate to the evaluator some new inputs\nX.evaluate_data(X.vocs.random_inputs(4))\n</pre> X = Xopt(generator=RandomGenerator(vocs=vocs), evaluator=Evaluator(function=f),          vocs=vocs) X.strict = False  # Evaluate to the evaluator some new inputs X.evaluate_data(X.vocs.random_inputs(4))  Out[16]: x y f1 xopt_runtime xopt_error xopt_error_str 0 0.491934 0.299155 0.331493 0.100208 False NaN 1 0.799752 0.706772 1.139131 0.100191 False NaN 2 0.255846 0.225521 0.116317 0.100631 False NaN 3 0.807108 0.994891 NaN 0.100472 True Traceback (most recent call last):\\n  File \"/u... In\u00a0[17]: Copied! <pre># Usage with a parallel executor.\nfrom xopt import AsynchronousXopt\n\nexecutor = ProcessPoolExecutor(max_workers=MAX_WORKERS)\n\nX2 = AsynchronousXopt(\n    generator=RandomGenerator(vocs=vocs),\n    evaluator=Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS),\n    vocs=vocs,\n)\nX2.strict = False\n</pre> # Usage with a parallel executor. from xopt import AsynchronousXopt  executor = ProcessPoolExecutor(max_workers=MAX_WORKERS)  X2 = AsynchronousXopt(     generator=RandomGenerator(vocs=vocs),     evaluator=Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS),     vocs=vocs, ) X2.strict = False In\u00a0[18]: Copied! <pre>X2.step()\n</pre> X2.step() In\u00a0[19]: Copied! <pre>for _ in range(20):\n    X2.step()\n\nlen(X2.data)\n</pre> for _ in range(20):     X2.step()  len(X2.data) Out[19]: <pre>60</pre> In\u00a0[20]: Copied! <pre>X2.data.plot.scatter(\"x\", \"y\")\n</pre> X2.data.plot.scatter(\"x\", \"y\") Out[20]: <pre>&lt;Axes: xlabel='x', ylabel='y'&gt;</pre> In\u00a0[21]: Copied! <pre># Asynchronous, Vectorized\nX2 = AsynchronousXopt(\n    generator=RandomGenerator(vocs=vocs),\n    evaluator=Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS),\n    vocs=vocs,\n)\nX2.evaluator.vectorized = True\nX2.strict = False\n\n# This takes fewer steps to achieve a similar number of evaluations\nfor _ in range(3):\n    X2.step()\n\nlen(X2.data)\n</pre> # Asynchronous, Vectorized X2 = AsynchronousXopt(     generator=RandomGenerator(vocs=vocs),     evaluator=Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS),     vocs=vocs, ) X2.evaluator.vectorized = True X2.strict = False  # This takes fewer steps to achieve a similar number of evaluations for _ in range(3):     X2.step()  len(X2.data) Out[21]: <pre>30</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/basic/xopt_evaluator/#xopt-evaluator-basic-usage","title":"Xopt Evaluator Basic Usage\u00b6","text":"<p>The <code>Evaluator</code> handles the execution of the user-provided <code>function</code> with optional <code>function_kwags</code>, asyncrhonously and parallel, with exception handling.</p>"},{"location":"examples/basic/xopt_evaluator/#executors","title":"Executors\u00b6","text":""},{"location":"examples/basic/xopt_evaluator/#evaluator-in-the-xopt-object","title":"Evaluator in the Xopt object\u00b6","text":""},{"location":"examples/basic/xopt_evaluator/#asynchronous-xopt","title":"Asynchronous Xopt\u00b6","text":"<p>Instead of waiting for evaluations to be finished, AsynchronousXopt can be used to generate candidates while waiting for other evaluations to finish (requires parallel execution). In this case, calling <code>X.step()</code> generates and executes a number of candidates that are executed in parallel using python <code>concurrent.futures</code> formalism. Calling <code>X.step()</code> again will generate and evaluate new points based on finished futures asynchronously.</p>"},{"location":"examples/basic/xopt_generator/","title":"Working with Xopt generators","text":"In\u00a0[1]: Copied! <pre># Import the class\nfrom xopt.generators import generators, get_generator\n</pre> # Import the class from xopt.generators import generators, get_generator In\u00a0[2]: Copied! <pre># named generators\ngenerators.keys()\n</pre> # named generators generators.keys() Out[2]: <pre>dict_keys(['random'])</pre> In\u00a0[3]: Copied! <pre># get default options for the upper confidence bound generator\ngenerator_type = get_generator(\"upper_confidence_bound\")\n</pre> # get default options for the upper confidence bound generator generator_type = get_generator(\"upper_confidence_bound\") In\u00a0[4]: Copied! <pre># define vocs for the problem\nfrom xopt.vocs import VOCS\nimport math\n\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> # define vocs for the problem from xopt.vocs import VOCS import math  vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[5]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\n\ndef test_function(input_dict):\n    return {\"f\": np.sin(input_dict[\"x\"])}\n</pre> # define a test function to optimize import numpy as np   def test_function(input_dict):     return {\"f\": np.sin(input_dict[\"x\"])} In\u00a0[6]: Copied! <pre># create xopt evaluator and run the optimization\nfrom xopt import Evaluator, Xopt\n\nevaluator = Evaluator(function=test_function)\ngenerator = generator_type(vocs=vocs)\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX\n</pre> # create xopt evaluator and run the optimization from xopt import Evaluator, Xopt  evaluator = Evaluator(function=test_function) generator = generator_type(vocs=vocs) X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X Out[6]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: __main__.test_function\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  beta: 2.0\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: upper_confidence_bound\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  supports_batch_generation: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints: {}\n  objectives:\n    f: MINIMIZE\n  observables: []\n  variables:\n    x:\n    - 0.0\n    - 6.283185307179586\n</pre> In\u00a0[7]: Copied! <pre># run the optimization for a couple of iterations (see bayes_opt folder for\n# more examples of ucb)\nX.random_evaluate(2)\nfor i in range(4):\n    X.step()\n</pre> # run the optimization for a couple of iterations (see bayes_opt folder for # more examples of ucb) X.random_evaluate(2) for i in range(4):     X.step() In\u00a0[8]: Copied! <pre>X.data\n</pre> X.data Out[8]: x f xopt_runtime xopt_error 0 0.846541 7.489931e-01 0.000016 False 1 2.131751 8.467478e-01 0.000002 False 2 6.283185 -2.449294e-16 0.000007 False 3 5.226709 -8.706273e-01 0.000007 False 4 4.360516 -9.387288e-01 0.000007 False 5 4.750919 -9.992578e-01 0.000007 False In\u00a0[8]: Copied! <pre>\n</pre> In\u00a0[8]: Copied! <pre>\n</pre>"},{"location":"examples/basic/xopt_generator/#working-with-xopt-generators","title":"Working with Xopt generators\u00b6","text":""},{"location":"examples/basic/xopt_parallel/","title":"Xopt Parallel Examples","text":"In\u00a0[1]: Copied! <pre>from xopt import AsynchronousXopt as Xopt\n</pre> from xopt import AsynchronousXopt as Xopt In\u00a0[2]: Copied! <pre># Helpers for this notebook\nimport multiprocessing\nN_CPUS=multiprocessing.cpu_count()\nN_CPUS\n\nimport os\n\n# directory for data. \nos.makedirs(\"temp\", exist_ok=True)\n\n# Notebook printing output\n#from xopt import output_notebook\n#output_notebook()\n\n# Nicer plotting\n%config InlineBackend.figure_format = 'retina'\n</pre> # Helpers for this notebook import multiprocessing N_CPUS=multiprocessing.cpu_count() N_CPUS  import os  # directory for data.  os.makedirs(\"temp\", exist_ok=True)  # Notebook printing output #from xopt import output_notebook #output_notebook()  # Nicer plotting %config InlineBackend.figure_format = 'retina'  <p>The <code>Xopt</code> object can be instantiated from a JSON or YAML file, or a dict, with the proper structure.</p> <p>Here we will make one</p> In\u00a0[3]: Copied! <pre># Make a proper input file.\nYAML = \"\"\"\n\nmax_evaluations: 1000\n\ngenerator:\n  name: cnsga\n  output_path: temp\n  population_size:  64\n  \nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    sleep: 0\n    random_sleep: 0.1\n  \nvocs:\n  variables:\n    x1: [0, 3.14159]\n    x2: [0, 3.14159]\n  objectives: {y1: MINIMIZE, y2: MINIMIZE}\n  constraints:\n    c1: [GREATER_THAN, 0]\n    c2: [LESS_THAN, 0.5]\n  constants: {a: dummy_constant}\n\n\"\"\"\nX = Xopt(YAML)\nX\n</pre> # Make a proper input file. YAML = \"\"\"  max_evaluations: 1000  generator:   name: cnsga   output_path: temp   population_size:  64    evaluator:   function: xopt.resources.test_functions.tnk.evaluate_TNK   function_kwargs:     sleep: 0     random_sleep: 0.1    vocs:   variables:     x1: [0, 3.14159]     x2: [0, 3.14159]   objectives: {y1: MINIMIZE, y2: MINIMIZE}   constraints:     c1: [GREATER_THAN, 0]     c2: [LESS_THAN, 0.5]   constants: {a: dummy_constant}  \"\"\" X = Xopt(YAML) X Out[3]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0\n    random_sleep: 0.1\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  crossover_probability: 0.9\n  mutation_probability: 1.0\n  name: cnsga\n  output_path: temp\n  population: null\n  population_file: null\n  population_size: 64\n  supports_multi_objective: true\nis_done: false\nmax_evaluations: 1000\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives:\n    y1: MINIMIZE\n    y2: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[4]: Copied! <pre>%%timeit\n# Check that the average time is close to random_sleep\nX.evaluator.function({\"x1\": 0.5, \"x2\": 0.5}, random_sleep = .1)\n</pre> %%timeit # Check that the average time is close to random_sleep X.evaluator.function({\"x1\": 0.5, \"x2\": 0.5}, random_sleep = .1) <pre>105 ms \u00b1 7.71 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[5]: Copied! <pre>%%time\nX.run()\n</pre> %%time X.run() <pre>CPU times: user 3.73 s, sys: 26.4 ms, total: 3.75 s\nWall time: 1min 41s\n</pre> In\u00a0[6]: Copied! <pre>from concurrent.futures import ProcessPoolExecutor\n</pre> from concurrent.futures import ProcessPoolExecutor In\u00a0[7]: Copied! <pre>%%time\nX = Xopt(YAML)\n\nwith ProcessPoolExecutor(max_workers=N_CPUS) as executor:\n    X.evaluator.executor = executor\n    X.evaluator.max_workers = N_CPUS\n    X.run()\nlen(X.data)\n</pre> %%time X = Xopt(YAML)  with ProcessPoolExecutor(max_workers=N_CPUS) as executor:     X.evaluator.executor = executor     X.evaluator.max_workers = N_CPUS     X.run() len(X.data) <pre>CPU times: user 3.66 s, sys: 163 ms, total: 3.83 s\nWall time: 26.5 s\n</pre> Out[7]: <pre>1000</pre> In\u00a0[8]: Copied! <pre>from concurrent.futures import ThreadPoolExecutor\n</pre> from concurrent.futures import ThreadPoolExecutor In\u00a0[9]: Copied! <pre>%%time\nX = Xopt(YAML)\n\nwith ThreadPoolExecutor(max_workers=N_CPUS) as executor:\n    X.evaluator.executor = executor\n    X.evaluator.max_workers = N_CPUS\n    X.run()\nlen(X.data)\n</pre> %%time X = Xopt(YAML)  with ThreadPoolExecutor(max_workers=N_CPUS) as executor:     X.evaluator.executor = executor     X.evaluator.max_workers = N_CPUS     X.run() len(X.data) <pre>CPU times: user 3.43 s, sys: 86.5 ms, total: 3.52 s\nWall time: 25.6 s\n</pre> Out[9]: <pre>1000</pre> In\u00a0[10]: Copied! <pre>X = Xopt(YAML)\nX.dump('test.yaml') # Write this input to file\n!cat test.yaml\n</pre> X = Xopt(YAML) X.dump('test.yaml') # Write this input to file !cat test.yaml <pre>data: null\r\ndump_file: null\r\nevaluator:\r\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\r\n  function_kwargs:\r\n    raise_probability: 0\r\n    random_sleep: 0.1\r\n    sleep: 0\r\n  max_workers: 1\r\n  vectorized: false\r\ngenerator:\r\n  crossover_probability: 0.9\r\n  mutation_probability: 1.0\r\n  name: cnsga\r\n  output_path: temp\r\n  population: null\r\n  population_file: null\r\n  population_size: 64\r\n  supports_multi_objective: true\r\nis_done: false\r\nmax_evaluations: 1000\r\nserialize_inline: false\r\nserialize_torch: false\r\nstrict: true\r\nvocs:\r\n  constants:\r\n    a: dummy_constant\r\n  constraints:\r\n    c1:\r\n    - GREATER_THAN\r\n    - 0.0\r\n    c2:\r\n    - LESS_THAN\r\n    - 0.5\r\n  objectives:\r\n    y1: MINIMIZE\r\n    y2: MINIMIZE\r\n  observables: []\r\n  variables:\r\n    x1:\r\n    - 0.0\r\n    - 3.14159\r\n    x2:\r\n    - 0.0\r\n    - 3.14159\r\n</pre> In\u00a0[11]: Copied! <pre>%%time\n!mpirun -n 8 python -m mpi4py.futures -m xopt.mpi.run -vv --logfile xopt.log test.yaml\n</pre> %%time !mpirun -n 8 python -m mpi4py.futures -m xopt.mpi.run -vv --logfile xopt.log test.yaml <pre>Namespace(input_file='test.yaml', logfile='xopt.log', verbose=2, asynchronous=True)\r\nParallel execution with 8 workers\r\nEnabling async mode\r\nInitialized generator cnsga\r\nCreated toolbox with 2 variables, 2 constraints, and 2 objectives.\r\n    Using selection algorithm: nsga2\r\n\r\n            Xopt\r\n________________________________\r\nVersion: 0+untagged.1.g7a7eff9\r\nData size: 0\r\nConfig as YAML:\r\ndump_file: null\r\nevaluator:\r\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\r\n  function_kwargs:\r\n    raise_probability: 0\r\n    random_sleep: 0.1\r\n    sleep: 0\r\n  max_workers: 1\r\n  vectorized: false\r\ngenerator:\r\n  crossover_probability: 0.9\r\n  mutation_probability: 1.0\r\n  name: cnsga\r\n  output_path: temp\r\n  population: null\r\n  population_file: null\r\n  population_size: 64\r\n  supports_multi_objective: true\r\nis_done: false\r\nmax_evaluations: 1000\r\nserialize_inline: false\r\nserialize_torch: false\r\nstrict: true\r\nvocs:\r\n  constants:\r\n    a: dummy_constant\r\n  constraints:\r\n    c1:\r\n    - GREATER_THAN\r\n    - 0.0\r\n    c2:\r\n    - LESS_THAN\r\n    - 0.5\r\n  objectives:\r\n    y1: MINIMIZE\r\n    y2: MINIMIZE\r\n  observables: []\r\n  variables:\r\n    x1:\r\n    - 0.0\r\n    - 3.14159\r\n    x2:\r\n    - 0.0\r\n    - 3.14159\r\n\r\n\r\n</pre> <pre>Xopt is done. Max evaluations 1000 reached.\r\n</pre> <pre>CPU times: user 193 ms, sys: 39.8 ms, total: 233 ms\nWall time: 22.2 s\n</pre> In\u00a0[12]: Copied! <pre>!tail xopt.log\n</pre> !tail xopt.log <pre>2024-02-23T21:59:28+0000 - xopt - INFO - Parallel execution with 8 workers\r\n2024-02-23T21:59:28+0000 - xopt - INFO - Enabling async mode\r\n2024-02-23T21:59:28+0000 - xopt.generator - INFO - Initialized generator cnsga\r\n2024-02-23T21:59:28+0000 - xopt.generators.ga.cnsga - INFO - Created toolbox with 2 variables, 2 constraints, and 2 objectives.\r\n2024-02-23T21:59:28+0000 - xopt.generators.ga.cnsga - INFO -     Using selection algorithm: nsga2\r\n2024-02-23T21:59:47+0000 - xopt.base - INFO - Xopt is done. Max evaluations 1000 reached.\r\n</pre> In\u00a0[13]: Copied! <pre>from dask.distributed import Client\nclient = Client()\nexecutor = client.get_executor()\nclient\n</pre> from dask.distributed import Client client = Client() executor = client.get_executor() client Out[13]: Client <p>Client-dd29edea-d296-11ee-8c5f-6045bd84cfed</p> Connection method: Cluster object Cluster type: distributed.LocalCluster Dashboard:  http://127.0.0.1:8787/status Cluster Info LocalCluster <p>6992af24</p> Dashboard: http://127.0.0.1:8787/status Workers: 4                  Total threads: 4                  Total memory: 15.61 GiB                  Status: running Using processes: True Scheduler Info Scheduler <p>Scheduler-173853a7-172e-445b-83d7-0d3d89d9198c</p> Comm: tcp://127.0.0.1:34137                      Workers: 4                      Dashboard: http://127.0.0.1:8787/status Total threads: 4                      Started: Just now                      Total memory: 15.61 GiB                      Workers Worker: 0 Comm:  tcp://127.0.0.1:44345                          Total threads:  1                          Dashboard:  http://127.0.0.1:40891/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:45729                          Local directory:  /tmp/dask-scratch-space/worker-95gpdntg                          Worker: 1 Comm:  tcp://127.0.0.1:45431                          Total threads:  1                          Dashboard:  http://127.0.0.1:36433/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:41813                          Local directory:  /tmp/dask-scratch-space/worker-bebpbt96                          Worker: 2 Comm:  tcp://127.0.0.1:34203                          Total threads:  1                          Dashboard:  http://127.0.0.1:35711/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:43401                          Local directory:  /tmp/dask-scratch-space/worker-t1yvdy85                          Worker: 3 Comm:  tcp://127.0.0.1:41951                          Total threads:  1                          Dashboard:  http://127.0.0.1:33759/status Memory:  3.90 GiB                          Nanny:  tcp://127.0.0.1:40683                          Local directory:  /tmp/dask-scratch-space/worker-f4696sjj                          In\u00a0[14]: Copied! <pre>%%time\nX = Xopt(YAML)\nX.evaluator.executor = executor\nX.evaluator.max_workers = N_CPUS\nX.run()\nlen(X.data)\n</pre> %%time X = Xopt(YAML) X.evaluator.executor = executor X.evaluator.max_workers = N_CPUS X.run() len(X.data) <pre>CPU times: user 7.74 s, sys: 782 ms, total: 8.52 s\nWall time: 29.1 s\n</pre> Out[14]: <pre>1000</pre> In\u00a0[15]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[16]: Copied! <pre>X.data\n</pre> X.data Out[16]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 1 1.369358 2.673874 dummy_constant 1.369358 2.673874 7.996997 5.481513 0.095909 False 1 2.906510 1.898907 dummy_constant 2.906510 1.898907 11.152283 7.748230 0.058479 False 2 2.207341 2.617207 dummy_constant 2.207341 2.617207 10.700811 7.397577 0.029413 False 3 0.883415 0.877340 dummy_constant 0.883415 0.877340 0.450300 0.289393 0.131576 False 4 0.607510 0.484998 dummy_constant 0.607510 0.484998 -0.374281 0.011783 0.022149 False ... ... ... ... ... ... ... ... ... ... 995 1.231872 0.139094 dummy_constant 1.231872 0.139094 0.559477 0.665889 0.197200 False 996 0.484834 0.877340 dummy_constant 0.484834 0.877340 0.026966 0.142615 0.194204 False 997 0.320179 0.945225 dummy_constant 0.320179 0.945225 -0.053136 0.230561 0.054183 False 998 0.586395 1.054059 dummy_constant 0.586395 1.054059 0.481471 0.314446 0.178908 False 999 1.013621 0.240894 dummy_constant 1.013621 0.240894 0.168459 0.330943 0.028450 False <p>1000 rows \u00d7 9 columns</p> In\u00a0[17]: Copied! <pre>df = pd.concat([X.data, X.vocs.feasibility_data(X.data)], axis=1)\ndf[df['feasible']]\n</pre> df = pd.concat([X.data, X.vocs.feasibility_data(X.data)], axis=1) df[df['feasible']] Out[17]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error feasible_c1 feasible_c2 feasible 3 0.883415 0.877340 dummy_constant 0.883415 0.877340 0.450300 0.289393 0.131576 False True True True 5 0.993110 0.591105 dummy_constant 0.993110 0.591105 0.402823 0.251458 0.100658 False True True True 69 0.602128 0.916660 dummy_constant 0.602128 0.916660 0.302033 0.184035 0.051751 False True True True 75 0.787880 0.551563 dummy_constant 0.787880 0.551563 0.019000 0.085534 0.161937 False True True True 117 1.130671 0.670071 dummy_constant 1.130671 0.670071 0.792274 0.426670 0.029477 False True True True ... ... ... ... ... ... ... ... ... ... ... ... ... 992 0.752194 0.923866 dummy_constant 0.752194 0.923866 0.425553 0.243264 0.066124 False True True True 994 0.261964 0.952600 dummy_constant 0.261964 0.952600 0.016715 0.261508 0.027097 False True True True 996 0.484834 0.877340 dummy_constant 0.484834 0.877340 0.026966 0.142615 0.194204 False True True True 998 0.586395 1.054059 dummy_constant 0.586395 1.054059 0.481471 0.314446 0.178908 False True True True 999 1.013621 0.240894 dummy_constant 1.013621 0.240894 0.168459 0.330943 0.028450 False True True True <p>462 rows \u00d7 12 columns</p> In\u00a0[18]: Copied! <pre># Plot the feasible ones\nfeasible_df = df[df[\"feasible\"]]\nfeasible_df.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\")\n</pre> # Plot the feasible ones feasible_df = df[df[\"feasible\"]] feasible_df.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\") In\u00a0[19]: Copied! <pre># Plot the infeasible ones\ninfeasible_df = df[~df[\"feasible\"]]\ninfeasible_df.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\")\n</pre> # Plot the infeasible ones infeasible_df = df[~df[\"feasible\"]] infeasible_df.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\") In\u00a0[20]: Copied! <pre># This is the final population\ndf1 = X.generator.population\ndf1.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\")\n</pre> # This is the final population df1 = X.generator.population df1.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\") In\u00a0[21]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[22]: Copied! <pre># Extract objectives from output\nk1, k2 = \"y1\", \"y2\"\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nax.scatter(\n    infeasible_df[k1],\n    infeasible_df[k2],\n    color=\"blue\",\n    marker=\".\",\n    alpha=0.5,\n    label=\"infeasible\",\n)\nax.scatter(\n    feasible_df[k1], feasible_df[k2], color=\"orange\", marker=\".\", label=\"feasible\"\n)\nax.scatter(df1[k1], df1[k2], color=\"red\", marker=\".\", label=\"final population\")\nax.set_xlabel(k1)\nax.set_ylabel(k2)\nax.set_aspect(\"auto\")\nax.set_title(f\"Xopt's CNSGA algorithm\")\nplt.legend()\n</pre> # Extract objectives from output k1, k2 = \"y1\", \"y2\"  fig, ax = plt.subplots(figsize=(6, 6))  ax.scatter(     infeasible_df[k1],     infeasible_df[k2],     color=\"blue\",     marker=\".\",     alpha=0.5,     label=\"infeasible\", ) ax.scatter(     feasible_df[k1], feasible_df[k2], color=\"orange\", marker=\".\", label=\"feasible\" ) ax.scatter(df1[k1], df1[k2], color=\"red\", marker=\".\", label=\"final population\") ax.set_xlabel(k1) ax.set_ylabel(k2) ax.set_aspect(\"auto\") ax.set_title(f\"Xopt's CNSGA algorithm\") plt.legend() Out[22]: <pre>&lt;matplotlib.legend.Legend at 0x7f2e2480ee50&gt;</pre> In\u00a0[23]: Copied! <pre># Cleanup\n#!rm -r dask-worker-space\n!rm -r temp\n!rm xopt.log*\n!rm test.yaml\n</pre> # Cleanup #!rm -r dask-worker-space !rm -r temp !rm xopt.log* !rm test.yaml"},{"location":"examples/basic/xopt_parallel/#xopt-parallel-examples","title":"Xopt Parallel Examples\u00b6","text":"<p>Xopt provides methods to parallelize optimizations using Processes, Threads, MPI, and Dask using the <code>concurrent.futures</code> interface as defined in  https://www.python.org/dev/peps/pep-3148/ .</p>"},{"location":"examples/basic/xopt_parallel/#processes","title":"Processes\u00b6","text":""},{"location":"examples/basic/xopt_parallel/#threads","title":"Threads\u00b6","text":"<p>Continue running, this time with threads.</p>"},{"location":"examples/basic/xopt_parallel/#mpi","title":"MPI\u00b6","text":"<p>The <code>test.yaml</code> file completely defines the problem. We will also direct the logging to an <code>xopt.log</code> file. The following invocation recruits 4 MPI workers to solve this problem.</p> <p>We can also continue by calling <code>.save</code> with a JSON filename. This will write all of previous results into the file.</p>"},{"location":"examples/basic/xopt_parallel/#dask","title":"Dask\u00b6","text":""},{"location":"examples/basic/xopt_parallel/#load-output-into-pandas","title":"Load output into Pandas\u00b6","text":"<p>This algorithm writes two types of files: <code>gen_{i}.json</code> with all of the new individuals evaluated in a generation, and <code>pop_{i}.json</code> with the latest best population. Xopt provides some functions to load these easily into a Pandas dataframe for further analysis.</p>"},{"location":"examples/basic/xopt_parallel/#matplotlib-plotting","title":"matplotlib plotting\u00b6","text":"<p>You can always use matplotlib for customizable plotting</p>"},{"location":"examples/basic/xopt_vocs/","title":"VOCS data structure","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\n</pre> from xopt.vocs import VOCS In\u00a0[2]: Copied! <pre>Y = \"\"\"\nvariables:\n  a: [0, 1e3] # Note that 1e3 usually parses as a str with YAML. \n  b: [-1, 1]\nobjectives:\n  c: maximize\n  d: minimize \nconstraints:\n  e: ['Less_than', 2]\n  f: ['greater_than', 0]\nconstants:\n  g: 1234\n\n\"\"\"\n\nvocs = VOCS.from_yaml(Y)\nvocs\n</pre> Y = \"\"\" variables:   a: [0, 1e3] # Note that 1e3 usually parses as a str with YAML.    b: [-1, 1] objectives:   c: maximize   d: minimize  constraints:   e: ['Less_than', 2]   f: ['greater_than', 0] constants:   g: 1234  \"\"\"  vocs = VOCS.from_yaml(Y) vocs Out[2]: <pre>VOCS(variables={'a': [0.0, 1000.0], 'b': [-1.0, 1.0]}, constraints={'e': ['LESS_THAN', 2.0], 'f': ['GREATER_THAN', 0.0]}, objectives={'c': 'MAXIMIZE', 'd': 'MINIMIZE'}, constants={'g': 1234}, observables=[])</pre> In\u00a0[3]: Copied! <pre># as dict\ndict(vocs)\n</pre> # as dict dict(vocs) Out[3]: <pre>{'variables': {'a': [0.0, 1000.0], 'b': [-1.0, 1.0]},\n 'constraints': {'e': ['LESS_THAN', 2.0], 'f': ['GREATER_THAN', 0.0]},\n 'objectives': {'c': 'MAXIMIZE', 'd': 'MINIMIZE'},\n 'constants': {'g': 1234},\n 'observables': []}</pre> In\u00a0[4]: Copied! <pre>#  re-parse dict\nvocs2 = VOCS.from_dict(dict(vocs))\n</pre> #  re-parse dict vocs2 = VOCS.from_dict(dict(vocs)) In\u00a0[5]: Copied! <pre># Check that these are the same\nvocs2 == vocs\n</pre> # Check that these are the same vocs2 == vocs Out[5]: <pre>True</pre> In\u00a0[6]: Copied! <pre># This replaces the old vocs[\"variables\"]\ngetattr(vocs, \"variables\")\n</pre> # This replaces the old vocs[\"variables\"] getattr(vocs, \"variables\") Out[6]: <pre>{'a': [0.0, 1000.0], 'b': [-1.0, 1.0]}</pre> In\u00a0[7]: Copied! <pre>vocs.objectives[\"c\"] == 'MAXIMIZE'\n</pre> vocs.objectives[\"c\"] == 'MAXIMIZE' Out[7]: <pre>True</pre> In\u00a0[8]: Copied! <pre># json\nvocs.to_json()\n</pre> # json vocs.to_json() Out[8]: <pre>'{\"variables\":{\"a\":[0.0,1000.0],\"b\":[-1.0,1.0]},\"constraints\":{\"e\":[\"LESS_THAN\",2.0],\"f\":[\"GREATER_THAN\",0.0]},\"objectives\":{\"c\":\"MAXIMIZE\",\"d\":\"MINIMIZE\"},\"constants\":{\"g\":1234},\"observables\":[]}'</pre> In\u00a0[9]: Copied! <pre>from xopt.vocs import form_objective_data, form_constraint_data, form_feasibility_data\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame(vocs.random_inputs(10))\n# Add some outputs\ndata[\"c\"] = data[\"a\"] + data[\"b\"]\ndata[\"d\"] = data[\"a\"] - data[\"b\"]\ndata[\"e\"] = data[\"a\"] * 2 + data[\"b\"] * 2\ndata[\"f\"] = data[\"a\"] * 2 - data[\"b\"] * 2\ndata.index = np.arange(len(data)) + 5  # custom index\ndata\n</pre> from xopt.vocs import form_objective_data, form_constraint_data, form_feasibility_data import pandas as pd import numpy as np  data = pd.DataFrame(vocs.random_inputs(10)) # Add some outputs data[\"c\"] = data[\"a\"] + data[\"b\"] data[\"d\"] = data[\"a\"] - data[\"b\"] data[\"e\"] = data[\"a\"] * 2 + data[\"b\"] * 2 data[\"f\"] = data[\"a\"] * 2 - data[\"b\"] * 2 data.index = np.arange(len(data)) + 5  # custom index data Out[9]: a b g c d e f 5 597.647599 -0.841071 1234 596.806528 598.488670 1193.613056 1196.977341 6 555.463677 -0.099606 1234 555.364072 555.563283 1110.728143 1111.126566 7 926.777474 -0.994550 1234 925.782923 927.772024 1851.565847 1855.544048 8 45.366374 -0.225117 1234 45.141257 45.591491 90.282514 91.182982 9 791.863145 0.017353 1234 791.880498 791.845792 1583.760996 1583.691585 10 593.265716 -0.666417 1234 592.599299 593.932133 1185.198597 1187.864265 11 214.125419 0.721594 1234 214.847012 213.403825 429.694025 426.807650 12 11.554859 -0.988013 1234 10.566846 12.542872 21.133692 25.085744 13 392.944679 -0.732282 1234 392.212397 393.676961 784.424795 787.353922 14 606.165074 -0.077119 1234 606.087955 606.242193 1212.175911 1212.484386 In\u00a0[10]: Copied! <pre>vocs.objectives\n</pre> vocs.objectives Out[10]: <pre>{'c': 'MAXIMIZE', 'd': 'MINIMIZE'}</pre> In\u00a0[11]: Copied! <pre># These are in standard form for minimization\nform_objective_data(vocs.objectives, data)\n</pre> # These are in standard form for minimization form_objective_data(vocs.objectives, data) Out[11]: objective_c objective_d 5 -596.806528 598.488670 6 -555.364072 555.563283 7 -925.782923 927.772024 8 -45.141257 45.591491 9 -791.880498 791.845792 10 -592.599299 593.932133 11 -214.847012 213.403825 12 -10.566846 12.542872 13 -392.212397 393.676961 14 -606.087955 606.242193 In\u00a0[12]: Copied! <pre># This is also available as a method\nvocs.objective_data(data)\n</pre> # This is also available as a method vocs.objective_data(data) Out[12]: objective_c objective_d 5 -596.806528 598.488670 6 -555.364072 555.563283 7 -925.782923 927.772024 8 -45.141257 45.591491 9 -791.880498 791.845792 10 -592.599299 593.932133 11 -214.847012 213.403825 12 -10.566846 12.542872 13 -392.212397 393.676961 14 -606.087955 606.242193 In\u00a0[13]: Copied! <pre># use the to_numpy() method to convert for low level use.\nvocs.objective_data(data).to_numpy()\n</pre> # use the to_numpy() method to convert for low level use. vocs.objective_data(data).to_numpy() Out[13]: <pre>array([[-596.80652823,  598.48867039],\n       [-555.36407152,  555.56328301],\n       [-925.78292327,  927.77202414],\n       [ -45.14125713,   45.59149091],\n       [-791.88049796,  791.84579244],\n       [-592.59929859,  593.93213255],\n       [-214.84701237,  213.40382479],\n       [ -10.56684611,   12.54287191],\n       [-392.21239748,  393.67696081],\n       [-606.0879554 ,  606.24219276]])</pre> In\u00a0[14]: Copied! <pre>vocs.constraint_data(data)\n</pre> vocs.constraint_data(data) Out[14]: constraint_e constraint_f 5 1191.613056 -1196.977341 6 1108.728143 -1111.126566 7 1849.565847 -1855.544048 8 88.282514 -91.182982 9 1581.760996 -1583.691585 10 1183.198597 -1187.864265 11 427.694025 -426.807650 12 19.133692 -25.085744 13 782.424795 -787.353922 14 1210.175911 -1212.484386 In\u00a0[15]: Copied! <pre>vocs.feasibility_data(data)\n</pre> vocs.feasibility_data(data) Out[15]: feasible_e feasible_f feasible 5 False True False 6 False True False 7 False True False 8 False True False 9 False True False 10 False True False 11 False True False 12 False True False 13 False True False 14 False True False In\u00a0[16]: Copied! <pre># normalize inputs to unit domain [0,1]\nnormed_data = vocs.normalize_inputs(data)\nnormed_data\n</pre> # normalize inputs to unit domain [0,1] normed_data = vocs.normalize_inputs(data) normed_data Out[16]: a b 5 0.597648 0.079464 6 0.555464 0.450197 7 0.926777 0.002725 8 0.045366 0.387442 9 0.791863 0.508676 10 0.593266 0.166792 11 0.214125 0.860797 12 0.011555 0.005994 13 0.392945 0.133859 14 0.606165 0.461441 In\u00a0[17]: Copied! <pre># and denormalize\nvocs.denormalize_inputs(normed_data)\n</pre> # and denormalize vocs.denormalize_inputs(normed_data) Out[17]: a b 5 597.647599 -0.841071 6 555.463677 -0.099606 7 926.777474 -0.994550 8 45.366374 -0.225117 9 791.863145 0.017353 10 593.265716 -0.666417 11 214.125419 0.721594 12 11.554859 -0.988013 13 392.944679 -0.732282 14 606.165074 -0.077119 In\u00a0[18]: Copied! <pre>Y = \"\"\"\nvariables:\n  a: [0, 1e3] # Note that 1e3 usually parses as a str with YAML. \n  b: [-1, 1]\nobjectives:\n  c: maximize\n  d: minimize \nconstraints:\n  e: ['Less_than', 2]\n  f: ['greater_than', 0]\nconstants:\n  g: 1234\n\n\"\"\"\n\nvocs = VOCS.from_yaml(Y)\n</pre> Y = \"\"\" variables:   a: [0, 1e3] # Note that 1e3 usually parses as a str with YAML.    b: [-1, 1] objectives:   c: maximize   d: minimize  constraints:   e: ['Less_than', 2]   f: ['greater_than', 0] constants:   g: 1234  \"\"\"  vocs = VOCS.from_yaml(Y) In\u00a0[19]: Copied! <pre>d = {'a': [1,2,3]}\n\ndf = pd.DataFrame(d)\ndf2 = pd.DataFrame(df).copy()\n\ndf2['b'] = np.nan\ndf2['b'] - 1\n</pre> d = {'a': [1,2,3]}  df = pd.DataFrame(d) df2 = pd.DataFrame(df).copy()  df2['b'] = np.nan df2['b'] - 1 Out[19]: <pre>0   NaN\n1   NaN\n2   NaN\nName: b, dtype: float64</pre> In\u00a0[20]: Copied! <pre>data['a']  = np.nan\n</pre> data['a']  = np.nan In\u00a0[21]: Copied! <pre>a = 2\ndef f(x=a):\n    return x\na=99\nf()\n</pre> a = 2 def f(x=a):     return x a=99 f() Out[21]: <pre>2</pre> In\u00a0[22]: Copied! <pre>pd.DataFrame(6e66, index=[1,2,3], columns=['A'])\n</pre> pd.DataFrame(6e66, index=[1,2,3], columns=['A']) Out[22]: A 1 6.000000e+66 2 6.000000e+66 3 6.000000e+66 In\u00a0[23]: Copied! <pre># These are in standard form for minimization\n\ndata = pd.DataFrame({'c':[1,2,3,4]}, index=[9,3,4,5])\n\nform_objective_data(vocs.objectives, data)\n</pre> # These are in standard form for minimization  data = pd.DataFrame({'c':[1,2,3,4]}, index=[9,3,4,5])  form_objective_data(vocs.objectives, data) Out[23]: objective_c objective_d 9 -1.0 inf 3 -2.0 inf 4 -3.0 inf 5 -4.0 inf"},{"location":"examples/basic/xopt_vocs/#vocs-data-structure","title":"VOCS data structure\u00b6","text":"<p>Variables, Objectives, Constraints, and other Settings (VOCS) helps define our optimization problems.</p>"},{"location":"examples/basic/xopt_vocs/#objective-evaluation","title":"Objective Evaluation\u00b6","text":""},{"location":"examples/basic/xopt_vocs/#error-handling","title":"Error handling\u00b6","text":""},{"location":"examples/bayes_exp/bayesian_exploration/","title":"Bayesian Exploration","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom copy import deepcopy\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import BayesianExplorationGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\nvocs = deepcopy(tnk_vocs)\nvocs.objectives = {}\nvocs.observables = [\"y1\"]\n\ngenerator = BayesianExplorationGenerator(vocs=vocs)\ngenerator.max_travel_distances = [0.25, 0.25]\ngenerator.numerical_optimizer.n_restarts = NUM_RESTARTS\ngenerator.n_monte_carlo_samples = NUM_MC_SAMPLES\n\nevaluator = Evaluator(function=evaluate_TNK)\n\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") NUM_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  from copy import deepcopy from xopt import Xopt, Evaluator from xopt.generators.bayesian import BayesianExplorationGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs  vocs = deepcopy(tnk_vocs) vocs.objectives = {} vocs.observables = [\"y1\"]  generator = BayesianExplorationGenerator(vocs=vocs) generator.max_travel_distances = [0.25, 0.25] generator.numerical_optimizer.n_restarts = NUM_RESTARTS generator.n_monte_carlo_samples = NUM_MC_SAMPLES  evaluator = Evaluator(function=evaluate_TNK)  X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X Out[1]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1266.gb391bbe.dirty\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0\n    random_sleep: 0\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    use_low_noise_prior: true\n  max_travel_distances:\n  - 0.25\n  - 0.25\n  model: null\n  n_candidates: 1\n  n_monte_carlo_samples: 128\n  name: bayesian_exploration\n  numerical_optimizer:\n    max_iter: 2000\n    n_restarts: 20\n    name: LBFGS\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives:\n    y1: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[2]: Copied! <pre>X.evaluate_data({\"x1\":[1.0, 0.75],\"x2\":[0.7, 0.95]})\n</pre> X.evaluate_data({\"x1\":[1.0, 0.75],\"x2\":[0.7, 0.95]}) Out[2]: x1 x2 y1 y2 c1 c2 xopt_runtime xopt_error 0 1.00 0.70 1.00 0.70 0.584045 0.290 0.000024 False 1 0.75 0.95 0.75 0.95 0.494833 0.265 0.000005 False In\u00a0[3]: Copied! <pre>for i in range(2):\n    print(f\"step {i}\")\n    X.step()\n</pre> for i in range(2):     print(f\"step {i}\")     X.step() <pre>step 0\nstep 1\n</pre> In\u00a0[4]: Copied! <pre># view the data\nX.data\n</pre> # view the data X.data Out[4]: x1 x2 y1 y2 c1 c2 xopt_runtime xopt_error 0 1.000000 0.700000 1.000000 0.700000 0.584045 0.290000 0.000024 False 1 0.750000 0.950000 0.750000 0.950000 0.494833 0.265000 0.000005 False 3 1.535397 1.735397 1.535397 1.735397 4.313110 2.598255 0.000014 False 4 2.320795 0.950000 2.320795 0.950000 5.188811 3.517794 0.000018 False In\u00a0[5]: Copied! <pre># plot results\nax = X.data.plot(\"x1\", \"x2\")\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(\"x1\", \"x2\") ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre>fig, ax = X.generator.visualize_model(show_feasibility=True, n_grid=100)\n</pre> fig, ax = X.generator.visualize_model(show_feasibility=True, n_grid=100) In\u00a0[7]: Copied! <pre># print generator model hyperparameters\nfor name, val in X.generator.model.named_parameters():\n    print(f\"{name}:{val}\")\n\nX.generator.model.models[2].covar_module.base_kernel.lengthscale\n</pre> # print generator model hyperparameters for name, val in X.generator.model.named_parameters():     print(f\"{name}:{val}\")  X.generator.model.models[2].covar_module.base_kernel.lengthscale <pre>models.0.likelihood.noise_covar.raw_noise:Parameter containing:\ntensor([-21.1848], dtype=torch.float64, requires_grad=True)\nmodels.0.mean_module.raw_constant:Parameter containing:\ntensor(0.1798, dtype=torch.float64, requires_grad=True)\nmodels.0.covar_module.raw_outputscale:Parameter containing:\ntensor(2.4870, dtype=torch.float64, requires_grad=True)\nmodels.0.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[-1.1313, -0.7034]], dtype=torch.float64, requires_grad=True)\nmodels.1.likelihood.noise_covar.raw_noise:Parameter containing:\ntensor([-22.1766], dtype=torch.float64, requires_grad=True)\nmodels.1.mean_module.raw_constant:Parameter containing:\ntensor(0.2642, dtype=torch.float64, requires_grad=True)\nmodels.1.covar_module.raw_outputscale:Parameter containing:\ntensor(1.9192, dtype=torch.float64, requires_grad=True)\nmodels.1.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[-0.8180, -0.8447]], dtype=torch.float64, requires_grad=True)\nmodels.2.likelihood.noise_covar.raw_noise:Parameter containing:\ntensor([-22.6123], dtype=torch.float64, requires_grad=True)\nmodels.2.mean_module.raw_constant:Parameter containing:\ntensor(0.2634, dtype=torch.float64, requires_grad=True)\nmodels.2.covar_module.raw_outputscale:Parameter containing:\ntensor(1.9071, dtype=torch.float64, requires_grad=True)\nmodels.2.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[-0.8057, -0.8525]], dtype=torch.float64, requires_grad=True)\n</pre> Out[7]: <pre>tensor([[0.3693, 0.3551]], dtype=torch.float64, grad_fn=&lt;SoftplusBackward0&gt;)</pre> In\u00a0[8]: Copied! <pre>X.vocs.feasibility_data(X.data)\n</pre> X.vocs.feasibility_data(X.data) Out[8]: feasible_c1 feasible_c2 feasible 0 True True True 1 True True True 3 True False False 4 True False False In\u00a0[9]: Copied! <pre># generate next point\nX.generator.generate(1)\n</pre> # generate next point X.generator.generate(1) Out[9]: <pre>[{'x1': 1.5606836791468726, 'x2': 0.16460249999999998}]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/bayes_exp/bayesian_exploration/#bayesian-exploration","title":"Bayesian Exploration\u00b6","text":"<p>Here we demonstrate the use of Bayesian Exploration to characterize an unknown function in the presence of constraints (see here). The function we wish to explore is the first objective of the TNK test problem.</p>"},{"location":"examples/bayes_exp/bayesian_exploration/#specifiying-generator-options","title":"Specifiying generator options\u00b6","text":"<p>We start with the generator defaults and modify as needed for conservative exploration, which should prevent any constraint violations.</p>"},{"location":"examples/bayes_exp/bayesian_exploration/#run-exploration","title":"Run exploration\u00b6","text":"<p>We start with evaluating 2 points that we know satisfy the constraints. We then run 30 exploration steps.</p>"},{"location":"examples/bayes_exp/bayesian_exploration/#introspect-models-acquisition-function-and-feasibility-prediction","title":"Introspect models, acquisition function and feasibility prediction\u00b6","text":"<p>During exploration we generate Gaussian Process models of each objective and constraint. We demonstrate how they are viewed below.</p>"},{"location":"examples/bayes_exp/bayesian_exploration/#generator-model-hyperparameters","title":"Generator model hyperparameters\u00b6","text":""},{"location":"examples/bayes_exp/bayesian_exploration/#examine-the-number-of-constraint-violations","title":"Examine the number of constraint violations\u00b6","text":"<p>Using the convience function provided by the vocs object we can evaluate which samples violate either or both of our constraints.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_from_yaml/","title":"Bayesian Exploration from yaml","text":"In\u00a0[1]: Copied! <pre>import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom xopt import Xopt\n\n# set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\nYAML = \"\"\"\ngenerator:\n    name: bayesian_exploration\n\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    observables: [y1]\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    constants: {a: dummy_constant}\n\n\"\"\"\n</pre> import warnings warnings.filterwarnings(\"ignore\")  import torch from xopt import Xopt  # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")  YAML = \"\"\" generator:     name: bayesian_exploration  evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     observables: [y1]     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     constants: {a: dummy_constant}  \"\"\" In\u00a0[2]: Copied! <pre>X = Xopt.from_yaml(YAML)\n\n# for testing purposes only\nif SMOKE_TEST:\n    X.generator.numerical_optimizer.n_restarts = 1\n    X.generator.n_monte_carlo_samples = 1\n\nX\n</pre> X = Xopt.from_yaml(YAML)  # for testing purposes only if SMOKE_TEST:     X.generator.numerical_optimizer.n_restarts = 1     X.generator.n_monte_carlo_samples = 1  X Out[2]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1266.gb391bbe.dirty\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0\n    random_sleep: 0\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    use_low_noise_prior: true\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_monte_carlo_samples: 128\n  name: bayesian_exploration\n  numerical_optimizer:\n    max_iter: 2000\n    n_restarts: 20\n    name: LBFGS\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives:\n    y1: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[3]: Copied! <pre>X.random_evaluate(5)\n\nfor i in range(5):\n    print(f\"step {i}\")\n    X.step()\n</pre> X.random_evaluate(5)  for i in range(5):     print(f\"step {i}\")     X.step() <pre>step 0\nstep 1\nstep 2\nstep 3\nstep 4\n</pre> In\u00a0[4]: Copied! <pre>print(X.data)\n</pre> print(X.data) <pre>          x1        x2               a        y1        y2         c1  \\\n0   2.993476  1.092411  dummy_constant  2.993476  1.092411   9.076792   \n1   1.169031  1.117434  dummy_constant  1.169031  1.117434   1.521738   \n2   2.862666  2.549712  dummy_constant  2.862666  2.549712  13.635635   \n3   2.151784  0.052872  dummy_constant  2.151784  0.052872   3.540597   \n4   1.292425  2.416395  dummy_constant  1.292425  2.416395   6.509756   \n6   0.000000  1.456950             NaN  0.000000  1.456950   1.022702   \n7   0.238050  0.000000             NaN  0.238050  0.000000  -1.043332   \n8   0.671759  1.206354             NaN  0.671759  1.206354   0.933746   \n9   1.190292  0.244689             NaN  1.190292  0.244689   0.576143   \n10  0.906943  0.760457             NaN  0.906943  0.760457   0.384045   \n\n          c2  xopt_runtime  xopt_error  \n0   6.568374      0.000022       False  \n1   0.828827      0.000007       False  \n2   9.783511      0.000006       False  \n3   2.928315      0.000006       False  \n4   4.300506      0.000012       False  \n6   1.165752      0.000016       False  \n7   0.318618      0.000028       False  \n8   0.528438      0.000018       False  \n9   0.541686      0.000015       False  \n10  0.233440      0.000016       False  \n</pre> In\u00a0[5]: Copied! <pre># plot results\nax = X.data.plot(\"x1\", \"x2\")\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(\"x1\", \"x2\") ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre>fig, ax = X.generator.visualize_model(show_feasibility=True, n_grid=100)\n</pre> fig, ax = X.generator.visualize_model(show_feasibility=True, n_grid=100) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/bayes_exp/bayesian_exploration_w_interpolation/","title":"Bayesian Exploration","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom copy import deepcopy\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import BayesianExplorationGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\nvocs = deepcopy(tnk_vocs)\nvocs.objectives = {}\nvocs.observables = [\"y1\"]\n\ngenerator = BayesianExplorationGenerator(vocs=vocs)\ngenerator.numerical_optimizer.n_restarts = NUM_RESTARTS\ngenerator.numerical_optimizer.max_iter = 100\ngenerator.n_monte_carlo_samples = NUM_MC_SAMPLES\ngenerator.n_interpolate_points = 5\n\nevaluator = Evaluator(function=evaluate_TNK)\n\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") NUM_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  from copy import deepcopy from xopt import Xopt, Evaluator from xopt.generators.bayesian import BayesianExplorationGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs  vocs = deepcopy(tnk_vocs) vocs.objectives = {} vocs.observables = [\"y1\"]  generator = BayesianExplorationGenerator(vocs=vocs) generator.numerical_optimizer.n_restarts = NUM_RESTARTS generator.numerical_optimizer.max_iter = 100 generator.n_monte_carlo_samples = NUM_MC_SAMPLES generator.n_interpolate_points = 5  evaluator = Evaluator(function=evaluate_TNK)  X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X Out[1]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0\n    random_sleep: 0\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: 5\n  n_monte_carlo_samples: 128\n  name: bayesian_exploration\n  numerical_optimizer:\n    max_iter: 100\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  supports_batch_generation: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives: {}\n  observables:\n  - y1\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[2]: Copied! <pre>X.evaluate_data({\"x1\":[1.0, 0.75],\"x2\":[0.7, 0.95]})\n</pre> X.evaluate_data({\"x1\":[1.0, 0.75],\"x2\":[0.7, 0.95]}) Out[2]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 0 1.00 0.70 dummy_constant 1.00 0.70 0.584045 0.290 0.000033 False 1 0.75 0.95 dummy_constant 0.75 0.95 0.494833 0.265 0.000008 False In\u00a0[3]: Copied! <pre>for i in range(20):\n    print(f\"step {i}\")\n    X.step()\n</pre> for i in range(20):     print(f\"step {i}\")     X.step() <pre>step 0\n</pre> <pre>step 1\n</pre> <pre>step 2\n</pre> <pre>step 3\n</pre> <pre>step 4\n</pre> <pre>step 5\n</pre> <pre>step 6\n</pre> <pre>step 7\n</pre> <pre>step 8\n</pre> <pre>step 9\n</pre> <pre>step 10\n</pre> <pre>step 11\n</pre> <pre>step 12\n</pre> <pre>step 13\n</pre> <pre>step 14\n</pre> <pre>step 15\n</pre> <pre>step 16\n</pre> <pre>step 17\n</pre> <pre>step 18\n</pre> <pre>step 19\n</pre> In\u00a0[4]: Copied! <pre># view the data\nX.data\n</pre> # view the data X.data Out[4]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 0 1.000000 0.700000 dummy_constant 1.000000 0.700000 0.584045 0.290000 0.000033 False 1 0.750000 0.950000 dummy_constant 0.750000 0.950000 0.494833 0.265000 0.000008 False 2 1.228318 1.388318 dummy_constant 1.228318 1.388318 2.380252 1.319556 0.000022 False 3 1.706636 1.826636 dummy_constant 1.706636 1.826636 5.163599 3.215934 0.000008 False 4 2.184954 2.264954 dummy_constant 2.184954 2.264954 8.808148 5.954133 0.000006 False ... ... ... ... ... ... ... ... ... ... 97 0.920883 0.499670 dummy_constant 0.920883 0.499670 0.107710 0.177143 0.000023 False 98 0.871396 0.576111 dummy_constant 0.871396 0.576111 0.190930 0.143728 0.000009 False 99 0.821909 0.652553 dummy_constant 0.821909 0.652553 0.126965 0.126898 0.000007 False 100 0.772422 0.728995 dummy_constant 0.772422 0.728995 0.038581 0.126652 0.000006 False 101 0.722934 0.805436 dummy_constant 0.722934 0.805436 0.106333 0.142991 0.000006 False <p>102 rows \u00d7 9 columns</p> In\u00a0[5]: Copied! <pre># plot results\nax = X.data.plot(\"x1\", \"x2\")\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(\"x1\", \"x2\") ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre>fig, ax = X.generator.visualize_model(show_feasibility=True, n_grid=100)\n</pre> fig, ax = X.generator.visualize_model(show_feasibility=True, n_grid=100) In\u00a0[7]: Copied! <pre># print generator model hyperparameters\nfor name, val in X.generator.model.named_parameters():\n    print(f\"{name}:{val}\")\n\nX.generator.model.models[2].covar_module.base_kernel.lengthscale\n</pre> # print generator model hyperparameters for name, val in X.generator.model.named_parameters():     print(f\"{name}:{val}\")  X.generator.model.models[2].covar_module.base_kernel.lengthscale <pre>models.0.likelihood.noise_covar.raw_noise:Parameter containing:\ntensor([-26.1032], dtype=torch.float64, requires_grad=True)\nmodels.0.mean_module.raw_constant:Parameter containing:\ntensor(1.7818, dtype=torch.float64, requires_grad=True)\nmodels.0.covar_module.raw_outputscale:Parameter containing:\ntensor(2.1475, dtype=torch.float64, requires_grad=True)\nmodels.0.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[-0.5587, -0.4574]], dtype=torch.float64, requires_grad=True)\nmodels.1.likelihood.noise_covar.raw_noise:Parameter containing:\ntensor([-19.9410], dtype=torch.float64, requires_grad=True)\nmodels.1.mean_module.raw_constant:Parameter containing:\ntensor(7.6865, dtype=torch.float64, requires_grad=True)\nmodels.1.covar_module.raw_outputscale:Parameter containing:\ntensor(8.2948, dtype=torch.float64, requires_grad=True)\nmodels.1.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[1.5637, 1.6505]], dtype=torch.float64, requires_grad=True)\nmodels.2.likelihood.noise_covar.raw_noise:Parameter containing:\ntensor([-24.6450], dtype=torch.float64, requires_grad=True)\nmodels.2.mean_module.raw_constant:Parameter containing:\ntensor(1.0197, dtype=torch.float64, requires_grad=True)\nmodels.2.covar_module.raw_outputscale:Parameter containing:\ntensor(2.9529, dtype=torch.float64, requires_grad=True)\nmodels.2.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[1.3214, 2.7666]], dtype=torch.float64, requires_grad=True)\n</pre> Out[7]: <pre>tensor([[1.5579, 2.8276]], dtype=torch.float64, grad_fn=&lt;SoftplusBackward0&gt;)</pre> In\u00a0[8]: Copied! <pre>X.vocs.feasibility_data(X.data)\n</pre> X.vocs.feasibility_data(X.data) Out[8]: feasible_c1 feasible_c2 feasible 0 True True True 1 True True True 2 True False False 3 True False False 4 True False False ... ... ... ... 97 True True True 98 True True True 99 True True True 100 True True True 101 True True True <p>102 rows \u00d7 3 columns</p> In\u00a0[9]: Copied! <pre># generate next point\nX.generator.generate(1)\n</pre> # generate next point X.generator.generate(1) Out[9]: <pre>[{'x1': 0.694904227393601, 'x2': 0.8375033106144042},\n {'x1': 0.6668741018579859, 'x2': 0.8695705893366993},\n {'x1': 0.6388439763223708, 'x2': 0.9016378680589944},\n {'x1': 0.6108138507867557, 'x2': 0.9337051467812895},\n {'x1': 0.5827837252511405, 'x2': 0.9657724255035846}]</pre> In\u00a0[9]: Copied! <pre>\n</pre>"},{"location":"examples/bayes_exp/bayesian_exploration_w_interpolation/#bayesian-exploration","title":"Bayesian Exploration\u00b6","text":"<p>Here we demonstrate the use of Bayesian Exploration to characterize an unknown function in the presence of constraints (see here). The function we wish to explore is the first objective of the TNK test problem.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_w_interpolation/#specifiying-generator-options","title":"Specifiying generator options\u00b6","text":"<p>We start with the generator defaults and modify as needed for conservative exploration, which should prevent any constraint violations.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_w_interpolation/#run-exploration","title":"Run exploration\u00b6","text":"<p>We start with evaluating 2 points that we know satisfy the constraints. We then run 30 exploration steps.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_w_interpolation/#introspect-models-acquisition-function-and-feasibility-prediction","title":"Introspect models, acquisition function and feasibility prediction\u00b6","text":"<p>During exploration we generate Gaussian Process models of each objective and constraint. We demonstrate how they are viewed below.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_w_interpolation/#generator-model-hyperparameters","title":"Generator model hyperparameters\u00b6","text":""},{"location":"examples/bayes_exp/bayesian_exploration_w_interpolation/#examine-the-number-of-constraint-violations","title":"Examine the number of constraint violations\u00b6","text":"<p>Using the convience function provided by the vocs object we can evaluate which samples violate either or both of our constraints.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_with_nans/","title":"Bayesian Exploration with NaNs","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport torch\nimport yaml\nfrom copy import deepcopy\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import BayesianExplorationGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\nvocs = deepcopy(tnk_vocs)\nvocs.objectives = {}\nvocs.observables = [\"y1\"]\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") NUM_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import pandas as pd import torch import yaml from copy import deepcopy from xopt import Xopt, Evaluator from xopt.generators.bayesian import BayesianExplorationGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs  vocs = deepcopy(tnk_vocs) vocs.objectives = {} vocs.observables = [\"y1\"] In\u00a0[2]: Copied! <pre># modify the evaluate function to return NaNs if constraints are violated\ndef evaluate(input_dict):\n    output_dict = evaluate_TNK(input_dict)\n    del output_dict[\"y2\"]\n    for c in vocs.constraints.keys():\n        if vocs.constraints[c][0].upper() == \"GREATER_THAN\" and output_dict[c] &lt;= vocs.constraints[c][1]:\n            output_dict[\"y1\"] = torch.nan\n        elif vocs.constraints[c][0].upper() == \"LESS_THAN\" and output_dict[c] &gt;= vocs.constraints[c][1]:\n            output_dict[\"y1\"] = torch.nan\n    return output_dict\n</pre> # modify the evaluate function to return NaNs if constraints are violated def evaluate(input_dict):     output_dict = evaluate_TNK(input_dict)     del output_dict[\"y2\"]     for c in vocs.constraints.keys():         if vocs.constraints[c][0].upper() == \"GREATER_THAN\" and output_dict[c] &lt;= vocs.constraints[c][1]:             output_dict[\"y1\"] = torch.nan         elif vocs.constraints[c][0].upper() == \"LESS_THAN\" and output_dict[c] &gt;= vocs.constraints[c][1]:             output_dict[\"y1\"] = torch.nan     return output_dict In\u00a0[3]: Copied! <pre>generator = BayesianExplorationGenerator(vocs=vocs)\ngenerator.max_travel_distances = [0.25, 0.25]\ngenerator.n_monte_carlo_samples = NUM_MC_SAMPLES\ngenerator.numerical_optimizer.n_restarts = NUM_RESTARTS\n\nevaluator = Evaluator(function=evaluate)\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX\n</pre> generator = BayesianExplorationGenerator(vocs=vocs) generator.max_travel_distances = [0.25, 0.25] generator.n_monte_carlo_samples = NUM_MC_SAMPLES generator.numerical_optimizer.n_restarts = NUM_RESTARTS  evaluator = Evaluator(function=evaluate) X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X Out[3]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: __main__.evaluate\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances:\n  - 0.25\n  - 0.25\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: bayesian_exploration\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  supports_batch_generation: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives: {}\n  observables:\n  - y1\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[4]: Copied! <pre>X.evaluate_data(pd.DataFrame({\"x1\": [1.0, 0.75], \"x2\": [0.7, 0.95]}))\n</pre> X.evaluate_data(pd.DataFrame({\"x1\": [1.0, 0.75], \"x2\": [0.7, 0.95]})) Out[4]: x1 x2 a y1 c1 c2 xopt_runtime xopt_error 0 1.00 0.70 dummy_constant 1.00 0.584045 0.290 0.000051 False 1 0.75 0.95 dummy_constant 0.75 0.494833 0.265 0.000016 False In\u00a0[5]: Copied! <pre>N_STEPS = 1 if SMOKE_TEST else 30\nfor i in range(N_STEPS):\n    print(f\"step {i}\")\n    X.step()\n</pre> N_STEPS = 1 if SMOKE_TEST else 30 for i in range(N_STEPS):     print(f\"step {i}\")     X.step() <pre>step 0\n</pre> <pre>step 1\n</pre> <pre>step 2\n</pre> <pre>step 3\n</pre> <pre>step 4\n</pre> <pre>step 5\n</pre> <pre>step 6\n</pre> <pre>step 7\n</pre> <pre>step 8\n</pre> <pre>step 9\n</pre> <pre>step 10\n</pre> <pre>step 11\n</pre> <pre>step 12\n</pre> <pre>step 13\n</pre> <pre>step 14\n</pre> <pre>step 15\n</pre> <pre>step 16\n</pre> <pre>step 17\n</pre> <pre>step 18\n</pre> <pre>step 19\n</pre> <pre>step 20\n</pre> <pre>step 21\n</pre> <pre>step 22\n</pre> <pre>step 23\n</pre> <pre>step 24\n</pre> <pre>step 25\n</pre> <pre>step 26\n</pre> <pre>step 27\n</pre> <pre>step 28\n</pre> <pre>step 29\n</pre> In\u00a0[6]: Copied! <pre># view the data\nX.data\n</pre> # view the data X.data Out[6]: x1 x2 a y1 c1 c2 xopt_runtime xopt_error 0 1.000000 0.700000 dummy_constant 1.000000 0.584045 0.290000 0.000051 False 1 0.750000 0.950000 dummy_constant 0.750000 0.494833 0.265000 0.000016 False 2 1.535397 1.735397 dummy_constant NaN 4.313110 2.598255 0.000024 False 3 2.320795 0.950000 dummy_constant NaN 5.188811 3.517794 0.000027 False 4 1.566283 0.164602 dummy_constant NaN 1.490770 1.249452 0.000027 False 5 0.780886 0.000000 dummy_constant NaN -0.490217 0.328897 0.000028 False 6 0.000000 0.778003 dummy_constant NaN -0.494711 0.327286 0.000027 False 7 0.047026 1.512956 dummy_constant NaN 1.203354 1.231266 0.000028 False 8 0.409870 1.032124 dummy_constant 0.409870 0.136021 0.291280 0.000026 False 9 1.057960 0.309480 dummy_constant 1.057960 0.230892 0.347617 0.000028 False 10 0.720106 1.094878 dummy_constant 0.720106 0.816633 0.402326 0.000026 False 11 0.849474 0.634353 dummy_constant 0.849474 0.190904 0.140183 0.000027 False 12 1.075867 0.814162 dummy_constant 1.075867 0.879319 0.430321 0.000028 False 13 0.290470 1.128176 dummy_constant 0.290470 0.420069 0.438508 0.000028 False 14 0.145636 1.070197 dummy_constant 0.145636 0.222437 0.450699 0.000026 False 15 0.653910 1.164653 dummy_constant 0.653910 0.816576 0.465453 0.000027 False 16 1.167903 0.531030 dummy_constant 1.167903 0.560464 0.447058 0.000027 False 17 1.054851 0.103850 dummy_constant 1.054851 0.123429 0.464794 0.000027 False 18 0.633210 0.889247 dummy_constant 0.633210 0.280610 0.169258 0.000026 False 19 0.975702 0.987707 dummy_constant 0.975702 0.828039 0.464151 0.000027 False 20 0.519429 1.187784 dummy_constant 0.519429 0.585489 0.473424 0.000026 False 21 1.179909 0.606566 dummy_constant 1.179909 0.734744 0.473633 0.000027 False 22 0.745222 0.701767 dummy_constant NaN -0.040849 0.100844 0.000025 False 23 1.066948 0.102039 dummy_constant 1.066948 0.144266 0.479803 0.000028 False 24 1.073195 0.887436 dummy_constant 1.073195 0.933353 0.478659 0.000026 False 25 0.850698 1.095319 dummy_constant 0.850698 0.965094 0.477394 0.000027 False 26 0.084304 1.051202 dummy_constant 0.084304 0.083502 0.476628 0.000028 False 27 0.473880 0.937680 dummy_constant 0.473880 0.067928 0.192246 0.000028 False 28 0.075697 1.051035 dummy_constant 0.075697 0.069588 0.483672 0.000027 False 29 0.861094 0.578022 dummy_constant 0.861094 0.175535 0.136477 0.000026 False 30 1.184778 0.406375 dummy_constant 1.184778 0.514509 0.477686 0.000027 False 31 0.431923 1.191773 dummy_constant 0.431923 0.531702 0.483184 0.000027 False In\u00a0[7]: Copied! <pre># plot results\nax = X.data.plot(\"x1\", \"x2\")\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(\"x1\", \"x2\") ax.set_aspect(\"equal\") In\u00a0[7]: Copied! <pre>\n</pre>"},{"location":"examples/bayes_exp/bayesian_exploration_with_nans/#bayesian-exploration-with-nans","title":"Bayesian Exploration with NaNs\u00b6","text":"<p>As violations of constraints can lead to invalid values of the objective, the evaluate function may simply return NaNs. We demonstrate below how we can still perform Bayesian Exploration in that case.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_with_nans/#run-exploration","title":"Run exploration\u00b6","text":"<p>We start with evaluating 2 points that we know satisfy the constraints. We then run 30 exploration steps.</p>"},{"location":"examples/cnsga/cnsga_tnk/","title":"Xopt CNSGA algorithm","text":"In\u00a0[1]: Copied! <pre>from xopt.generators.ga.cnsga import CNSGAGenerator\n\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\nfrom xopt import Xopt, Evaluator\n\nimport pandas as pd\n</pre> from xopt.generators.ga.cnsga import CNSGAGenerator  from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs  from xopt import Xopt, Evaluator  import pandas as pd In\u00a0[2]: Copied! <pre># Useful for debugging\n#%load_ext autoreload\n#%autoreload 2\n</pre> # Useful for debugging #%load_ext autoreload #%autoreload 2 In\u00a0[3]: Copied! <pre>ev = Evaluator(function=evaluate_TNK)\nev.function_kwargs = {'raise_probability':0.1} # optional random crashing, to mimic real-world use.\n</pre> ev = Evaluator(function=evaluate_TNK) ev.function_kwargs = {'raise_probability':0.1} # optional random crashing, to mimic real-world use.  In\u00a0[4]: Copied! <pre>X = Xopt(\n    generator=CNSGAGenerator(vocs=tnk_vocs),\n    evaluator=ev,\n    vocs=tnk_vocs,\n)\nX.strict = False\n</pre> X = Xopt(     generator=CNSGAGenerator(vocs=tnk_vocs),     evaluator=ev,     vocs=tnk_vocs, ) X.strict = False <p>Run 100 generations</p> In\u00a0[5]: Copied! <pre>%%time\nfor _ in range(64 * 20):\n    X.step()\n</pre> %%time for _ in range(64 * 20):     X.step() <pre>CPU times: user 3.37 s, sys: 3.3 ms, total: 3.37 s\nWall time: 3.37 s\n</pre> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[7]: Copied! <pre>def plot_population(X):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    \n    fdata = tnk_vocs.feasibility_data(X.data)\n    \n    k1 = \"x1\"\n    k2 = \"x2\"\n    \n    X.data.plot.scatter(k1, k2, marker=\".\", alpha=0.1, color=\"black\", ax=ax)\n    X.data[fdata[\"feasible\"]].plot.scatter(\n        k1, k2, marker=\"x\", alpha=0.3, color=\"orange\", ax=ax\n    )\n    X.generator.population.plot.scatter(k1, k2, marker=\"o\", color=\"red\", alpha=1, ax=ax)\n    ax.set_xlabel(k1)\n    ax.set_ylabel(k2)\n    ax.set_xlim(0, 1.5)\n    ax.set_ylim(0, 1.5)\n    ax.set_title(\"TNK with Xopt's CNSGA\")\n</pre> def plot_population(X):     fig, ax = plt.subplots(figsize=(8, 8))          fdata = tnk_vocs.feasibility_data(X.data)          k1 = \"x1\"     k2 = \"x2\"          X.data.plot.scatter(k1, k2, marker=\".\", alpha=0.1, color=\"black\", ax=ax)     X.data[fdata[\"feasible\"]].plot.scatter(         k1, k2, marker=\"x\", alpha=0.3, color=\"orange\", ax=ax     )     X.generator.population.plot.scatter(k1, k2, marker=\"o\", color=\"red\", alpha=1, ax=ax)     ax.set_xlabel(k1)     ax.set_ylabel(k2)     ax.set_xlim(0, 1.5)     ax.set_ylim(0, 1.5)     ax.set_title(\"TNK with Xopt's CNSGA\") In\u00a0[8]: Copied! <pre>plot_population(X)\n</pre> plot_population(X) <p>Write the current population</p> In\u00a0[9]: Copied! <pre>X.generator.write_population('test.csv')\n</pre> X.generator.write_population('test.csv') In\u00a0[10]: Copied! <pre>from xopt import Xopt\n</pre> from xopt import Xopt In\u00a0[11]: Copied! <pre>YAML = \"\"\"\nmax_evaluations: 6400\nstrict: False\ngenerator:\n    name: cnsga\n    population_size: 64\n    population_file: test.csv\n    output_path: .\n\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n    function_kwargs:\n      raise_probability: 0.1\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    constants: {a: dummy_constant}\n\n\"\"\"\n\nX = Xopt(YAML)\nX\n</pre> YAML = \"\"\" max_evaluations: 6400 strict: False generator:     name: cnsga     population_size: 64     population_file: test.csv     output_path: .  evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK     function_kwargs:       raise_probability: 0.1  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     constants: {a: dummy_constant}  \"\"\"  X = Xopt(YAML) X Out[11]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0.1\n    random_sleep: 0\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  crossover_probability: 0.9\n  mutation_probability: 1.0\n  name: cnsga\n  output_path: .\n  population: null\n  population_file: test.csv\n  population_size: 64\n  supports_multi_objective: true\nmax_evaluations: 6400\nserialize_inline: false\nserialize_torch: false\nstrict: false\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives:\n    y1: MINIMIZE\n    y2: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> <p>This will have loaded children from the population file. These will need to be re-evaluated.</p> In\u00a0[12]: Copied! <pre>len(X.generator._children)\n</pre> len(X.generator._children) Out[12]: <pre>64</pre> In\u00a0[13]: Copied! <pre>%%time\nX.run()\n</pre> %%time X.run() <pre>CPU times: user 16.8 s, sys: 35.9 ms, total: 16.8 s\nWall time: 16.8 s\n</pre> In\u00a0[14]: Copied! <pre>plot_population(X)\n</pre> plot_population(X) In\u00a0[15]: Copied! <pre>len(X.data)\n</pre> len(X.data) Out[15]: <pre>6400</pre> <p>Setting <code>output_path</code> will write .csv files for each population, as well as the offspring considered in each generation</p> In\u00a0[16]: Copied! <pre>from glob import glob\npop_files = sorted(glob(\"cnsga_population*\"))\npop_files[:10]\n</pre> from glob import glob pop_files = sorted(glob(\"cnsga_population*\")) pop_files[:10] Out[16]: <pre>['cnsga_population_2024-02-23T22:09:14.650974+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:14.818345+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:14.983781+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:15.149497+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:15.307418+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:15.467591+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:15.630594+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:15.792587+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:15.952624+00:00.csv',\n 'cnsga_population_2024-02-23T22:09:16.117199+00:00.csv']</pre> In\u00a0[17]: Copied! <pre>offspring_files = sorted(glob(\"cnsga_offspring*\"))\noffspring_files[0:10]\n</pre> offspring_files = sorted(glob(\"cnsga_offspring*\")) offspring_files[0:10] Out[17]: <pre>['cnsga_offspring_2024-02-23T22:09:14.649824+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:14.817267+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:14.982717+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:15.148460+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:15.306435+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:15.466633+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:15.629513+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:15.791472+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:15.951525+00:00.csv',\n 'cnsga_offspring_2024-02-23T22:09:16.116099+00:00.csv']</pre> In\u00a0[18]: Copied! <pre>from xopt.utils import read_xopt_csv\npop_df = read_xopt_csv(pop_files[-1])\npop_df.plot.scatter(\"x1\", \"x2\", marker=\"o\", color=\"red\", alpha=1)\n</pre> from xopt.utils import read_xopt_csv pop_df = read_xopt_csv(pop_files[-1]) pop_df.plot.scatter(\"x1\", \"x2\", marker=\"o\", color=\"red\", alpha=1) Out[18]: <pre>&lt;Axes: xlabel='x1', ylabel='x2'&gt;</pre> <p>Similarly, offsrping files can be loaded. This will load the last few:</p> In\u00a0[19]: Copied! <pre>offspring_df = read_xopt_csv(*offspring_files[-10:])\noffspring_df.plot.scatter(\"x1\", \"x2\", marker=\".\", color=\"black\", alpha=.1)\n</pre> offspring_df = read_xopt_csv(*offspring_files[-10:]) offspring_df.plot.scatter(\"x1\", \"x2\", marker=\".\", color=\"black\", alpha=.1) Out[19]: <pre>&lt;Axes: xlabel='x1', ylabel='x2'&gt;</pre> <p>Occationally there are duplicates in offspring</p> In\u00a0[20]: Copied! <pre>all_offspring = read_xopt_csv(*offspring_files) \nlen(all_offspring), len(all_offspring.drop_duplicates())\n</pre> all_offspring = read_xopt_csv(*offspring_files)  len(all_offspring), len(all_offspring.drop_duplicates()) Out[20]: <pre>(6400, 6386)</pre> In\u00a0[21]: Copied! <pre># Cleanup\n!rm cnsga_population*\n!rm cnsga_offspring*\n!rm test.csv\n</pre> # Cleanup !rm cnsga_population* !rm cnsga_offspring* !rm test.csv In\u00a0[22]: Copied! <pre>df = pd.DataFrame(X.generator.generate(1000))\n\nfig, ax = plt.subplots()\ndf.plot.scatter(\"x1\", \"x2\", marker=\".\", color=\"green\", alpha=0.5, ax=ax, label='candidates')\npop_df.plot.scatter(\"x1\", \"x2\", marker=\"o\", color=\"red\", alpha=1, ax=ax, label='population')\nplt.legend()\n</pre> df = pd.DataFrame(X.generator.generate(1000))  fig, ax = plt.subplots() df.plot.scatter(\"x1\", \"x2\", marker=\".\", color=\"green\", alpha=0.5, ax=ax, label='candidates') pop_df.plot.scatter(\"x1\", \"x2\", marker=\"o\", color=\"red\", alpha=1, ax=ax, label='population') plt.legend() Out[22]: <pre>&lt;matplotlib.legend.Legend at 0x7f272c068370&gt;</pre> In\u00a0[23]: Copied! <pre># Notice that this returns `some_array`\nevaluate_TNK({'x1':1, 'x2':1})\n</pre> # Notice that this returns `some_array` evaluate_TNK({'x1':1, 'x2':1}) Out[23]: <pre>{'y1': 1, 'y2': 1, 'c1': 0.9, 'c2': 0.5}</pre> In\u00a0[24]: Copied! <pre># Here we make a version that does not have this\ndef evaluate_TNK2(*args, **kwargs):\n    outputs = evaluate_TNK(*args, **kwargs)\n    outputs.pop('some_array')\n    return outputs\n</pre> # Here we make a version that does not have this def evaluate_TNK2(*args, **kwargs):     outputs = evaluate_TNK(*args, **kwargs)     outputs.pop('some_array')     return outputs In\u00a0[25]: Copied! <pre>from xopt import Xopt\n\nYAML = \"\"\"\nmax_evaluations: 6400\nstrict: False\ngenerator:\n    name: cnsga\n    population_size: 64\n\nevaluator:\n    function: __main__.evaluate_TNK2\n    function_kwargs:\n      raise_probability: 0.1\n    vectorized: True\n    max_workers: 100 \n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    constants: {a: dummy_constant}\n\n\"\"\"\n\n\nX2 = Xopt.from_yaml(YAML)\nX2.evaluator.function = evaluate_TNK2\n\nX2.run()\n\nlen(X2.data)\n</pre> from xopt import Xopt  YAML = \"\"\" max_evaluations: 6400 strict: False generator:     name: cnsga     population_size: 64  evaluator:     function: __main__.evaluate_TNK2     function_kwargs:       raise_probability: 0.1     vectorized: True     max_workers: 100   vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     constants: {a: dummy_constant}  \"\"\"   X2 = Xopt.from_yaml(YAML) X2.evaluator.function = evaluate_TNK2  X2.run()  len(X2.data) Out[25]: <pre>6400</pre> In\u00a0[26]: Copied! <pre>plot_population(X)\n</pre> plot_population(X)"},{"location":"examples/cnsga/cnsga_tnk/#xopt-cnsga-algorithm","title":"Xopt CNSGA algorithm\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/#plot","title":"Plot\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/#yaml-method","title":"YAML method\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/#examine-generator","title":"Examine generator\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/#vectorized-evaluation","title":"Vectorized evaluation\u00b6","text":"<p>Some functions also allow vectorized inputs. This can often be very fast.</p> <p>However, vectorized evaluation has some restrictions. For example, the output dict cannot append additional arrays with odd lengths.</p>"},{"location":"examples/es/extremum_seeking/","title":"Extremum seeking","text":"In\u00a0[1]: Copied! <pre># If you encounter the \"Initializing libomp.dylib, but found libomp.dylib already initialized.\" error\n# Please run this cell\n\nimport os\n</pre> # If you encounter the \"Initializing libomp.dylib, but found libomp.dylib already initialized.\" error # Please run this cell  import os  In\u00a0[2]: Copied! <pre># set values if testing\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_STEPS = 10 if SMOKE_TEST else 1000\n</pre> # set values if testing SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") NUM_STEPS = 10 if SMOKE_TEST else 1000 In\u00a0[3]: Copied! <pre>import numpy as np\nfrom xopt.generators.es.extremumseeking import ExtremumSeekingGenerator\nfrom xopt.vocs import VOCS\nfrom xopt.evaluator import Evaluator\nfrom xopt import Xopt\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> import numpy as np from xopt.generators.es.extremumseeking import ExtremumSeekingGenerator from xopt.vocs import VOCS from xopt.evaluator import Evaluator from xopt import Xopt from tqdm.auto import tqdm import warnings warnings.filterwarnings(\"ignore\") In\u00a0[4]: Copied! <pre>np.random.seed(42)  # set deterministic run\nimport pandas as pd\nnES = 10\n\n# This global dict is used as a counter to emulate drifting\nstates = {\n    'count': 0\n}\n\nnoise = 0.1 * np.random.randn(NUM_STEPS)\n\n# This is the unknown optimal point\np_opt = 1.5 * (2 * np.random.rand(nES) - 1)\n\n# Various frequencies for unknown points\nw_opt = 0.25 + 2 * np.random.rand(nES)\n\ndef f_ES_minimize(input_dict):\n    p = []\n    for i in range(10):\n        p.append(input_dict[f'p{i}'])\n    p = np.array(p)\n    \n    # Vary the optimal point with time\n    p_opt_i = np.zeros(nES)\n    i = states['count']\n    \n    outcome_dict = {}\n    for n in np.arange(nES):\n        p_opt_i[n] = p_opt[n] * (1 + np.sin(2 * np.pi * w_opt[n] * i / 2000))\n    # This simple cost will be distance from the optimal point\n    f_val = np.sum((p - p_opt_i) ** 2) + noise[i]\n    \n    states['count'] += 1\n    outcome_dict = {'f': f_val, 'p_opt': pd.Series(p_opt_i)}\n    \n    return outcome_dict\n</pre> np.random.seed(42)  # set deterministic run import pandas as pd nES = 10  # This global dict is used as a counter to emulate drifting states = {     'count': 0 }  noise = 0.1 * np.random.randn(NUM_STEPS)  # This is the unknown optimal point p_opt = 1.5 * (2 * np.random.rand(nES) - 1)  # Various frequencies for unknown points w_opt = 0.25 + 2 * np.random.rand(nES)  def f_ES_minimize(input_dict):     p = []     for i in range(10):         p.append(input_dict[f'p{i}'])     p = np.array(p)          # Vary the optimal point with time     p_opt_i = np.zeros(nES)     i = states['count']          outcome_dict = {}     for n in np.arange(nES):         p_opt_i[n] = p_opt[n] * (1 + np.sin(2 * np.pi * w_opt[n] * i / 2000))     # This simple cost will be distance from the optimal point     f_val = np.sum((p - p_opt_i) ** 2) + noise[i]          states['count'] += 1     outcome_dict = {'f': f_val, 'p_opt': pd.Series(p_opt_i)}          return outcome_dict In\u00a0[5]: Copied! <pre>YAML = \"\"\"\nmax_evaluations: 5000\ngenerator:\n    name: extremum_seeking\n    k: 2.0\n    oscillation_size: 0.1\n    decay_rate: 1.0\nevaluator:\n    function: __main__.f_ES_minimize\nvocs:\n    variables:\n        p0: [-2, 2]\n        p1: [-2, 2]\n        p2: [-2, 2]\n        p3: [-2, 2]\n        p4: [-2, 2]\n        p5: [-2, 2]\n        p6: [-2, 2]\n        p7: [-2, 2]\n        p8: [-2, 2]\n        p9: [-2, 2]\n    objectives:\n        f: MINIMIZE\n\"\"\"\n\nX = Xopt.from_yaml(YAML)\nX.max_evaluations = NUM_STEPS\n\nX\n</pre>  YAML = \"\"\" max_evaluations: 5000 generator:     name: extremum_seeking     k: 2.0     oscillation_size: 0.1     decay_rate: 1.0 evaluator:     function: __main__.f_ES_minimize vocs:     variables:         p0: [-2, 2]         p1: [-2, 2]         p2: [-2, 2]         p3: [-2, 2]         p4: [-2, 2]         p5: [-2, 2]         p6: [-2, 2]         p7: [-2, 2]         p8: [-2, 2]         p9: [-2, 2]     objectives:         f: MINIMIZE \"\"\"  X = Xopt.from_yaml(YAML) X.max_evaluations = NUM_STEPS  X Out[5]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: __main__.f_ES_minimize\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  decay_rate: 1.0\n  k: 2.0\n  name: extremum_seeking\n  oscillation_size: 0.1\nmax_evaluations: 1000\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints: {}\n  objectives:\n    f: MINIMIZE\n  observables: []\n  variables:\n    p0:\n    - -2.0\n    - 2.0\n    p1:\n    - -2.0\n    - 2.0\n    p2:\n    - -2.0\n    - 2.0\n    p3:\n    - -2.0\n    - 2.0\n    p4:\n    - -2.0\n    - 2.0\n    p5:\n    - -2.0\n    - 2.0\n    p6:\n    - -2.0\n    - 2.0\n    p7:\n    - -2.0\n    - 2.0\n    p8:\n    - -2.0\n    - 2.0\n    p9:\n    - -2.0\n    - 2.0\n</pre> In\u00a0[6]: Copied! <pre># Reset global counter to guarantee deterministic optimization\nstates['count'] = 0\n\nX.random_evaluate(1)\nX.step()\n</pre> # Reset global counter to guarantee deterministic optimization states['count'] = 0  X.random_evaluate(1) X.step() <p>Now you can go directly to the Visualization section and check out the results.</p> In\u00a0[7]: Copied! <pre>variables = {}\nfor i in range(nES):\n    variables[f'p{i}'] = [-2, 2]\n\nvocs = VOCS(\n    variables=variables,\n    objectives={'f': 'MINIMIZE'},\n)\n</pre> variables = {} for i in range(nES):     variables[f'p{i}'] = [-2, 2]  vocs = VOCS(     variables=variables,     objectives={'f': 'MINIMIZE'}, ) In\u00a0[8]: Copied! <pre>vocs\n</pre> vocs Out[8]: <pre>VOCS(variables={'p0': [-2.0, 2.0], 'p1': [-2.0, 2.0], 'p2': [-2.0, 2.0], 'p3': [-2.0, 2.0], 'p4': [-2.0, 2.0], 'p5': [-2.0, 2.0], 'p6': [-2.0, 2.0], 'p7': [-2.0, 2.0], 'p8': [-2.0, 2.0], 'p9': [-2.0, 2.0]}, constraints={}, objectives={'f': 'MINIMIZE'}, constants={}, observables=[])</pre> In\u00a0[9]: Copied! <pre>evaluator = Evaluator(function=f_ES_minimize)\n</pre> evaluator = Evaluator(function=f_ES_minimize) In\u00a0[10]: Copied! <pre>generator = ExtremumSeekingGenerator(vocs=vocs)\n</pre> generator = ExtremumSeekingGenerator(vocs=vocs) In\u00a0[11]: Copied! <pre>generator.dict()\n</pre> generator.dict() Out[11]: <pre>{'k': 2.0, 'oscillation_size': 0.1, 'decay_rate': 1.0}</pre> <p>Note that ES has 3 hyper-parameters: <code>k</code>, <code>oscillation_size</code>, and <code>decay_rate</code>.</p> <ul> <li><code>k</code>: ES feedback gain (set <code>k &lt; 0</code> for maximization instead of minimization)</li> <li><code>oscillation_size</code>: ES dithering size</li> <li><code>decay_rate</code>: This value is optional, it causes the oscillation sizes to naturally decay. If you want the parameters to persistently oscillate without decay, set <code>decay_rate = 1.0</code></li> </ul> In\u00a0[12]: Copied! <pre>X = Xopt(vocs=vocs, evaluator=evaluator, generator=generator)\n</pre> X = Xopt(vocs=vocs, evaluator=evaluator, generator=generator) In\u00a0[13]: Copied! <pre>X.max_evaluations = NUM_STEPS\n</pre> X.max_evaluations = NUM_STEPS In\u00a0[14]: Copied! <pre># Reset global counter to guarantee deterministic optimization\nstates['count'] = 0\n\nfor i in tqdm(range(NUM_STEPS)):\n    X.step()\n</pre> # Reset global counter to guarantee deterministic optimization states['count'] = 0  for i in tqdm(range(NUM_STEPS)):     X.step() In\u00a0[15]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[16]: Copied! <pre># Plot all results\nplt.figure(1,figsize=(8,10))\n\nplt.subplot(2,1,1)\nplt.plot(X.data['f'])\nplt.ylabel('ES cost')\nplt.xticks([])\n\n\nplt.subplot(2,1,2)\nplt.plot(X.data[[f'p{i}' for i in range(10)]],alpha=0.25)\n_p_opt = np.vstack(X.data['p_opt'].values).astype(float)  # do not use p_opt as var name!\nplt.plot(_p_opt, 'k--')\nplt.plot(2+np.zeros(NUM_STEPS),'r')\nplt.plot(-2+np.zeros(NUM_STEPS),'r')\nplt.legend(frameon=False)\nplt.ylabel('ES parameter')\nplt.xlabel('ES step')\n\nplt.tight_layout()\n</pre> # Plot all results plt.figure(1,figsize=(8,10))  plt.subplot(2,1,1) plt.plot(X.data['f']) plt.ylabel('ES cost') plt.xticks([])   plt.subplot(2,1,2) plt.plot(X.data[[f'p{i}' for i in range(10)]],alpha=0.25) _p_opt = np.vstack(X.data['p_opt'].values).astype(float)  # do not use p_opt as var name! plt.plot(_p_opt, 'k--') plt.plot(2+np.zeros(NUM_STEPS),'r') plt.plot(-2+np.zeros(NUM_STEPS),'r') plt.legend(frameon=False) plt.ylabel('ES parameter') plt.xlabel('ES step')  plt.tight_layout() <pre>No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n</pre> In\u00a0[17]: Copied! <pre># Plot Individual Parameter Trajectories\nplt.figure(2,figsize=(15,8))\n\nfor n in np.arange(nES):\n    plt.subplot(2,5,n+1)\n    plt.plot(X.data[f'p{n}'],label=f'$p^{{ES}}_{n+1}$')\n    plt.plot(_p_opt[:,n],'k--',label=f'$p^*_{n+1}$')\n    plt.plot(2+np.zeros(NUM_STEPS),'r--')\n    plt.plot(-2+np.zeros(NUM_STEPS),'r--')\n    plt.ylim([-3,5])\n    plt.legend(frameon=False,loc=1)\n    if n == 0:\n        plt.ylabel('parameters')\n    elif n == 5:\n        plt.ylabel('parameters')\n    else:\n        plt.yticks([])\n    if n &gt; 4:\n        plt.xlabel('ES step')\n    else:\n        plt.xticks([])\n\nplt.tight_layout()\n</pre> # Plot Individual Parameter Trajectories plt.figure(2,figsize=(15,8))  for n in np.arange(nES):     plt.subplot(2,5,n+1)     plt.plot(X.data[f'p{n}'],label=f'$p^{{ES}}_{n+1}$')     plt.plot(_p_opt[:,n],'k--',label=f'$p^*_{n+1}$')     plt.plot(2+np.zeros(NUM_STEPS),'r--')     plt.plot(-2+np.zeros(NUM_STEPS),'r--')     plt.ylim([-3,5])     plt.legend(frameon=False,loc=1)     if n == 0:         plt.ylabel('parameters')     elif n == 5:         plt.ylabel('parameters')     else:         plt.yticks([])     if n &gt; 4:         plt.xlabel('ES step')     else:         plt.xticks([])  plt.tight_layout() In\u00a0[17]: Copied! <pre>\n</pre>"},{"location":"examples/es/extremum_seeking/#extremum-seeking-optimization","title":"Extremum Seeking Optimization\u00b6","text":"<p>In this example we demonstrate extremum seeking optimization. The optimum of the test evaluate function would drift around a center point and we would be trying to follow the trend by applying extremum seeking technique.</p>"},{"location":"examples/es/extremum_seeking/#extremum-seeking-test-problem","title":"Extremum seeking test problem\u00b6","text":"<p>This test problem is a 10-D quadratic function, with its optimum drifting around the initial position. We also add some noise to make the problem more realistic.</p>"},{"location":"examples/es/extremum_seeking/#run-es-on-the-test-problem-yaml-method","title":"Run ES on the test problem (YAML method)\u00b6","text":""},{"location":"examples/es/extremum_seeking/#run-es-on-the-test-problem-api-method","title":"Run ES on the test problem (API method)\u00b6","text":""},{"location":"examples/es/extremum_seeking/#vocs","title":"VOCS\u00b6","text":"<p>We'll set the bounds for all the variables pi to [-2, 2].</p>"},{"location":"examples/es/extremum_seeking/#evaluator","title":"Evaluator\u00b6","text":""},{"location":"examples/es/extremum_seeking/#generator","title":"Generator\u00b6","text":""},{"location":"examples/es/extremum_seeking/#run-the-optimization","title":"Run the optimization\u00b6","text":""},{"location":"examples/es/extremum_seeking/#visualization","title":"Visualization\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mggpo/","title":"Multi-objective Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nN_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom copy import deepcopy\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian.mggpo import MGGPOGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\n\nevaluator = Evaluator(function=evaluate_TNK)\nevaluator.max_workers = 10\n\n# test check options\nvocs = deepcopy(tnk_vocs)\ngen = MGGPOGenerator(vocs=vocs, reference_point = {\"y1\":1.5,\"y2\":1.5})\ngen.n_monte_carlo_samples = N_MC_SAMPLES\ngen.numerical_optimizer.n_restarts = NUM_RESTARTS\nX = Xopt(evaluator=evaluator, generator=gen, vocs=vocs)\nX.evaluate_data(pd.DataFrame({\"x1\": [1.0, 0.75], \"x2\": [0.75, 1.0]}))\n\nX\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") N_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  from copy import deepcopy  import pandas as pd import numpy as np import torch  from xopt import Xopt, Evaluator from xopt.generators.bayesian.mggpo import MGGPOGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs   evaluator = Evaluator(function=evaluate_TNK) evaluator.max_workers = 10  # test check options vocs = deepcopy(tnk_vocs) gen = MGGPOGenerator(vocs=vocs, reference_point = {\"y1\":1.5,\"y2\":1.5}) gen.n_monte_carlo_samples = N_MC_SAMPLES gen.numerical_optimizer.n_restarts = NUM_RESTARTS X = Xopt(evaluator=evaluator, generator=gen, vocs=vocs) X.evaluate_data(pd.DataFrame({\"x1\": [1.0, 0.75], \"x2\": [0.75, 1.0]}))  X Out[1]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 2\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0\n    random_sleep: 0\n    sleep: 0\n  max_workers: 10\n  vectorized: false\ngenerator:\n  computation_time: null\n  fixed_features: null\n  ga_generator:\n    crossover_probability: 0.9\n    mutation_probability: 1.0\n    output_path: null\n    population: null\n    population_file: null\n    population_size: 64\n    supports_multi_objective: true\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: mggpo\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  population_size: 64\n  reference_point:\n    y1: 1.5\n    y2: 1.5\n  supports_batch_generation: true\n  supports_multi_objective: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives:\n    y1: MINIMIZE\n    y2: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[2]: Copied! <pre>for i in range(10):\n    print(i)\n    X.step()\n</pre> for i in range(10):     print(i)     X.step() <pre>0\n</pre> <pre>1\n</pre> <pre>2\n</pre> <pre>3\n</pre> <pre>4\n</pre> <pre>5\n</pre> <pre>6\n</pre> <pre>7\n</pre> <pre>8\n</pre> <pre>9\n</pre> In\u00a0[3]: Copied! <pre>X.generator.data\n</pre> X.generator.data Out[3]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 0 1.000000 0.750000 dummy_constant 1.000000 0.750000 0.626888 0.312500 0.000037 False 1 0.750000 1.000000 dummy_constant 0.750000 1.000000 0.626888 0.312500 0.000008 False 2 0.489740 2.744250 dummy_constant 0.489740 2.744250 6.865804 5.036765 0.000025 False 3 0.152615 2.562599 dummy_constant 0.152615 2.562599 5.532182 4.374993 0.000008 False 4 1.137227 2.585382 dummy_constant 1.137227 2.585382 6.883452 4.754877 0.000007 False ... ... ... ... ... ... ... ... ... ... 97 0.553558 0.800796 dummy_constant 0.553558 0.800796 0.044527 0.093347 0.000006 False 98 0.583130 0.790726 dummy_constant 0.583130 0.790726 0.038995 0.091432 0.000005 False 99 0.234806 0.995286 dummy_constant 0.234806 0.995286 0.130170 0.315636 0.000005 False 100 0.716297 0.018000 dummy_constant 0.716297 0.018000 -0.578624 0.279108 0.000005 False 101 0.802291 0.592590 dummy_constant 0.802291 0.592590 0.067722 0.099953 0.000005 False <p>102 rows \u00d7 9 columns</p> In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\ntheta = np.linspace(0, np.pi / 2)\nr = np.sqrt(1 + 0.1 * np.cos(16 * theta))\nx_1 = r * np.sin(theta)\nx_2_lower = r * np.cos(theta)\nx_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5\n\nz = np.zeros_like(x_1)\n\n# ax2.plot(x_1, x_2_lower,'r')\nax.fill_between(x_1, z, x_2_lower, fc=\"white\")\ncircle = plt.Circle(\n    (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\"\n)\nax.add_patch(circle)\nhistory = pd.concat(\n    [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False\n)\n\nax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\nax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\nax.set_xlim(0, 3.14)\nax.set_ylim(0, 3.14)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots()  theta = np.linspace(0, np.pi / 2) r = np.sqrt(1 + 0.1 * np.cos(16 * theta)) x_1 = r * np.sin(theta) x_2_lower = r * np.cos(theta) x_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5  z = np.zeros_like(x_1)  # ax2.plot(x_1, x_2_lower,'r') ax.fill_between(x_1, z, x_2_lower, fc=\"white\") circle = plt.Circle(     (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\" ) ax.add_patch(circle) history = pd.concat(     [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False )  ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\") ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")  ax.set_xlim(0, 3.14) ax.set_ylim(0, 3.14) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") ax.set_aspect(\"equal\") In\u00a0[5]: Copied! <pre>from matplotlib import pyplot as plt  # plot model predictions\n\ndata = X.data\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.train_model(X.generator.data)\n\n# create mesh\nn = 50\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nxx, yy = xx.numpy(), yy.numpy()\n\noutputs = X.generator.vocs.output_names\nwith torch.no_grad():\n    post = model.posterior(pts)\n\n    for i in range(len(vocs.output_names)):\n        mean = post.mean[...,i]\n        fig, ax = plt.subplots()\n        ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C1\")\n        c = ax.pcolor(\n            xx, yy, mean.squeeze().reshape(n, n),\n            cmap=\"seismic\",\n            vmin=-10.0,\n            vmax=10.0)\n        fig.colorbar(c)\n        ax.set_title(f\"Posterior mean: {outputs[i]}\")\n</pre> from matplotlib import pyplot as plt  # plot model predictions  data = X.data  bounds = X.generator.vocs.bounds model = X.generator.train_model(X.generator.data)  # create mesh n = 50 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  xx, yy = xx.numpy(), yy.numpy()  outputs = X.generator.vocs.output_names with torch.no_grad():     post = model.posterior(pts)      for i in range(len(vocs.output_names)):         mean = post.mean[...,i]         fig, ax = plt.subplots()         ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C1\")         c = ax.pcolor(             xx, yy, mean.squeeze().reshape(n, n),             cmap=\"seismic\",             vmin=-10.0,             vmax=10.0)         fig.colorbar(c)         ax.set_title(f\"Posterior mean: {outputs[i]}\") In\u00a0[6]: Copied! <pre># plot the acquisition function\nfrom xopt.generators.bayesian.objectives import feasibility\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.model\n\n# create mesh\nn = 25\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nxx, yy = xx.numpy(), yy.numpy()\n\nacq_func = X.generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\n    ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")\n\n    feas = feasibility(pts.unsqueeze(1), model, tnk_vocs).flatten()\n\n    fig2, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n\ncandidate = pd.DataFrame(X.generator.generate(1), index=[0])\nprint(candidate[[\"x1\", \"x2\"]].to_numpy())\nax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\")\n</pre> # plot the acquisition function from xopt.generators.bayesian.objectives import feasibility  bounds = X.generator.vocs.bounds model = X.generator.model  # create mesh n = 25 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  xx, yy = xx.numpy(), yy.numpy()  acq_func = X.generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")     fig.colorbar(c)     ax.set_title(\"Acquisition function\")      ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")     ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")      ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")      feas = feasibility(pts.unsqueeze(1), model, tnk_vocs).flatten()      fig2, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(\"Feasible Region\")  candidate = pd.DataFrame(X.generator.generate(1), index=[0]) print(candidate[[\"x1\", \"x2\"]].to_numpy()) ax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\") <pre>[[0.96859348 0.27997873]]\n</pre> Out[6]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f4473717430&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/multi_objective_bayes_opt/mggpo/#multi-objective-bayesian-optimization","title":"Multi-objective Bayesian Optimization\u00b6","text":"<p>TNK function $n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/multi_objective_bayes_opt/mggpo/#plot-results","title":"plot results\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mobo/","title":"Multi-objective Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nN_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\nN_STEPS = 1 if SMOKE_TEST else 30\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import MOBOGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\n\nevaluator = Evaluator(function=evaluate_TNK)\nprint(tnk_vocs.dict())\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") N_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20 N_STEPS = 1 if SMOKE_TEST else 30  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")   import pandas as pd import numpy as np import torch  from xopt import Xopt, Evaluator from xopt.generators.bayesian import MOBOGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs   evaluator = Evaluator(function=evaluate_TNK) print(tnk_vocs.dict()) <pre>{'variables': {'x1': [0.0, 3.14159], 'x2': [0.0, 3.14159]}, 'constraints': {'c1': ['GREATER_THAN', 0.0], 'c2': ['LESS_THAN', 0.5]}, 'objectives': {'y1': 'MINIMIZE', 'y2': 'MINIMIZE'}, 'constants': {'a': 'dummy_constant'}, 'observables': []}\n</pre> In\u00a0[2]: Copied! <pre>generator = MOBOGenerator(vocs=tnk_vocs, reference_point = {\"y1\":1.5,\"y2\":1.5})\ngenerator.n_monte_carlo_samples = N_MC_SAMPLES\ngenerator.numerical_optimizer.n_restarts = NUM_RESTARTS\n\nX = Xopt(generator=generator, evaluator=evaluator, vocs=tnk_vocs)\nX.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.75, 1.0]}))\n\nfor i in range(N_STEPS):\n    print(i)\n    X.step()\n</pre> generator = MOBOGenerator(vocs=tnk_vocs, reference_point = {\"y1\":1.5,\"y2\":1.5}) generator.n_monte_carlo_samples = N_MC_SAMPLES generator.numerical_optimizer.n_restarts = NUM_RESTARTS  X = Xopt(generator=generator, evaluator=evaluator, vocs=tnk_vocs) X.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.75, 1.0]}))  for i in range(N_STEPS):     print(i)     X.step() <pre>0\n</pre> <pre>1\n</pre> <pre>2\n</pre> <pre>3\n</pre> <pre>4\n</pre> <pre>5\n</pre> <pre>6\n</pre> <pre>7\n</pre> <pre>8\n</pre> <pre>9\n</pre> <pre>10\n</pre> <pre>11\n</pre> <pre>12\n</pre> <pre>13\n</pre> <pre>14\n</pre> <pre>15\n</pre> <pre>16\n</pre> <pre>17\n</pre> <pre>18\n</pre> <pre>19\n</pre> <pre>20\n</pre> <pre>21\n</pre> <pre>22\n</pre> <pre>23\n</pre> <pre>24\n</pre> <pre>25\n</pre> <pre>26\n</pre> <pre>27\n</pre> <pre>28\n</pre> <pre>29\n</pre> In\u00a0[3]: Copied! <pre>X.generator.data\n</pre> X.generator.data Out[3]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 0 1.000000 0.750000 dummy_constant 1.000000 0.750000 0.626888 0.312500 0.000050 False 1 0.750000 1.000000 dummy_constant 0.750000 1.000000 0.626888 0.312500 0.000012 False 2 1.362365 2.476605 dummy_constant 1.362365 2.476605 7.008757 4.650639 0.000026 False 3 0.113292 0.337958 dummy_constant 0.113292 0.337958 -0.917598 0.175801 0.000024 False 4 0.123120 0.000000 dummy_constant 0.123120 0.000000 -1.084842 0.392039 0.000024 False 5 0.718207 0.000000 dummy_constant 0.718207 0.000000 -0.584178 0.297614 0.000023 False 6 0.995446 0.422276 dummy_constant 0.995446 0.422276 0.070153 0.251508 0.000026 False 7 0.506089 0.993146 dummy_constant 0.506089 0.993146 0.211631 0.243230 0.000025 False 8 0.000000 0.931858 dummy_constant 0.000000 0.931858 -0.231641 0.436501 0.000026 False 9 0.722141 0.751122 dummy_constant 0.722141 0.751122 -0.009417 0.112409 0.000025 False 10 1.095773 0.175526 dummy_constant 1.095773 0.175526 0.314048 0.460229 0.000025 False 11 3.141590 0.000000 dummy_constant 3.141590 0.000000 8.769588 7.227998 0.000025 False 12 0.211834 1.068776 dummy_constant 0.211834 1.068776 0.287150 0.406546 0.000027 False 13 0.801966 0.740405 dummy_constant 0.801966 0.740405 0.111036 0.148978 0.000026 False 14 0.075189 1.024490 dummy_constant 0.075189 1.024490 0.016418 0.455555 0.000027 False 15 1.011921 0.053864 dummy_constant 1.011921 0.053864 -0.039047 0.461101 0.000025 False 16 0.556669 0.893046 dummy_constant 0.556669 0.893046 0.194865 0.157696 0.000025 False 17 0.612523 0.829876 dummy_constant 0.612523 0.829876 0.137145 0.121480 0.000027 False 18 0.930908 0.396366 dummy_constant 0.930908 0.396366 -0.075069 0.196421 0.000022 False 19 0.291384 0.940055 dummy_constant 0.291384 0.940055 -0.041055 0.237169 0.000026 False 20 0.159615 1.463505 dummy_constant 0.159615 1.463505 1.183980 1.044204 0.000024 False 21 0.237733 0.824769 dummy_constant 0.237733 0.824769 -0.241197 0.174259 0.000025 False 22 0.022904 1.201526 dummy_constant 0.022904 1.201526 0.348805 0.719759 0.000025 False 23 1.174700 0.077295 dummy_constant 1.174700 0.077295 0.336249 0.633900 0.000025 False 24 0.068155 1.049268 dummy_constant 0.068155 1.049268 0.054798 0.488185 0.000026 False 25 0.085706 1.233066 dummy_constant 0.085706 1.233066 0.483358 0.709025 0.000024 False 26 0.029283 0.841682 dummy_constant 0.029283 0.841682 -0.375628 0.338321 0.000025 False 27 1.048151 0.075063 dummy_constant 1.048151 0.075063 0.062849 0.481040 0.000026 False 28 0.396976 0.920010 dummy_constant 0.396976 0.920010 -0.093256 0.187022 0.000026 False 29 0.499089 0.881570 dummy_constant 0.499089 0.881570 0.064124 0.145597 0.000026 False 30 0.121597 0.662786 dummy_constant 0.121597 0.662786 -0.448758 0.169688 0.000026 False 31 0.875884 0.589234 dummy_constant 0.875884 0.589234 0.214243 0.149251 0.000027 False In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\ntheta = np.linspace(0, np.pi / 2)\nr = np.sqrt(1 + 0.1 * np.cos(16 * theta))\nx_1 = r * np.sin(theta)\nx_2_lower = r * np.cos(theta)\nx_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5\n\nz = np.zeros_like(x_1)\n\n# ax2.plot(x_1, x_2_lower,'r')\nax.fill_between(x_1, z, x_2_lower, fc=\"white\")\ncircle = plt.Circle(\n    (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\"\n)\nax.add_patch(circle)\nhistory = pd.concat(\n    [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False\n)\n\n\nax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\nax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\nax.set_xlim(0, 3.14)\nax.set_ylim(0, 3.14)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots()  theta = np.linspace(0, np.pi / 2) r = np.sqrt(1 + 0.1 * np.cos(16 * theta)) x_1 = r * np.sin(theta) x_2_lower = r * np.cos(theta) x_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5  z = np.zeros_like(x_1)  # ax2.plot(x_1, x_2_lower,'r') ax.fill_between(x_1, z, x_2_lower, fc=\"white\") circle = plt.Circle(     (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\" ) ax.add_patch(circle) history = pd.concat(     [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False )   ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\") ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")  ax.set_xlim(0, 3.14) ax.set_ylim(0, 3.14) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") ax.set_aspect(\"equal\") In\u00a0[5]: Copied! <pre>ax = history.plot(\"x1\", \"x2\")\nax.set_ylim(0, 3.14)\nax.set_xlim(0, 3.14)\nax.set_aspect(\"equal\")\n</pre> ax = history.plot(\"x1\", \"x2\") ax.set_ylim(0, 3.14) ax.set_xlim(0, 3.14) ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre># plot the acquisition function\nfrom xopt.generators.bayesian.objectives import feasibility\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.model\n\n# create mesh\nn = 200\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nxx, yy = xx.numpy(), yy.numpy()\n\nacq_func = X.generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\n    ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")\n\n    feas = feasibility(pts.unsqueeze(1), model, tnk_vocs).flatten()\n\n    fig2, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n\ncandidate = pd.DataFrame(X.generator.generate(1), index=[0])\nprint(candidate[[\"x1\", \"x2\"]].to_numpy())\nax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\")\n</pre> # plot the acquisition function from xopt.generators.bayesian.objectives import feasibility  bounds = X.generator.vocs.bounds model = X.generator.model  # create mesh n = 200 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  xx, yy = xx.numpy(), yy.numpy()  acq_func = X.generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")     fig.colorbar(c)     ax.set_title(\"Acquisition function\")      ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")     ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")      ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")      feas = feasibility(pts.unsqueeze(1), model, tnk_vocs).flatten()      fig2, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(\"Feasible Region\")  candidate = pd.DataFrame(X.generator.generate(1), index=[0]) print(candidate[[\"x1\", \"x2\"]].to_numpy()) ax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\") <pre>[[0.52849092 0.7910534 ]]\n</pre> Out[6]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f3ae1e96580&gt;]</pre> In\u00a0[7]: Copied! <pre>%%time\ncandidate = X.generator.generate(1)\n</pre> %%time candidate = X.generator.generate(1) <pre>CPU times: user 2 s, sys: 55.5 ms, total: 2.06 s\nWall time: 1.03 s\n</pre>"},{"location":"examples/multi_objective_bayes_opt/mobo/#multi-objective-bayesian-optimization","title":"Multi-objective Bayesian Optimization\u00b6","text":"<p>TNK function $n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/multi_objective_bayes_opt/mobo/#plot-results","title":"plot results\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mobo/#plot-path-through-input-space","title":"Plot path through input space\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mobo_from_yaml/","title":"Multi-objective Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre>import os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_STEPS = 2 if SMOKE_TEST else 50\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom xopt import Xopt\n\n\nYAML = \"\"\"\ngenerator:\n    name: mobo\n    reference_point: {y1: 1.5, y2: 1.5}\n\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    constants: {a: dummy_constant}\n\n\"\"\"\n</pre> import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") NUM_STEPS = 2 if SMOKE_TEST else 50  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import torch from xopt import Xopt   YAML = \"\"\" generator:     name: mobo     reference_point: {y1: 1.5, y2: 1.5}  evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     constants: {a: dummy_constant}  \"\"\" In\u00a0[2]: Copied! <pre>X = Xopt.from_yaml(YAML)\n\n# for testing purposes only\nif SMOKE_TEST:\n    X.generator.numerical_optimizer.n_restarts = 1\n    X.generator.n_monte_carlo_samples = 1\n\nX.random_evaluate(5)\nfor i in range(NUM_STEPS):\n    print(i)\n    X.step()\n</pre> X = Xopt.from_yaml(YAML)  # for testing purposes only if SMOKE_TEST:     X.generator.numerical_optimizer.n_restarts = 1     X.generator.n_monte_carlo_samples = 1  X.random_evaluate(5) for i in range(NUM_STEPS):     print(i)     X.step() <pre>0\n</pre> <pre>1\n</pre> <pre>2\n</pre> <pre>3\n</pre> <pre>4\n</pre> <pre>5\n</pre> <pre>6\n</pre> <pre>7\n</pre> <pre>8\n</pre> <pre>9\n</pre> <pre>10\n</pre> <pre>11\n</pre> <pre>12\n</pre> <pre>13\n</pre> <pre>14\n</pre> <pre>15\n</pre> <pre>16\n</pre> <pre>17\n</pre> <pre>18\n</pre> <pre>19\n</pre> <pre>20\n</pre> <pre>21\n</pre> <pre>22\n</pre> <pre>23\n</pre> <pre>24\n</pre> <pre>25\n</pre> <pre>26\n</pre> <pre>27\n</pre> <pre>28\n</pre> <pre>29\n</pre> <pre>30\n</pre> <pre>31\n</pre> <pre>32\n</pre> <pre>33\n</pre> <pre>34\n</pre> <pre>35\n</pre> <pre>36\n</pre> <pre>37\n</pre> <pre>38\n</pre> <pre>39\n</pre> <pre>40\n</pre> <pre>41\n</pre> <pre>42\n</pre> <pre>43\n</pre> <pre>44\n</pre> <pre>45\n</pre> <pre>46\n</pre> <pre>47\n</pre> <pre>48\n</pre> <pre>49\n</pre> In\u00a0[3]: Copied! <pre>X.generator.data\n</pre> X.generator.data Out[3]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 0 2.104539 3.034119 dummy_constant 2.104539 3.034119 12.731120 8.996306 0.000034 False 1 1.238668 0.205253 dummy_constant 1.238668 0.205253 0.663495 0.632505 0.000009 False 2 0.310844 0.435250 dummy_constant 0.310844 0.435250 -0.626080 0.039973 0.000007 False 3 1.202555 1.347195 dummy_constant 1.202555 1.347195 2.199435 1.211323 0.000006 False 4 3.057288 0.748026 dummy_constant 3.057288 0.748026 8.983186 6.601240 0.000006 False 5 0.313944 0.106545 dummy_constant 0.313944 0.106545 -0.939989 0.189424 0.000026 False 6 0.271397 0.863938 dummy_constant 0.271397 0.863938 -0.195655 0.184710 0.000024 False 7 1.135652 0.267033 dummy_constant 1.135652 0.267033 0.446083 0.458327 0.000024 False 8 0.287086 1.035270 dummy_constant 0.287086 1.035270 0.191689 0.331846 0.000024 False 9 0.970193 0.326842 dummy_constant 0.970193 0.326842 0.001330 0.251065 0.000024 False 10 0.865866 0.009187 dummy_constant 0.865866 0.009187 -0.348754 0.374756 0.000026 False 11 0.888363 0.776701 dummy_constant 0.888363 0.776701 0.344562 0.227389 0.000026 False 12 0.795926 0.704645 dummy_constant 0.795926 0.704645 0.073666 0.129452 0.000025 False 13 0.548570 0.877508 dummy_constant 0.548570 0.877508 0.159394 0.144872 0.000026 False 14 0.165110 1.051962 dummy_constant 0.165110 1.051962 0.213454 0.416813 0.000024 False 15 0.800300 0.262495 dummy_constant 0.800300 0.262495 -0.325716 0.146589 0.000025 False 16 0.905493 0.134918 dummy_constant 0.905493 0.134918 -0.090439 0.297709 0.000026 False 17 0.252166 0.967968 dummy_constant 0.252166 0.967968 0.059856 0.280416 0.000025 False 18 0.891693 0.545392 dummy_constant 0.891693 0.545392 0.172666 0.155484 0.000027 False 19 0.660603 0.777662 dummy_constant 0.660603 0.777662 0.014343 0.102890 0.000027 False 20 0.575094 0.823419 dummy_constant 0.575094 0.823419 0.103360 0.110239 0.000026 False 21 0.000000 0.968828 dummy_constant 0.000000 0.968828 -0.161372 0.469800 0.000025 False 22 0.446832 0.771188 dummy_constant 0.446832 0.771188 -0.153490 0.076370 0.000025 False 23 1.032317 0.071095 dummy_constant 1.032317 0.071095 0.025389 0.467320 0.000025 False 24 0.402650 0.467148 dummy_constant 0.402650 0.467148 -0.657341 0.010556 0.000025 False 25 0.405516 0.904644 dummy_constant 0.405516 0.904644 -0.106811 0.172664 0.000027 False 26 1.049709 0.100195 dummy_constant 1.049709 0.100195 0.107109 0.462024 0.000026 False 27 0.100721 0.387660 dummy_constant 0.100721 0.387660 -0.779437 0.172044 0.000027 False 28 0.394181 0.766093 dummy_constant 0.394181 0.766093 -0.282527 0.082003 0.000025 False 29 0.062739 1.028362 dummy_constant 0.062739 1.028362 0.005342 0.470363 0.000026 False 30 0.913796 0.421049 dummy_constant 0.913796 0.421049 -0.068781 0.177460 0.000025 False 31 0.057361 1.049362 dummy_constant 0.057361 1.049362 0.040254 0.497728 0.000025 False 32 0.404055 0.530439 dummy_constant 0.404055 0.530439 -0.500568 0.010132 0.000025 False 33 0.087295 1.049741 dummy_constant 0.087295 1.049741 0.085484 0.472540 0.000026 False 34 0.831686 0.207140 dummy_constant 0.831686 0.207140 -0.193178 0.195783 0.000027 False 35 0.800699 0.382057 dummy_constant 0.800699 0.382057 -0.279653 0.104330 0.000027 False 36 0.818331 0.446601 dummy_constant 0.818331 0.446601 -0.117011 0.104186 0.000026 False 37 0.821382 0.605651 dummy_constant 0.821382 0.605651 0.115262 0.114448 0.000025 False 38 0.331490 0.251863 dummy_constant 0.331490 0.251863 -0.770233 0.089967 0.000027 False 39 0.737897 0.692585 dummy_constant 0.737897 0.692585 -0.063272 0.093684 0.000026 False 40 0.941389 0.367462 dummy_constant 0.941389 0.367462 -0.073402 0.212391 0.000027 False 41 0.204411 0.993595 dummy_constant 0.204411 0.993595 0.128466 0.331009 0.000026 False 42 0.259371 0.451273 dummy_constant 0.259371 0.451273 -0.681805 0.060277 0.000027 False 43 0.780466 0.668468 dummy_constant 0.780466 0.668468 0.022957 0.107042 0.000025 False 44 0.659264 0.287325 dummy_constant 0.659264 0.287325 -0.578557 0.070596 0.000025 False 45 0.881720 0.105644 dummy_constant 0.881720 0.105644 -0.178329 0.301227 0.000026 False 46 0.047437 0.658769 dummy_constant 0.047437 0.658769 -0.604608 0.230021 0.000026 False 47 0.411271 0.801312 dummy_constant 0.411271 0.801312 -0.215135 0.098662 0.000028 False 48 0.445466 0.486760 dummy_constant 0.445466 0.486760 -0.640574 0.003149 0.000026 False 49 0.664210 0.415287 dummy_constant 0.664210 0.415287 -0.297870 0.034141 0.000027 False 50 0.954424 0.343676 dummy_constant 0.954424 0.343676 -0.043921 0.230939 0.000026 False 51 0.157334 0.933578 dummy_constant 0.157334 0.933578 -0.014532 0.305410 0.000028 False 52 0.997371 0.165628 dummy_constant 0.997371 0.165628 0.109525 0.359183 0.000026 False 53 0.012663 1.250670 dummy_constant 0.012663 1.250670 0.465645 0.801002 0.000028 False 54 0.668418 0.325972 dummy_constant 0.668418 0.325972 -0.502938 0.058650 0.000026 False In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfig, ax = plt.subplots()\n\ntheta = np.linspace(0, np.pi / 2)\nr = np.sqrt(1 + 0.1 * np.cos(16 * theta))\nx_1 = r * np.sin(theta)\nx_2_lower = r * np.cos(theta)\nx_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5\n\nz = np.zeros_like(x_1)\n\n# ax2.plot(x_1, x_2_lower,'r')\nax.fill_between(x_1, z, x_2_lower, fc=\"white\")\ncircle = plt.Circle(\n    (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\"\n)\nax.add_patch(circle)\nhistory = pd.concat(\n    [X.data, X.vocs.feasibility_data(X.data)], axis=1, ignore_index=False\n)\n\n\nax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\nax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\nax.set_xlim(0, 3.14)\nax.set_ylim(0, 3.14)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd  fig, ax = plt.subplots()  theta = np.linspace(0, np.pi / 2) r = np.sqrt(1 + 0.1 * np.cos(16 * theta)) x_1 = r * np.sin(theta) x_2_lower = r * np.cos(theta) x_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5  z = np.zeros_like(x_1)  # ax2.plot(x_1, x_2_lower,'r') ax.fill_between(x_1, z, x_2_lower, fc=\"white\") circle = plt.Circle(     (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\" ) ax.add_patch(circle) history = pd.concat(     [X.data, X.vocs.feasibility_data(X.data)], axis=1, ignore_index=False )   ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\") ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")  ax.set_xlim(0, 3.14) ax.set_ylim(0, 3.14) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") ax.set_aspect(\"equal\") In\u00a0[5]: Copied! <pre>ax = history.plot(\"x1\", \"x2\")\nax.set_ylim(0, 3.14)\nax.set_xlim(0, 3.14)\nax.set_aspect(\"equal\")\n</pre> ax = history.plot(\"x1\", \"x2\") ax.set_ylim(0, 3.14) ax.set_xlim(0, 3.14) ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre># plot the acquisition function\nfrom xopt.generators.bayesian.objectives import feasibility\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.model\n\n# create mesh\nn = 100\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nxx, yy = xx.numpy(), yy.numpy()\n\nacq_func = X.generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n    c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\n    ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")\n\n    feas = feasibility(pts.unsqueeze(1), model, X.vocs).flatten()\n\n    fig2, ax2 = plt.subplots(figsize=(8,8))\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n\ncandidate = pd.DataFrame(X.generator.generate(1), index=[0])\nprint(candidate[[\"x1\", \"x2\"]].to_numpy())\nax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\")\n</pre> # plot the acquisition function from xopt.generators.bayesian.objectives import feasibility  bounds = X.generator.vocs.bounds model = X.generator.model  # create mesh n = 100 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  xx, yy = xx.numpy(), yy.numpy()  acq_func = X.generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots(figsize=(8,8))     c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")     fig.colorbar(c)     ax.set_title(\"Acquisition function\")      ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")     ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")      ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")      feas = feasibility(pts.unsqueeze(1), model, X.vocs).flatten()      fig2, ax2 = plt.subplots(figsize=(8,8))     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(\"Feasible Region\")  candidate = pd.DataFrame(X.generator.generate(1), index=[0]) print(candidate[[\"x1\", \"x2\"]].to_numpy()) ax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\") <pre>[[0.85617608 0.56022887]]\n</pre> Out[6]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f11bd1a5f10&gt;]</pre>"},{"location":"examples/multi_objective_bayes_opt/mobo_from_yaml/#multi-objective-bayesian-optimization","title":"Multi-objective Bayesian Optimization\u00b6","text":"<p>TNK function $n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/multi_objective_bayes_opt/mobo_from_yaml/#plot-results","title":"plot results\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mobo_from_yaml/#plot-path-through-input-space","title":"Plot path through input space\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/","title":"Multi-fidelity Multi-objective Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nN_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\nBUDGET = 0.02 if SMOKE_TEST else 10\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import MultiFidelityGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\nevaluator = Evaluator(function=evaluate_TNK)\nprint(tnk_vocs.dict())\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") N_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20 BUDGET = 0.02 if SMOKE_TEST else 10  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import pandas as pd import numpy as np import torch  from xopt import Xopt, Evaluator from xopt.generators.bayesian import MultiFidelityGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs  evaluator = Evaluator(function=evaluate_TNK) print(tnk_vocs.dict()) <pre>{'variables': {'x1': [0.0, 3.14159], 'x2': [0.0, 3.14159]}, 'constraints': {'c1': ['GREATER_THAN', 0.0], 'c2': ['LESS_THAN', 0.5]}, 'objectives': {'y1': 'MINIMIZE', 'y2': 'MINIMIZE'}, 'constants': {'a': 'dummy_constant'}, 'observables': []}\n</pre> In\u00a0[2]: Copied! <pre>from copy import deepcopy\nmy_vocs = deepcopy(tnk_vocs)\nmy_vocs.constraints = {}\ngenerator = MultiFidelityGenerator(vocs=my_vocs, reference_point = {\"y1\":1.5,\"y2\":1.5})\n\n# set cost function according to approximate scaling of laser plasma accelerator\n# problem, see https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.5.013063\ngenerator.cost_function = lambda s: s**3.5\ngenerator.numerical_optimizer.n_restarts = NUM_RESTARTS\ngenerator.n_monte_carlo_samples = N_MC_SAMPLES\n\nX = Xopt(generator=generator, evaluator=evaluator, vocs=my_vocs)\n\n# evaluate at some explicit initial points\nX.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.75, 1.0],\"s\":[0.0,0.1]}))\n\nX\n</pre> from copy import deepcopy my_vocs = deepcopy(tnk_vocs) my_vocs.constraints = {} generator = MultiFidelityGenerator(vocs=my_vocs, reference_point = {\"y1\":1.5,\"y2\":1.5})  # set cost function according to approximate scaling of laser plasma accelerator # problem, see https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.5.013063 generator.cost_function = lambda s: s**3.5 generator.numerical_optimizer.n_restarts = NUM_RESTARTS generator.n_monte_carlo_samples = N_MC_SAMPLES  X = Xopt(generator=generator, evaluator=evaluator, vocs=my_vocs)  # evaluate at some explicit initial points X.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.75, 1.0],\"s\":[0.0,0.1]}))  X Out[2]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 2\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0\n    random_sleep: 0\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: multi_fidelity\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  reference_point:\n    s: 0.0\n    y1: 1.5\n    y2: 1.5\n  supports_batch_generation: true\n  supports_multi_objective: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints: {}\n  objectives:\n    s: MAXIMIZE\n    y1: MINIMIZE\n    y2: MINIMIZE\n  observables: []\n  variables:\n    s:\n    - 0\n    - 1\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[3]: Copied! <pre>budget = BUDGET\nwhile X.generator.calculate_total_cost() &lt; budget:\n    X.step()\n    print(f\"n_samples: {len(X.data)} \"\n          f\"budget used: {X.generator.calculate_total_cost():.4} \"\n          f\"hypervolume: {X.generator.calculate_hypervolume():.4}\")\n</pre> budget = BUDGET while X.generator.calculate_total_cost() &lt; budget:     X.step()     print(f\"n_samples: {len(X.data)} \"           f\"budget used: {X.generator.calculate_total_cost():.4} \"           f\"hypervolume: {X.generator.calculate_hypervolume():.4}\") <pre>n_samples: 3 budget used: 0.006531 hypervolume: 0.0375\n</pre> <pre>n_samples: 4 budget used: 0.01653 hypervolume: 0.5723\n</pre> <pre>n_samples: 5 budget used: 0.04188 hypervolume: 0.7874\n</pre> <pre>n_samples: 6 budget used: 0.09459 hypervolume: 0.9705\n</pre> <pre>n_samples: 7 budget used: 0.211 hypervolume: 1.217\n</pre> <pre>n_samples: 8 budget used: 0.221 hypervolume: 1.217\n</pre> <pre>n_samples: 9 budget used: 0.4893 hypervolume: 1.545\n</pre> <pre>n_samples: 10 budget used: 1.024 hypervolume: 1.882\n</pre> <pre>n_samples: 11 budget used: 2.024 hypervolume: 2.25\n</pre> <pre>n_samples: 12 budget used: 3.024 hypervolume: 2.25\n</pre> <pre>n_samples: 13 budget used: 4.024 hypervolume: 2.25\n</pre> <pre>n_samples: 14 budget used: 4.053 hypervolume: 2.25\n</pre> <pre>n_samples: 15 budget used: 4.097 hypervolume: 2.25\n</pre> <pre>n_samples: 16 budget used: 5.097 hypervolume: 2.25\n</pre> <pre>n_samples: 17 budget used: 6.097 hypervolume: 2.25\n</pre> <pre>n_samples: 18 budget used: 7.097 hypervolume: 2.25\n</pre> <pre>n_samples: 19 budget used: 8.097 hypervolume: 2.25\n</pre> <pre>n_samples: 20 budget used: 8.316 hypervolume: 2.25\n</pre> <pre>n_samples: 21 budget used: 8.522 hypervolume: 2.25\n</pre> <pre>n_samples: 22 budget used: 8.539 hypervolume: 2.25\n</pre> <pre>n_samples: 23 budget used: 9.022 hypervolume: 2.25\n</pre> <pre>n_samples: 24 budget used: 9.258 hypervolume: 2.25\n</pre> <pre>n_samples: 25 budget used: 9.258 hypervolume: 2.25\n</pre> <pre>n_samples: 26 budget used: 9.276 hypervolume: 2.25\n</pre> <pre>n_samples: 27 budget used: 9.314 hypervolume: 2.25\n</pre> <pre>n_samples: 28 budget used: 9.314 hypervolume: 2.25\n</pre> <pre>n_samples: 29 budget used: 9.315 hypervolume: 2.25\n</pre> <pre>n_samples: 30 budget used: 9.517 hypervolume: 2.25\n</pre> <pre>n_samples: 31 budget used: 9.586 hypervolume: 2.25\n</pre> <pre>n_samples: 32 budget used: 9.791 hypervolume: 2.25\n</pre> <pre>n_samples: 33 budget used: 9.803 hypervolume: 2.25\n</pre> <pre>n_samples: 34 budget used: 9.813 hypervolume: 2.25\n</pre> <pre>n_samples: 35 budget used: 10.29 hypervolume: 2.25\n</pre> In\u00a0[4]: Copied! <pre>X.data\n</pre> X.data Out[4]: x1 x2 s a y1 y2 c1 c2 xopt_runtime xopt_error 0 1.000000 0.750000 0.000000 dummy_constant 1.000000 0.750000 0.626888 0.312500 0.000034 False 1 0.750000 1.000000 0.100000 dummy_constant 0.750000 1.000000 0.626888 0.312500 0.000007 False 2 0.343820 1.733162 0.234178 dummy_constant 0.343820 1.733162 2.222058 1.545080 0.000025 False 3 0.000000 0.077833 0.268270 dummy_constant 0.000000 0.077833 -1.093942 0.428225 0.000025 False 4 0.000000 0.000000 0.349937 dummy_constant 0.000000 0.000000 -1.100000 0.500000 0.000024 False 5 0.000000 0.000000 0.431336 dummy_constant 0.000000 0.000000 -1.100000 0.500000 0.000024 False 6 0.000000 0.000000 0.540967 dummy_constant 0.000000 0.000000 -1.100000 0.500000 0.000023 False 7 3.141590 0.000000 0.268270 dummy_constant 3.141590 0.000000 8.769588 7.227998 0.000025 False 8 0.000000 0.000000 0.686629 dummy_constant 0.000000 0.000000 -1.100000 0.500000 0.000024 False 9 0.000000 0.000000 0.836362 dummy_constant 0.000000 0.000000 -1.100000 0.500000 0.000024 False 10 0.000000 0.000000 1.000000 dummy_constant 0.000000 0.000000 -1.100000 0.500000 0.000026 False 11 0.000000 0.514968 1.000000 dummy_constant 0.000000 0.514968 -0.834808 0.250224 0.000023 False 12 0.489695 0.000000 1.000000 dummy_constant 0.489695 0.000000 -0.860199 0.250106 0.000026 False 13 0.000000 0.543494 0.361513 dummy_constant 0.000000 0.543494 -0.804615 0.251892 0.000025 False 14 0.538918 0.000000 0.409269 dummy_constant 0.538918 0.000000 -0.809568 0.251515 0.000027 False 15 1.011301 0.000000 1.000000 dummy_constant 1.011301 0.000000 -0.077271 0.511429 0.000026 False 16 0.000000 0.240184 1.000000 dummy_constant 0.000000 0.240184 -1.042312 0.317504 0.000024 False 17 0.000000 3.141590 1.000000 dummy_constant 0.000000 3.141590 8.769588 7.227998 0.000026 False 18 0.688207 0.776052 1.000000 dummy_constant 0.688207 0.776052 0.018430 0.111626 0.000027 False 19 1.381731 1.538617 0.648267 dummy_constant 1.381731 1.538617 3.211182 1.856175 0.000027 False 20 0.858784 0.000000 0.636468 dummy_constant 0.858784 0.000000 -0.362489 0.378726 0.000025 False 21 1.105453 0.000000 0.316778 dummy_constant 1.105453 0.000000 0.122027 0.616574 0.000025 False 22 2.009357 1.652572 0.812205 dummy_constant 2.009357 1.652572 5.766828 3.606580 0.000026 False 23 0.000000 0.855070 0.661901 dummy_constant 0.000000 0.855070 -0.368855 0.376075 0.000024 False 24 1.963257 1.260135 0.014067 dummy_constant 1.963257 1.260135 4.538010 2.718928 0.000026 False 25 2.050407 0.973183 0.314366 dummy_constant 2.050407 0.973183 4.082098 2.627664 0.000025 False 26 0.762199 2.530715 0.394688 dummy_constant 0.762199 2.530715 5.988642 4.192551 0.000025 False 27 1.723203 2.284853 0.007387 dummy_constant 1.723203 2.284853 7.251040 4.681926 0.000026 False 28 0.480892 0.652474 0.142364 dummy_constant 0.480892 0.652474 -0.269011 0.023613 0.000027 False 29 2.669072 0.141036 0.632432 dummy_constant 2.669072 0.141036 6.077439 4.833729 0.000027 False 30 1.857334 1.042791 0.465798 dummy_constant 1.857334 1.042791 3.569643 2.136979 0.000025 False 31 0.320982 0.809365 0.635984 dummy_constant 0.320982 0.809365 -0.338980 0.127754 0.000027 False 32 0.090912 1.302402 0.282886 dummy_constant 0.090912 1.302402 0.660501 0.811201 0.000024 False 33 2.907054 0.156523 0.272324 dummy_constant 2.907054 0.156523 7.410270 5.911888 0.000024 False 34 0.185640 2.013705 0.810636 dummy_constant 0.185640 2.013705 3.079491 2.390124 0.000027 False In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\ntheta = np.linspace(0, np.pi / 2)\nr = np.sqrt(1 + 0.1 * np.cos(16 * theta))\nx_1 = r * np.sin(theta)\nx_2_lower = r * np.cos(theta)\nx_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5\n\nz = np.zeros_like(x_1)\n\n# ax2.plot(x_1, x_2_lower,'r')\nax.fill_between(x_1, z, x_2_lower, fc=\"white\")\ncircle = plt.Circle(\n    (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\"\n)\nax.add_patch(circle)\nhistory = pd.concat(\n    [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False\n)\n\nax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\nax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\nax.set_xlim(0, 3.14)\nax.set_ylim(0, 3.14)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots()  theta = np.linspace(0, np.pi / 2) r = np.sqrt(1 + 0.1 * np.cos(16 * theta)) x_1 = r * np.sin(theta) x_2_lower = r * np.cos(theta) x_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5  z = np.zeros_like(x_1)  # ax2.plot(x_1, x_2_lower,'r') ax.fill_between(x_1, z, x_2_lower, fc=\"white\") circle = plt.Circle(     (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\" ) ax.add_patch(circle) history = pd.concat(     [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False )  ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\") ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")  ax.set_xlim(0, 3.14) ax.set_ylim(0, 3.14) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre>ax = history.hist([\"x1\", \"x2\", \"s\"],bins=20)\n</pre> ax = history.hist([\"x1\", \"x2\", \"s\"],bins=20) In\u00a0[7]: Copied! <pre>history.plot(y=[\"x1\", \"x2\", \"s\"])\n</pre> history.plot(y=[\"x1\", \"x2\", \"s\"]) Out[7]: <pre>&lt;Axes: &gt;</pre> In\u00a0[8]: Copied! <pre># plot the acquisition function\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.model\n\n# create mesh over non-fidelity parameters\nn = 50\nx = torch.linspace(*bounds.T[1], n)\ny = torch.linspace(*bounds.T[2], n)\nxx, yy = torch.meshgrid(x, y)\n\n# plot function(s) at a single fidelity parameter\nfidelities = [0.0, 0.5, 1.0]\nfor fidelity in fidelities:\n    pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n    pts = torch.cat((torch.ones(pts.shape[0],1)*fidelity, pts), dim=-1)\n\n    acq_func = X.generator.get_acquisition(model)\n    with torch.no_grad():\n        acq_pts = pts.unsqueeze(1)\n        acq = acq_func(acq_pts)\n\n        fig, ax = plt.subplots()\n\n        xxn, yyn = xx.numpy(), yy.numpy()\n\n        c = ax.pcolor(xxn, yyn, acq.reshape(n, n), cmap=\"Blues\")\n        fig.colorbar(c)\n        ax.set_title(f\"Acquisition function - s: {fidelity}\")\n\n        ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\n        ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\n        ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")\n\n\ncandidate = pd.DataFrame(X.generator.generate(1), index=[0])\nprint(candidate[[\"x1\", \"x2\"]].to_numpy())\nax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\")\n</pre> # plot the acquisition function  bounds = X.generator.vocs.bounds model = X.generator.model  # create mesh over non-fidelity parameters n = 50 x = torch.linspace(*bounds.T[1], n) y = torch.linspace(*bounds.T[2], n) xx, yy = torch.meshgrid(x, y)  # plot function(s) at a single fidelity parameter fidelities = [0.0, 0.5, 1.0] for fidelity in fidelities:     pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()     pts = torch.cat((torch.ones(pts.shape[0],1)*fidelity, pts), dim=-1)      acq_func = X.generator.get_acquisition(model)     with torch.no_grad():         acq_pts = pts.unsqueeze(1)         acq = acq_func(acq_pts)          fig, ax = plt.subplots()          xxn, yyn = xx.numpy(), yy.numpy()          c = ax.pcolor(xxn, yyn, acq.reshape(n, n), cmap=\"Blues\")         fig.colorbar(c)         ax.set_title(f\"Acquisition function - s: {fidelity}\")          ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")         ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")          ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")   candidate = pd.DataFrame(X.generator.generate(1), index=[0]) print(candidate[[\"x1\", \"x2\"]].to_numpy()) ax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\") <pre>[[2.6783825  0.40776384]]\n</pre> Out[8]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f91f176f070&gt;]</pre> In\u00a0[9]: Copied! <pre># examine lengthscale of the first objective\nlist(model.models[0].named_parameters())\n</pre> # examine lengthscale of the first objective list(model.models[0].named_parameters()) Out[9]: <pre>[('likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-26.2393], dtype=torch.float64, requires_grad=True)),\n ('mean_module.raw_constant',\n  Parameter containing:\n  tensor(0.0133, dtype=torch.float64, requires_grad=True)),\n ('covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(0.1265, dtype=torch.float64, requires_grad=True)),\n ('covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[0.7620, 2.1937, 2.2436]], dtype=torch.float64, requires_grad=True))]</pre>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#multi-fidelity-multi-objective-bayesian-optimization","title":"Multi-fidelity Multi-objective Bayesian Optimization\u00b6","text":"<p>Here we attempt to solve for the constrained Pareto front of the TNK multi-objective optimization problem using Multi-Fidelity Multi-Objective Bayesian optimization. For simplicity we assume that the objective and constraint functions at lower fidelities is exactly equal to the functions at higher fidelities (this is obviously not a requirement, although for the best results lower fidelity calculations should correlate with higher fidelity ones). The algorithm should learn this relationship and use information gathered at lower fidelities to gather samples to improve the hypervolume of the Pareto front at the maximum fidelity.</p> <p>TNK function $n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#set-up-the-multi-fidelity-multi-objective-optimization-algorithm","title":"Set up the Multi-Fidelity Multi-objective optimization algorithm\u00b6","text":"<p>Here we create the Multi-Fidelity generator object which can solve both single and multi-objective optimization problems depending on the number of objectives in VOCS. We specify a cost function as a function of fidelity parameter $s=[0,1]$ as $C(s) = s^{3.5}$ as an example from a real life multi-fidelity simulation problem.</p>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#run-optimization-routine","title":"Run optimization routine\u00b6","text":"<p>Instead of ending the optimization routine after an explict number of samples we end optimization once a given optimization budget has been exceeded. WARNING: This will slightly exceed the given budget</p>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#show-results","title":"Show results\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#plot-results","title":"Plot results\u00b6","text":"<p>Here we plot the resulting observations in input space, colored by feasibility (neglecting the fact that these data points are at varying fidelities).</p>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#plot-path-through-input-space","title":"Plot path through input space\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#plot-the-acqusisition-function","title":"Plot the acqusisition function\u00b6","text":"<p>Here we plot the acquisition function at a small set of fidelities $[0, 0.5, 1.0]$.</p>"},{"location":"examples/rcds/rcds/","title":"RCDS","text":"In\u00a0[1]: Copied! <pre># If you encounter the \"Initializing libomp.dylib, but found libomp.dylib already initialized.\" error\n# Please run this cell\n\nimport os\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n</pre> # If you encounter the \"Initializing libomp.dylib, but found libomp.dylib already initialized.\" error # Please run this cell  import os  os.environ['KMP_DUPLICATE_LIB_OK']='True' In\u00a0[2]: Copied! <pre>import time\nimport numpy as np\nfrom xopt.generators.rcds.rcds import RCDSGenerator\nfrom xopt.vocs import VOCS\nfrom xopt.evaluator import Evaluator\nfrom xopt import Xopt\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> import time import numpy as np from xopt.generators.rcds.rcds import RCDSGenerator from xopt.vocs import VOCS from xopt.evaluator import Evaluator from xopt import Xopt import pandas as pd from tqdm.auto import tqdm import warnings warnings.filterwarnings(\"ignore\") In\u00a0[3]: Copied! <pre>def f_RCDS_minimize(input_dict):\n    p = []\n    for i in range(2):\n        p.append(input_dict[f'p{i}'])\n    \n    obj = np.linalg.norm(p)\n    outcome_dict = {'f': obj}\n    \n    return outcome_dict\n</pre> def f_RCDS_minimize(input_dict):     p = []     for i in range(2):         p.append(input_dict[f'p{i}'])          obj = np.linalg.norm(p)     outcome_dict = {'f': obj}          return outcome_dict In\u00a0[4]: Copied! <pre>YAML = \"\"\"\nmax_evaluations: 100\ngenerator:\n    name: rcds\n    x0: null\n    init_mat: null\n    noise: 0.00001\n    step: 0.01\n    tol: 0.00001\nevaluator:\n    function: __main__.f_RCDS_minimize\nvocs:\n    variables:\n        p0: [0, 1]\n        p1: [0, 1]\n    objectives:\n        f: MINIMIZE\n\"\"\"\n\nX = Xopt.from_yaml(YAML)\nX\n</pre> YAML = \"\"\" max_evaluations: 100 generator:     name: rcds     x0: null     init_mat: null     noise: 0.00001     step: 0.01     tol: 0.00001 evaluator:     function: __main__.f_RCDS_minimize vocs:     variables:         p0: [0, 1]         p1: [0, 1]     objectives:         f: MINIMIZE \"\"\"  X = Xopt.from_yaml(YAML) X Out[4]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: __main__.f_RCDS_minimize\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  init_mat: null\n  name: rcds\n  noise: 1.0e-05\n  step: 0.01\n  tol: 1.0e-05\n  x0: null\nmax_evaluations: 100\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints: {}\n  objectives:\n    f: MINIMIZE\n  observables: []\n  variables:\n    p0:\n    - 0.0\n    - 1.0\n    p1:\n    - 0.0\n    - 1.0\n</pre> In\u00a0[5]: Copied! <pre>X.run()\n</pre> X.run() <p>Now you can go directly to the Visualization section and check out the results.</p> In\u00a0[6]: Copied! <pre>n_var = 2\n</pre> n_var = 2 In\u00a0[7]: Copied! <pre>variables = {}\nfor i in range(n_var):\n    variables[f'p{i}'] = [0, 1]\n\nvocs = VOCS(\n    variables=variables,\n    objectives={'f': 'MINIMIZE'},\n)\n</pre> variables = {} for i in range(n_var):     variables[f'p{i}'] = [0, 1]  vocs = VOCS(     variables=variables,     objectives={'f': 'MINIMIZE'}, ) In\u00a0[8]: Copied! <pre>vocs\n</pre> vocs Out[8]: <pre>VOCS(variables={'p0': [0.0, 1.0], 'p1': [0.0, 1.0]}, constraints={}, objectives={'f': 'MINIMIZE'}, constants={}, observables=[])</pre> In\u00a0[9]: Copied! <pre>evaluator = Evaluator(function=f_RCDS_minimize)\n</pre> evaluator = Evaluator(function=f_RCDS_minimize) In\u00a0[10]: Copied! <pre>generator = RCDSGenerator(vocs=vocs)\n</pre> generator = RCDSGenerator(vocs=vocs) In\u00a0[11]: Copied! <pre>generator.model_dump()\n</pre> generator.model_dump() Out[11]: <pre>{'x0': None, 'init_mat': None, 'noise': 1e-05, 'step': 0.01, 'tol': 1e-05}</pre> In\u00a0[12]: Copied! <pre>X = Xopt.from_yaml(YAML)\n</pre> X = Xopt.from_yaml(YAML) In\u00a0[13]: Copied! <pre>for i in tqdm(range(X.max_evaluations)):\n    X.step()\n</pre>  for i in tqdm(range(X.max_evaluations)):     X.step() In\u00a0[14]: Copied! <pre>X.data.plot(y='f')\n</pre> X.data.plot(y='f') Out[14]: <pre>&lt;Axes: &gt;</pre> In\u00a0[14]: Copied! <pre>\n</pre>"},{"location":"examples/rcds/rcds/#rcds-optimization","title":"RCDS Optimization\u00b6","text":"<p>In this example we demonstrate RCDS optimization.</p>"},{"location":"examples/rcds/rcds/#rcds-test-problem","title":"RCDS test problem\u00b6","text":"<p>This test problem is a 2-D quadratic function.</p>"},{"location":"examples/rcds/rcds/#run-rcds-on-the-test-problem-yaml-method","title":"Run RCDS on the test problem (YAML method)\u00b6","text":""},{"location":"examples/rcds/rcds/#run-rcds-on-the-test-problem-api-method","title":"Run RCDS on the test problem (API method)\u00b6","text":""},{"location":"examples/rcds/rcds/#vocs","title":"VOCS\u00b6","text":"<p>We'll set the bounds for all the variables pi to [0, 1].</p>"},{"location":"examples/rcds/rcds/#evaluator","title":"Evaluator\u00b6","text":""},{"location":"examples/rcds/rcds/#generator","title":"Generator\u00b6","text":""},{"location":"examples/rcds/rcds/#run-the-optimization","title":"Run the optimization\u00b6","text":""},{"location":"examples/rcds/rcds/#visualization","title":"Visualization\u00b6","text":""},{"location":"examples/scipy/latin_hypercube/","title":"Latin Hypercube Generator Example","text":"In\u00a0[1]: Copied! <pre>from copy import deepcopy\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.scipy.latin_hypercube import LatinHypercubeGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> from copy import deepcopy from xopt import Xopt, Evaluator from xopt.generators.scipy.latin_hypercube import LatinHypercubeGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre># Create the test problem\nvocs = deepcopy(tnk_vocs)\nvocs.objectives = {}\nvocs.observables = [\"y1\"]\nevaluator = Evaluator(function=evaluate_TNK)\n\n# Create the generator and xopt object. Note: the samples are generated in\n# batches and the batch size determines the arrangement of points to cover\n# the bounded region of the variables.\ngenerator = LatinHypercubeGenerator(vocs=vocs, batch_size=1024)\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX\n</pre> # Create the test problem vocs = deepcopy(tnk_vocs) vocs.objectives = {} vocs.observables = [\"y1\"] evaluator = Evaluator(function=evaluate_TNK)  # Create the generator and xopt object. Note: the samples are generated in # batches and the batch size determines the arrangement of points to cover # the bounded region of the variables. generator = LatinHypercubeGenerator(vocs=vocs, batch_size=1024) X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X Out[2]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0\n    random_sleep: 0\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  batch_size: 1024\n  name: latin_hypercube\n  optimization: random-cd\n  scramble: true\n  seed: null\n  strength: 1\n  supports_batch_generation: true\n  supports_multi_objective: true\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives: {}\n  observables:\n  - y1\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[3]: Copied! <pre># Sample the function a number of times using latin hypercube points\nfor _ in range(1024):\n    X.step()\nX.data.head()\n</pre> # Sample the function a number of times using latin hypercube points for _ in range(1024):     X.step() X.data.head() Out[3]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 0 2.114304 0.358962 dummy_constant 2.114304 0.358962 3.689145 2.625869 0.000034 False 1 2.637505 0.458438 dummy_constant 2.637505 0.458438 6.259164 4.570656 0.000015 False 2 0.168917 1.768774 dummy_constant 0.168917 1.768774 2.152354 1.719404 0.000015 False 3 0.314415 2.133037 dummy_constant 0.314415 2.133037 3.718375 2.701253 0.000015 False 4 0.088559 1.575698 dummy_constant 0.088559 1.575698 1.428374 1.326410 0.000015 False In\u00a0[4]: Copied! <pre># Plot the data\nplt.scatter(X.data['x1'], X.data['x2'], c=X.data['y1'])\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.colorbar(label='y1')\n</pre> # Plot the data plt.scatter(X.data['x1'], X.data['x2'], c=X.data['y1']) plt.xlabel('x1') plt.ylabel('x2') plt.colorbar(label='y1') Out[4]: <pre>&lt;matplotlib.colorbar.Colorbar at 0x7f48e4197b20&gt;</pre> In\u00a0[5]: Copied! <pre>n = 16\ngenerator = LatinHypercubeGenerator(vocs=vocs, batch_size=n, scramble=False, seed=0)\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nfor _ in range(n):\n    X.step()\n    \nplt.scatter(X.data['x1'], X.data['x2'], c=X.data['y1'])\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.colorbar(label='y1')\n\nplt.hlines(np.linspace(0, 3.1416, n+1), 0, 3.1416, color='k')\nplt.vlines(np.linspace(0, 3.1416, n+1), 0, 3.1416, color='k')\n</pre> n = 16 generator = LatinHypercubeGenerator(vocs=vocs, batch_size=n, scramble=False, seed=0) X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) for _ in range(n):     X.step()      plt.scatter(X.data['x1'], X.data['x2'], c=X.data['y1']) plt.xlabel('x1') plt.ylabel('x2') plt.colorbar(label='y1')  plt.hlines(np.linspace(0, 3.1416, n+1), 0, 3.1416, color='k') plt.vlines(np.linspace(0, 3.1416, n+1), 0, 3.1416, color='k') Out[5]: <pre>&lt;matplotlib.collections.LineCollection at 0x7f494f230f70&gt;</pre> <p>The scramble feature will randomize the location of the points within each square while still maintaining the latin hypercube style cells.</p> In\u00a0[6]: Copied! <pre>n = 16\ngenerator = LatinHypercubeGenerator(vocs=vocs, batch_size=n, scramble=True, seed=0)\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nfor _ in range(n):\n    X.step()\n    \nplt.scatter(X.data['x1'], X.data['x2'], c=X.data['y1'])\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.colorbar(label='y1')\n\nplt.hlines(np.linspace(0, 3.1416, n+1), 0, 3.1416, color='k')\nplt.vlines(np.linspace(0, 3.1416, n+1), 0, 3.1416, color='k')\n</pre> n = 16 generator = LatinHypercubeGenerator(vocs=vocs, batch_size=n, scramble=True, seed=0) X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) for _ in range(n):     X.step()      plt.scatter(X.data['x1'], X.data['x2'], c=X.data['y1']) plt.xlabel('x1') plt.ylabel('x2') plt.colorbar(label='y1')  plt.hlines(np.linspace(0, 3.1416, n+1), 0, 3.1416, color='k') plt.vlines(np.linspace(0, 3.1416, n+1), 0, 3.1416, color='k') Out[6]: <pre>&lt;matplotlib.collections.LineCollection at 0x7f494f230fa0&gt;</pre>"},{"location":"examples/scipy/latin_hypercube/#latin-hypercube-generator-example","title":"Latin Hypercube Generator Example\u00b6","text":"<p>This notebook demonstrates basic use of the latin hypercube generator. This generator is a wrapper for the scipy latin hypercube method and allows users to efficiently sample functions (eg for surrogate models). Because the distribution of points depends on the number of sample requested, internally the xopt routine stores a batch of samples. The batch size is specified as an argument to the object's constructor. All other parameters to the scipy function are broken out this way and for a detailed explanation of what they do, the scipy documentation should be consulted.</p>"},{"location":"examples/scipy/latin_hypercube/#distribution-of-latin-hypercube-points","title":"Distribution of Latin Hypercube points\u00b6","text":"<p>Points in latin hypercube sampling are arranged in a grid such that none occupy the same row or column. That is, in chess it is similar to having n rooks on the board which cannot take each other. We can demonstrate this in the sampler by turning off the \"scramble\" feature (turned on by default).</p>"},{"location":"examples/scipy/neldermead/","title":"Nelder-Mead Generator adapted from SciPy","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\n\nfrom xopt import Xopt\nimport numpy as np\n\n#from xopt import output_notebook\n#output_notebook()\n\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd  from xopt import Xopt import numpy as np  #from xopt import output_notebook #output_notebook()  import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre>YAML = \"\"\"\ngenerator:\n  name: neldermead\n  initial_point: {x0: -1, x1: -1}\n  adaptive: true\n  xatol: 0.0001\n  fatol: 0.0001  \nevaluator:\n  function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock\nvocs:\n  variables:\n    x0: [-5, 5]\n    x1: [-5, 5]\n  objectives: {y: MINIMIZE}\n\"\"\"\nX = Xopt.from_yaml(YAML)\n</pre> YAML = \"\"\" generator:   name: neldermead   initial_point: {x0: -1, x1: -1}   adaptive: true   xatol: 0.0001   fatol: 0.0001   evaluator:   function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock vocs:   variables:     x0: [-5, 5]     x1: [-5, 5]   objectives: {y: MINIMIZE} \"\"\" X = Xopt.from_yaml(YAML) In\u00a0[3]: Copied! <pre>XMIN = [1,1] # True minimum\n</pre> XMIN = [1,1] # True minimum In\u00a0[4]: Copied! <pre>X.run()\nX.data\n</pre> X.run() X.data Out[4]: x0 x1 y xopt_runtime xopt_error 0 -1.000000 -1.000000 4.040000e+02 0.000011 False 1 -1.050000 -1.000000 4.462531e+02 0.000007 False 2 -1.000000 -1.050000 4.242500e+02 0.000006 False 3 -0.950000 -1.050000 3.850281e+02 0.000007 False 4 -0.900000 -1.075000 3.589325e+02 0.000006 False ... ... ... ... ... ... 120 0.999935 0.999867 5.114951e-09 0.000006 False 121 0.999877 0.999764 2.587916e-08 0.000006 False 122 0.999999 0.999995 5.309344e-10 0.000006 False 123 1.000045 1.000097 7.751675e-09 0.000006 False 124 0.999963 0.999925 1.412126e-09 0.000006 False <p>125 rows \u00d7 5 columns</p> In\u00a0[5]: Copied! <pre># Evaluation progression\nX.data['y'].plot(marker='.')\nplt.yscale('log')\nplt.xlabel('iteration')\nplt.ylabel('Rosenbrock value')\n</pre> # Evaluation progression X.data['y'].plot(marker='.') plt.yscale('log') plt.xlabel('iteration') plt.ylabel('Rosenbrock value') Out[5]: <pre>Text(0, 0.5, 'Rosenbrock value')</pre> In\u00a0[6]: Copied! <pre># Minimum\ndict(X.data.iloc[X.data[\"y\"].argmin()])\n</pre> # Minimum dict(X.data.iloc[X.data[\"y\"].argmin()]) Out[6]: <pre>{'x0': 0.9999988592114838,\n 'x1': 0.9999954170486077,\n 'y': 5.309343918637161e-10,\n 'xopt_runtime': 5.850999968970427e-06,\n 'xopt_error': False}</pre> In\u00a0[7]: Copied! <pre>from xopt.resources.test_functions.rosenbrock import rosenbrock\n</pre> from xopt.resources.test_functions.rosenbrock import rosenbrock In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots(figsize=(8,8))\n\nXgrid, Ygrid = np.meshgrid(np.linspace(-2, 2, 201), np.linspace(-2, 2, 201) )\n\nZgrid = np.vectorize(lambda x, y: rosenbrock([x, y]))(Xgrid, Ygrid)\nZgrid = np.log(Zgrid+1)\n\nax.pcolormesh(Xgrid, Ygrid, Zgrid)\nax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black')\nax.set_xlabel('x0')\nax.set_ylabel('x1')\n\n\n# Add all evaluations\nax.plot(X.data[\"x0\"], X.data[\"x1\"], color='red', alpha=0.5, marker='.')\nax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\")\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\n#plt.legend()\nax.set_title(\"Xopt's Nelder-Mead progression\")\n</pre> fig, ax = plt.subplots(figsize=(8,8))  Xgrid, Ygrid = np.meshgrid(np.linspace(-2, 2, 201), np.linspace(-2, 2, 201) )  Zgrid = np.vectorize(lambda x, y: rosenbrock([x, y]))(Xgrid, Ygrid) Zgrid = np.log(Zgrid+1)  ax.pcolormesh(Xgrid, Ygrid, Zgrid) ax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black') ax.set_xlabel('x0') ax.set_ylabel('x1')   # Add all evaluations ax.plot(X.data[\"x0\"], X.data[\"x1\"], color='red', alpha=0.5, marker='.') ax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\") ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) #plt.legend() ax.set_title(\"Xopt's Nelder-Mead progression\") Out[8]: <pre>Text(0.5, 1.0, \"Xopt's Nelder-Mead progression\")</pre> In\u00a0[9]: Copied! <pre># Manually step the algorithm and collect simplexes\nX = Xopt.from_yaml(YAML)\nsimplexes = []\nwhile not X.generator.is_done:\n    X.step()\n    simplexes.append(X.generator.simplex)\n</pre> # Manually step the algorithm and collect simplexes X = Xopt.from_yaml(YAML) simplexes = [] while not X.generator.is_done:     X.step()     simplexes.append(X.generator.simplex)  In\u00a0[10]: Copied! <pre>def plot_simplex(simplex, ax=None):\n    x0 = simplex[\"x0\"]\n    x1 = simplex[\"x1\"]\n    x0 = np.append(x0, x0[0])\n    x1 = np.append(x1, x1[0])\n    ax.plot(x0, x1)\n\nfig, ax = plt.subplots(figsize=(8,8))\nax.pcolormesh(Xgrid, Ygrid, Zgrid)\n#ax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black')\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)  \nax.set_xlabel('x0')\nax.set_ylabel('x1')\nax.set_title('Nelder-Mead simplex progression')\n\nax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\")\n\nfor simplex in simplexes:\n    plot_simplex(simplex, ax)\n</pre> def plot_simplex(simplex, ax=None):     x0 = simplex[\"x0\"]     x1 = simplex[\"x1\"]     x0 = np.append(x0, x0[0])     x1 = np.append(x1, x1[0])     ax.plot(x0, x1)  fig, ax = plt.subplots(figsize=(8,8)) ax.pcolormesh(Xgrid, Ygrid, Zgrid) #ax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black') ax.set_xlim(-2, 2) ax.set_ylim(-2, 2)   ax.set_xlabel('x0') ax.set_ylabel('x1') ax.set_title('Nelder-Mead simplex progression')  ax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\")  for simplex in simplexes:     plot_simplex(simplex, ax) In\u00a0[11]: Copied! <pre>from scipy.optimize import fmin\n</pre> from scipy.optimize import fmin In\u00a0[12]: Copied! <pre>result = fmin(rosenbrock, [-1, -1])\nresult\n</pre> result = fmin(rosenbrock, [-1, -1]) result <pre>Optimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 67\n         Function evaluations: 125\n</pre> Out[12]: <pre>array([0.99999886, 0.99999542])</pre> In\u00a0[13]: Copied! <pre>X = Xopt.from_yaml(YAML)\n</pre> X = Xopt.from_yaml(YAML) In\u00a0[14]: Copied! <pre>X.run()\n# Almost exactly the same number evaluations. \nlen(X.data)\n</pre> X.run() # Almost exactly the same number evaluations.  len(X.data)  Out[14]: <pre>125</pre> In\u00a0[15]: Copied! <pre># results are the same\nxbest = X.data.iloc[X.data[\"y\"].argmin()]\nxbest['x0'] == result[0], xbest['x1'] == result[1]\n</pre> # results are the same xbest = X.data.iloc[X.data[\"y\"].argmin()] xbest['x0'] == result[0], xbest['x1'] == result[1] Out[15]: <pre>(True, True)</pre> In\u00a0[16]: Copied! <pre>from xopt.generators.scipy.neldermead import NelderMeadGenerator\nfrom xopt import Evaluator, VOCS\n</pre> from xopt.generators.scipy.neldermead import NelderMeadGenerator from xopt import Evaluator, VOCS In\u00a0[17]: Copied! <pre>NelderMeadGenerator.model_fields\n</pre> NelderMeadGenerator.model_fields Out[17]: <pre>{'supports_batch_generation': FieldInfo(annotation=bool, required=False, default=False, description='flag that describes if this generator can generate batches of points', exclude=True, frozen=True),\n 'supports_multi_objective': FieldInfo(annotation=bool, required=False, default=False, description='flag that describes if this generator can solve multi-objective problems', exclude=True, frozen=True),\n 'vocs': FieldInfo(annotation=VOCS, required=True, description='generator VOCS', exclude=True),\n 'data': FieldInfo(annotation=Union[DataFrame, NoneType], required=False, description='generator data', exclude=True),\n 'initial_point': FieldInfo(annotation=Union[Dict[str, float], NoneType], required=False),\n 'initial_simplex': FieldInfo(annotation=Union[Dict[str, Union[List[float], numpy.ndarray]], NoneType], required=False),\n 'adaptive': FieldInfo(annotation=bool, required=False, default=True, description='Change hyperparameters based on dimensionality'),\n 'xatol': FieldInfo(annotation=float, required=False, default=0.0001, description='Tolerance in x value'),\n 'fatol': FieldInfo(annotation=float, required=False, default=0.0001, description='Tolerance in function value'),\n 'current_state': FieldInfo(annotation=SimplexState, required=False, default=SimplexState(astg=-1, N=None, kend=0, jend=0, ind=None, sim=None, fsim=None, fxr=None, x=None, xr=None, xe=None, xc=None, xcc=None, xbar=None, doshrink=0, ngen=0)),\n 'future_state': FieldInfo(annotation=Union[SimplexState, NoneType], required=False),\n 'x': FieldInfo(annotation=Union[ndarray, NoneType], required=False),\n 'y': FieldInfo(annotation=Union[float, NoneType], required=False),\n 'is_done_bool': FieldInfo(annotation=bool, required=False, default=False)}</pre> In\u00a0[18]: Copied! <pre>Xbest = [33, 44]\n\ndef f(inputs, verbose=False):\n\n    if verbose:\n        print(f'evaluate f({inputs})')\n    x0 = inputs[\"x0\"]\n    x1 = inputs[\"x1\"]\n    \n    #if x0 &lt; 10:\n    #    raise ValueError('test XXXX')\n\n    y = (x0-Xbest[0])**2  + (x1-Xbest[1])**2\n\n    return {\"y\":y}\n\nev = Evaluator(function=f)\nvocs = VOCS(variables={\"x0\": [-100, 100], \"x1\": [-100,100]}, objectives={\"y\":\"MINIMIZE\"})\nvocs.json()\n</pre> Xbest = [33, 44]  def f(inputs, verbose=False):      if verbose:         print(f'evaluate f({inputs})')     x0 = inputs[\"x0\"]     x1 = inputs[\"x1\"]          #if x0 &lt; 10:     #    raise ValueError('test XXXX')      y = (x0-Xbest[0])**2  + (x1-Xbest[1])**2      return {\"y\":y}  ev = Evaluator(function=f) vocs = VOCS(variables={\"x0\": [-100, 100], \"x1\": [-100,100]}, objectives={\"y\":\"MINIMIZE\"}) vocs.json() Out[18]: <pre>'{\"variables\":{\"x0\":[-100.0,100.0],\"x1\":[-100.0,100.0]},\"constraints\":{},\"objectives\":{\"y\":\"MINIMIZE\"},\"constants\":{},\"observables\":[]}'</pre> In\u00a0[19]: Copied! <pre># check output\nf(vocs.random_inputs()[0])\n</pre> # check output f(vocs.random_inputs()[0]) Out[19]: <pre>{'y': array([1449.4886573])}</pre> In\u00a0[20]: Copied! <pre>G = NelderMeadGenerator(vocs=vocs)\ninputs = G.generate(1)\ninputs\n</pre> G = NelderMeadGenerator(vocs=vocs) inputs = G.generate(1) inputs Out[20]: <pre>[{'x0': -67.8997603166419, 'x1': -17.46602906758909}]</pre> In\u00a0[21]: Copied! <pre># Further generate calls will continue to produce same point, as with BO\nG.generate(1)\n</pre> # Further generate calls will continue to produce same point, as with BO G.generate(1) Out[21]: <pre>[{'x0': -67.8997603166419, 'x1': -17.46602906758909}]</pre> In\u00a0[22]: Copied! <pre>ev.evaluate(inputs[0])\n</pre> ev.evaluate(inputs[0]) Out[22]: <pre>{'y': 13958.834361293491,\n 'xopt_runtime': 3.325999955450243e-06,\n 'xopt_error': False}</pre> In\u00a0[23]: Copied! <pre># Adding new data will advance state to next step, and next generate() will yield new point\nG.add_data(pd.DataFrame([ev.evaluate(inputs[0])]))\nG.generate(1)\n</pre> # Adding new data will advance state to next step, and next generate() will yield new point G.add_data(pd.DataFrame([ev.evaluate(inputs[0])])) G.generate(1) Out[23]: <pre>[{'x0': -71.294748332474, 'x1': -17.46602906758909}]</pre> In\u00a0[24]: Copied! <pre># Create Xopt object\nX = Xopt(evaluator=ev, vocs=vocs, generator=NelderMeadGenerator(vocs=vocs))\n\n# Optional: give an initial pioint\nX.generator.initial_point = {'x0':0, 'x1':0}\n</pre> # Create Xopt object X = Xopt(evaluator=ev, vocs=vocs, generator=NelderMeadGenerator(vocs=vocs))  # Optional: give an initial pioint X.generator.initial_point = {'x0':0, 'x1':0} In\u00a0[25]: Copied! <pre>X.run()\n</pre> X.run() In\u00a0[26]: Copied! <pre># Generator is done and cannot be resumed\nX.generator.is_done\n</pre> # Generator is done and cannot be resumed X.generator.is_done Out[26]: <pre>True</pre> In\u00a0[27]: Copied! <pre># Generate calls will just return nothing\nX.generator.generate(1) is None\n</pre> # Generate calls will just return nothing X.generator.generate(1) is None Out[27]: <pre>True</pre> In\u00a0[28]: Copied! <pre># This shows the latest simplex\nX.generator.simplex\n</pre> # This shows the latest simplex X.generator.simplex Out[28]: <pre>{'x0': array([32.99996111, 32.99996171, 33.00002688]),\n 'x1': array([44.00000851, 44.00006811, 44.00003045])}</pre> In\u00a0[29]: Copied! <pre>X.data['y'].plot()\nplt.yscale('log')\n</pre> X.data['y'].plot() plt.yscale('log') In\u00a0[30]: Copied! <pre>fig, ax = plt.subplots()\nX.data.plot('x0', 'x1', ax=ax, color='black', alpha=0.5)\nax.scatter(Xbest[0], Xbest[1], marker='x', color='red')\n</pre> fig, ax = plt.subplots() X.data.plot('x0', 'x1', ax=ax, color='black', alpha=0.5) ax.scatter(Xbest[0], Xbest[1], marker='x', color='red')     Out[30]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f91c9be1130&gt;</pre> In\u00a0[31]: Copied! <pre># This is the raw internal state of the generator\na = X.generator.current_state\na\n</pre> # This is the raw internal state of the generator a = X.generator.current_state a Out[31]: <pre>SimplexState(astg=4, N=2, kend=3, jend=0, ind=array([2., 0., 1.]), sim=array([[32.99996111, 44.00000851],\n       [32.99996171, 44.00006811],\n       [33.00002688, 44.00003045]]), fsim=array([1.58463795e-09, 6.10453387e-09, 1.64942323e-09]), fxr=3.1653753661675e-08, x=array(nan), xr=array([32.99983049, 44.00005403]), xe=array([30.09830487, 46.28401515]), xc=array([32.99992966, 44.00013909]), xcc=array([33.00002688, 44.00003045]), xbar=array([32.99996141, 44.00003831]), doshrink=0, ngen=176)</pre> In\u00a0[32]: Copied! <pre># Check JSON representation of options\nX.generator.json()\n</pre> # Check JSON representation of options X.generator.json() Out[32]: <pre>'{\"initial_point\":{\"x0\":0.0,\"x1\":0.0},\"initial_simplex\":null,\"adaptive\":true,\"xatol\":0.0001,\"fatol\":0.0001,\"current_state\":{\"astg\":4,\"N\":2,\"kend\":3,\"jend\":0,\"ind\":[2.0,0.0,1.0],\"sim\":[[32.99996111240644,44.000008508408456],[32.99996171260399,44.0000681073357],[33.00002687564558,44.00003044869291]],\"fsim\":[1.5846379474430505e-9,6.104533869326014e-9,1.6494232253860804e-9],\"fxr\":3.1653753661675e-8,\"x\":null,\"xr\":[32.99983048622448,44.0000540262304],\"xe\":[30.098304873214147,46.28401515040761],\"xc\":[32.99992965625605,44.00013908571843],\"xcc\":[33.00002687564558,44.00003044869291],\"xbar\":[32.999961412505215,44.000038307872074],\"doshrink\":0,\"ngen\":176},\"future_state\":null,\"x\":null,\"y\":null,\"is_done_bool\":true}'</pre> In\u00a0[33]: Copied! <pre># Set the initial simplex to be the latest\nX2 = Xopt(evaluator=ev, vocs=vocs, generator=NelderMeadGenerator(vocs=vocs, initial_simplex=X.generator.simplex))\nX2.generator.xatol = 1e-9\nX2.generator.fatol = 1e-9\nX2.run()\n\nX2.data['y'].plot()\nplt.yscale('log')\n</pre> # Set the initial simplex to be the latest X2 = Xopt(evaluator=ev, vocs=vocs, generator=NelderMeadGenerator(vocs=vocs, initial_simplex=X.generator.simplex)) X2.generator.xatol = 1e-9 X2.generator.fatol = 1e-9 X2.run()  X2.data['y'].plot() plt.yscale('log') In\u00a0[34]: Copied! <pre>YAML = \"\"\"\ngenerator:\n  name: neldermead\nevaluator:\n  function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock\nvocs:\n  variables:\n    x1: [-5, 5]\n    x2: [-5, 5]\n    x3: [-5, 5]\n    x4: [-5, 5]\n    x5: [-5, 5]\n  objectives:\n    y: MINIMIZE\n\"\"\"\nX = Xopt.from_yaml(YAML)\n</pre> YAML = \"\"\" generator:   name: neldermead evaluator:   function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock vocs:   variables:     x1: [-5, 5]     x2: [-5, 5]     x3: [-5, 5]     x4: [-5, 5]     x5: [-5, 5]   objectives:     y: MINIMIZE \"\"\" X = Xopt.from_yaml(YAML) In\u00a0[35]: Copied! <pre>X.run()\nX.data['y'].plot()\nplt.yscale('log')\n</pre> X.run() X.data['y'].plot() plt.yscale('log') In\u00a0[36]: Copied! <pre>fig, ax = plt.subplots(figsize=(8,8))\n\nXgrid, Ygrid = np.meshgrid(np.linspace(-2, 2, 201), np.linspace(-2, 2, 201) )\n\nZgrid = np.vectorize(lambda x, y: rosenbrock([x, y, 1, 1, 1]))(Xgrid, Ygrid)  # The minimum is at 1,1,1,1,1\nZgrid = np.log(Zgrid+1)\n\nax.pcolormesh(Xgrid, Ygrid, Zgrid)\nax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black')\nax.set_xlabel('x0')\nax.set_ylabel('x1')\n\n\n# Add all evaluations\nax.plot(X.data[\"x1\"], X.data[\"x2\"], color='red', alpha=0.5, marker='.')\nax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\")\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\n#plt.legend()\nax.set_title(\"Xopt's Nelder-Mead progression\")\n</pre> fig, ax = plt.subplots(figsize=(8,8))  Xgrid, Ygrid = np.meshgrid(np.linspace(-2, 2, 201), np.linspace(-2, 2, 201) )  Zgrid = np.vectorize(lambda x, y: rosenbrock([x, y, 1, 1, 1]))(Xgrid, Ygrid)  # The minimum is at 1,1,1,1,1 Zgrid = np.log(Zgrid+1)  ax.pcolormesh(Xgrid, Ygrid, Zgrid) ax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black') ax.set_xlabel('x0') ax.set_ylabel('x1')   # Add all evaluations ax.plot(X.data[\"x1\"], X.data[\"x2\"], color='red', alpha=0.5, marker='.') ax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\") ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) #plt.legend() ax.set_title(\"Xopt's Nelder-Mead progression\") Out[36]: <pre>Text(0.5, 1.0, \"Xopt's Nelder-Mead progression\")</pre> In\u00a0[235]: Copied! <pre>\n</pre>"},{"location":"examples/scipy/neldermead/#nelder-mead-generator-adapted-from-scipy","title":"Nelder-Mead Generator adapted from SciPy\u00b6","text":"<p>Most of the algorithms in scipy.optimize are self-contained functions that operate on the user-provided <code>func</code>. Xopt has adapted the Nelder-Mead directly from scipy.optimize to be in a generator form. This allows for the manual stepping through the algorithm.</p>"},{"location":"examples/scipy/neldermead/#nelder-mead-optimization-of-the-rosenbrock-function-with-xopt","title":"Nelder-Mead optimization of the Rosenbrock function with Xopt\u00b6","text":""},{"location":"examples/scipy/neldermead/#visualize","title":"Visualize\u00b6","text":""},{"location":"examples/scipy/neldermead/#compare-with-scipyoptimizefmin-nelder-mead","title":"Compare with scipy.optimize.fmin Nelder-Mead\u00b6","text":"<p>Notice that fmin is much faster here. This is because the function runs very fast, so the internal Xopt bookkeeping overhead dominates.</p>"},{"location":"examples/scipy/neldermead/#neldermeadgenerator-object","title":"NelderMeadGenerator object\u00b6","text":""},{"location":"examples/scipy/neldermead/#5-dimensional-rosenbrock","title":"5-dimensional Rosenbrock\u00b6","text":"<p><code>evaluate_rosenbrock</code> works for arbitrary dimensions, so adding more variables to <code>vocs</code> transforms this problem.</p>"},{"location":"examples/single_objective_bayes_opt/bax_tutorial/","title":"Basic Optimization using BAX","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n\nimport torch\n\nfrom xopt import Xopt\nfrom xopt.vocs import VOCS\nfrom xopt.generators.bayesian.bax_generator import BaxGenerator\n\nfrom xopt.evaluator import Evaluator\n\nimport numpy as np\nimport random\n\n\n#random seeds for reproducibility \nrand_seed = 2\n\ntorch.manual_seed(rand_seed)\nnp.random.seed(rand_seed) #only affects initial random observations through Xopt\nrandom.seed(rand_seed)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import os os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'  import torch  from xopt import Xopt from xopt.vocs import VOCS from xopt.generators.bayesian.bax_generator import BaxGenerator  from xopt.evaluator import Evaluator  import numpy as np import random   #random seeds for reproducibility  rand_seed = 2  torch.manual_seed(rand_seed) np.random.seed(rand_seed) #only affects initial random observations through Xopt random.seed(rand_seed) In\u00a0[2]: Copied! <pre>import math\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    observables=[\"y1\"],\n)\n</pre> import math  # define variables and function objectives vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     observables=[\"y1\"], ) In\u00a0[3]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\ndef sin_function(input_dict):\n    return {\"y1\": np.sin(input_dict[\"x\"])}\n</pre> # define a test function to optimize import numpy as np  def sin_function(input_dict):     return {\"y1\": np.sin(input_dict[\"x\"])} In\u00a0[4]: Copied! <pre>from xopt.generators.bayesian.bax.algorithms import GridMinimize\n\n#Prepare BAX algorithm and generator options\nalgorithm = GridMinimize(n_mesh_points=50)\n\n#construct BAX generator\ngenerator = BaxGenerator(vocs=vocs, algorithm=algorithm)\n</pre> from xopt.generators.bayesian.bax.algorithms import GridMinimize  #Prepare BAX algorithm and generator options algorithm = GridMinimize(n_mesh_points=50)  #construct BAX generator generator = BaxGenerator(vocs=vocs, algorithm=algorithm) In\u00a0[5]: Copied! <pre>#construct evaluator\nevaluator = Evaluator(function=sin_function)\n\n#construct Xopt optimizer\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> #construct evaluator evaluator = Evaluator(function=sin_function)  #construct Xopt optimizer X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[6]: Copied! <pre># evaluate initial points\nX.random_evaluate(3)\n\n# inspect the gathered data\nX.data\n</pre> # evaluate initial points X.random_evaluate(3)  # inspect the gathered data X.data Out[6]: x y1 xopt_runtime xopt_error 0 3.543749 -0.391403 0.000017 False 1 6.120286 -0.162180 0.000002 False 2 2.829554 0.306999 0.000001 False In\u00a0[7]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\nn_steps = 3\n\n# test points for plotting\ntest_x = torch.linspace(*X.vocs.bounds.flatten(),50).double()\n\nfor i in range(5):\n    # get the Gaussian process model from the generator\n    model = X.generator.train_model()\n\n    # get acquisition function from generator\n    acq = X.generator.get_acquisition(model)\n\n    # calculate model posterior and acquisition function at each test point\n    # NOTE: need to add a dimension to the input tensor for evaluating the\n    # posterior and another for the acquisition function, see\n    # https://botorch.org/docs/batching for details\n    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n    # skipping calculations for backpropagation\n    with torch.no_grad():\n        posterior = model.posterior(test_x.unsqueeze(1))\n        acq_val = acq(test_x.reshape(-1,1,1))\n\n    # get mean function and confidence regions\n    mean = posterior.mean\n    l,u = posterior.mvn.confidence_region()\n\n    # plot model and acquisition function\n    fig,ax = plt.subplots(3,1,sharex=\"all\")\n    fig.set_size_inches(8,6)\n\n    # plot model posterior\n    ax[0].plot(test_x, mean, label=\"Posterior mean\")\n    ax[0].fill_between(test_x, l, u,alpha=0.25, label=\"Posterior confidence region\")\n\n    # add data to model plot\n    ax[0].plot(X.data[\"x\"],X.data[\"y1\"],\"C1o\", label=\"Training data\")\n\n    # plot true function\n    true_f = sin_function({\"x\": test_x})[\"y1\"]\n    ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")\n\n    if i == 0:\n        ax[0].legend(ncols=2)\n\n    # plot the function samples and their optima found by BAX\n    test_points = X.generator.algorithm_results[\"test_points\"]\n    posterior_samples = X.generator.algorithm_results[\"posterior_samples\"]\n    execution_paths = X.generator.algorithm_results[\"execution_paths\"]\n\n    label1 = 'Function Samples'\n    label2 = 'Sample Optima'\n    for i in range(X.generator.algorithm.n_samples):\n        samples, = ax[1].plot(test_points, posterior_samples[i], c='C0', alpha=0.3,\n                              label=label1)\n        optima = ax[1].scatter(*execution_paths[i], c='r', marker='x', s=80,\n                               label=label2,zorder=10)\n        label1 = None\n        label2 = None \n    \n    # add legend\n    if i == 0:\n        ax[1].legend()\n\n    # plot acquisition function\n    ax[2].plot(test_x, acq_val.flatten())\n\n    ax[0].set_ylabel(\"y1\")\n    ax[1].set_ylabel(\"y1\")\n    ax[2].set_ylabel(r\"$\\alpha(x)$\")\n    ax[2].set_xlabel(\"x\")\n\n    # do the optimization step\n    X.step()\n</pre> import torch import matplotlib.pyplot as plt n_steps = 3  # test points for plotting test_x = torch.linspace(*X.vocs.bounds.flatten(),50).double()  for i in range(5):     # get the Gaussian process model from the generator     model = X.generator.train_model()      # get acquisition function from generator     acq = X.generator.get_acquisition(model)      # calculate model posterior and acquisition function at each test point     # NOTE: need to add a dimension to the input tensor for evaluating the     # posterior and another for the acquisition function, see     # https://botorch.org/docs/batching for details     # NOTE: we use the `torch.no_grad()` environment to speed up computation by     # skipping calculations for backpropagation     with torch.no_grad():         posterior = model.posterior(test_x.unsqueeze(1))         acq_val = acq(test_x.reshape(-1,1,1))      # get mean function and confidence regions     mean = posterior.mean     l,u = posterior.mvn.confidence_region()      # plot model and acquisition function     fig,ax = plt.subplots(3,1,sharex=\"all\")     fig.set_size_inches(8,6)      # plot model posterior     ax[0].plot(test_x, mean, label=\"Posterior mean\")     ax[0].fill_between(test_x, l, u,alpha=0.25, label=\"Posterior confidence region\")      # add data to model plot     ax[0].plot(X.data[\"x\"],X.data[\"y1\"],\"C1o\", label=\"Training data\")      # plot true function     true_f = sin_function({\"x\": test_x})[\"y1\"]     ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")      if i == 0:         ax[0].legend(ncols=2)      # plot the function samples and their optima found by BAX     test_points = X.generator.algorithm_results[\"test_points\"]     posterior_samples = X.generator.algorithm_results[\"posterior_samples\"]     execution_paths = X.generator.algorithm_results[\"execution_paths\"]      label1 = 'Function Samples'     label2 = 'Sample Optima'     for i in range(X.generator.algorithm.n_samples):         samples, = ax[1].plot(test_points, posterior_samples[i], c='C0', alpha=0.3,                               label=label1)         optima = ax[1].scatter(*execution_paths[i], c='r', marker='x', s=80,                                label=label2,zorder=10)         label1 = None         label2 = None           # add legend     if i == 0:         ax[1].legend()      # plot acquisition function     ax[2].plot(test_x, acq_val.flatten())      ax[0].set_ylabel(\"y1\")     ax[1].set_ylabel(\"y1\")     ax[2].set_ylabel(r\"$\\alpha(x)$\")     ax[2].set_xlabel(\"x\")      # do the optimization step     X.step()  In\u00a0[8]: Copied! <pre># access the collected data\nX.data\n</pre> # access the collected data X.data Out[8]: x y1 xopt_runtime xopt_error 0 3.543749 -0.391403 0.000017 False 1 6.120286 -0.162180 0.000002 False 2 2.829554 0.306999 0.000001 False 3 4.103268 -0.820151 0.000007 False 4 4.367973 -0.941273 0.000007 False 5 4.710628 -0.999998 0.000007 False 6 4.762932 -0.998723 0.000007 False 7 4.741298 -0.999582 0.000007 False In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/bax_tutorial/#basic-optimization-using-bax","title":"Basic Optimization using BAX\u00b6","text":"<p>In this notebook we demonstrate the use of Xopt to perform Bayesian Algorithm Execution (BAX) as a means of minimizing the output of a simple test function. BAX is a generalization of Bayesian Optimization that seeks to acquire observations that provide our model with maximal information about our property of interest. In this simple example, our property of interest is the minimum function output and its location in input-space. See https://arxiv.org/pdf/2209.04587.pdf for details.</p>"},{"location":"examples/single_objective_bayes_opt/bax_tutorial/#imports-and-random-seeding-for-reproducibility","title":"Imports and random seeding for reproducibility\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/bax_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize the sin function in the domian [0,2*pi]. Note that the function used to evaluate the objective function takes a dictionary as input and returns a dictionary as the output.</p>"},{"location":"examples/single_objective_bayes_opt/bax_tutorial/#prepare-bax-generator-for-xopt","title":"Prepare BAX generator for Xopt\u00b6","text":"<p>Create a generator that uses the ExpectedInformationGain (InfoBAX) acquisition function to perform Bayesian Optimization. Note that we use minimization on a grid, so specifying the number of mesh points can negatively impact decision making time (especially in higher dimensional feature spaces).</p>"},{"location":"examples/single_objective_bayes_opt/bax_tutorial/#create-evaluator-and-xopt-objects","title":"Create Evaluator and Xopt objects\u00b6","text":"<p>Create the Evaluator (which allows Xopt to interface with our test function) and finish constructing our Xopt object.</p>"},{"location":"examples/single_objective_bayes_opt/bax_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/bax_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>To perform optimization we simply call <code>X.step()</code> in a loop. This allows us to do intermediate tasks in between optimization steps, such as examining the model and acquisition function at each step (as we demonstrate here).</p>"},{"location":"examples/single_objective_bayes_opt/benchmarking/","title":"Normal Model with Standard transforms and no constraints","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom copy import deepcopy\nfrom xopt.generators.bayesian import ExpectedImprovementGenerator\nfrom xopt.vocs import VOCS\n\nvocs = VOCS(\n    variables = {\"x\":[0,1]},\n    objectives = {\"y\":\"MAXIMIZE\"},\n    constraints = {\"c\": [\"LESS_THAN\", 0]}\n)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import matplotlib.pyplot as plt import pandas as pd import torch from copy import deepcopy from xopt.generators.bayesian import ExpectedImprovementGenerator from xopt.vocs import VOCS  vocs = VOCS(     variables = {\"x\":[0,1]},     objectives = {\"y\":\"MAXIMIZE\"},     constraints = {\"c\": [\"LESS_THAN\", 0]} ) In\u00a0[2]: Copied! <pre># define test functions\ndef y(x):\n    return torch.sin(2*3.14*x)\n\ndef c(x):\n    return 10.0*torch.cos(2*3.14*x + 0.25)\n\ntest_x = torch.linspace(*torch.tensor(vocs.bounds.flatten()), 100)\n\n# define training data to pass to the generator\ntrain_x = torch.tensor((0.2,0.5, 0.6))\ntrain_y = y(train_x)\ntrain_c = c(train_x)\n\ndata = pd.DataFrame(\n    {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c}\n)\n\ndef plot_ground_truth():\n    fig,ax = plt.subplots()\n    ax.plot(test_x, y(test_x),'--C0')\n    ax.plot(test_x, c(test_x),'--C1')\n    ax.plot(train_x, train_y,'oC0')\n    ax.plot(train_x, train_c,'oC1')\n\n    return ax\nplot_ground_truth();\n</pre> # define test functions def y(x):     return torch.sin(2*3.14*x)  def c(x):     return 10.0*torch.cos(2*3.14*x + 0.25)  test_x = torch.linspace(*torch.tensor(vocs.bounds.flatten()), 100)  # define training data to pass to the generator train_x = torch.tensor((0.2,0.5, 0.6)) train_y = y(train_x) train_c = c(train_x)  data = pd.DataFrame(     {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c} )  def plot_ground_truth():     fig,ax = plt.subplots()     ax.plot(test_x, y(test_x),'--C0')     ax.plot(test_x, c(test_x),'--C1')     ax.plot(train_x, train_y,'oC0')     ax.plot(train_x, train_c,'oC1')      return ax plot_ground_truth(); In\u00a0[3]: Copied! <pre># plot the generator model and acquisition function\ntest_vocs = deepcopy(vocs)\ntest_vocs.constraints = {}\ngenerator = ExpectedImprovementGenerator(vocs=test_vocs)\ngenerator.add_data(data)\nmodel = generator.train_model()\nfig, ax = generator.visualize_model()\n</pre> # plot the generator model and acquisition function test_vocs = deepcopy(vocs) test_vocs.constraints = {} generator = ExpectedImprovementGenerator(vocs=test_vocs) generator.add_data(data) model = generator.train_model() fig, ax = generator.visualize_model() In\u00a0[4]: Copied! <pre># plot the generator model and acquisition function\ngenerator = ExpectedImprovementGenerator(vocs=deepcopy(vocs))\ngenerator.add_data(data)\nmodel = generator.train_model()\nfig, ax = generator.visualize_model()\n</pre> # plot the generator model and acquisition function generator = ExpectedImprovementGenerator(vocs=deepcopy(vocs)) generator.add_data(data) model = generator.train_model() fig, ax = generator.visualize_model() In\u00a0[5]: Copied! <pre># plot the generator model and acquisition function\ntvocs = deepcopy(vocs)\ntvocs.constraints = {\"c\": [\"GREATER_THAN\", 0]}\n\ngenerator = ExpectedImprovementGenerator(vocs=tvocs)\ngenerator.add_data(data)\nmodel = generator.train_model()\nfig, ax = generator.visualize_model()\n</pre> # plot the generator model and acquisition function tvocs = deepcopy(vocs) tvocs.constraints = {\"c\": [\"GREATER_THAN\", 0]}  generator = ExpectedImprovementGenerator(vocs=tvocs) generator.add_data(data) model = generator.train_model() fig, ax = generator.visualize_model() In\u00a0[6]: Copied! <pre># plot the generator model and acquisition function\nfrom xopt.generators.bayesian import BayesianExplorationGenerator\ntest_vocs = deepcopy(vocs)\ntest_vocs.objectives = {}\ntest_vocs.observables = [\"y\"]\n\ngenerator = BayesianExplorationGenerator(vocs=test_vocs)\ngenerator.add_data(data)\nmodel = generator.train_model()\nfig, ax = generator.visualize_model()\n</pre> # plot the generator model and acquisition function from xopt.generators.bayesian import BayesianExplorationGenerator test_vocs = deepcopy(vocs) test_vocs.objectives = {} test_vocs.observables = [\"y\"]  generator = BayesianExplorationGenerator(vocs=test_vocs) generator.add_data(data) model = generator.train_model() fig, ax = generator.visualize_model() In\u00a0[7]: Copied! <pre>generator = BayesianExplorationGenerator(vocs=test_vocs)\n\ndata = pd.DataFrame(\n    {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": torch.zeros_like(train_y).numpy()}\n)\ngenerator.add_data(data)\nmodel = generator.train_model()\nfig, ax = generator.visualize_model()\n</pre> generator = BayesianExplorationGenerator(vocs=test_vocs)  data = pd.DataFrame(     {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": torch.zeros_like(train_y).numpy()} ) generator.add_data(data) model = generator.train_model() fig, ax = generator.visualize_model() In\u00a0[8]: Copied! <pre>test_vocs = deepcopy(test_vocs)\ntest_vocs.constraints = {}\ngenerator = BayesianExplorationGenerator(vocs=test_vocs)\n\ndata = pd.DataFrame(\n    {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": torch.zeros_like(train_y).numpy()}\n)\ngenerator.add_data(data)\nmodel = generator.train_model()\nfig, ax = generator.visualize_model()\n</pre> test_vocs = deepcopy(test_vocs) test_vocs.constraints = {} generator = BayesianExplorationGenerator(vocs=test_vocs)  data = pd.DataFrame(     {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": torch.zeros_like(train_y).numpy()} ) generator.add_data(data) model = generator.train_model() fig, ax = generator.visualize_model() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/benchmarking/#normal-model-with-standard-transforms-and-no-constraints","title":"Normal Model with Standard transforms and no constraints\u00b6","text":"<ul> <li>acquisition function is Expected Improvement</li> </ul>"},{"location":"examples/single_objective_bayes_opt/benchmarking/#normal-model-with-standard-transforms-and-constraints","title":"Normal Model with Standard transforms and constraints\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/bo_tutorial/","title":"Bayesian optimization tutorial","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\nimport math\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> from xopt.vocs import VOCS import math  # define variables and function objectives vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\ndef sin_function(input_dict):\n    return {\"f\": np.sin(input_dict[\"x\"])}\n</pre> # define a test function to optimize import numpy as np  def sin_function(input_dict):     return {\"f\": np.sin(input_dict[\"x\"])} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt import Xopt\n\nevaluator = Evaluator(function=sin_function)\ngenerator = UpperConfidenceBoundGenerator(vocs=vocs)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt import Xopt  evaluator = Evaluator(function=sin_function) generator = UpperConfidenceBoundGenerator(vocs=vocs) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre># call X.random_evaluate() to generate + evaluate 3 initial points\nX.random_evaluate(2)\n\n# inspect the gathered data\nX.data\n</pre> # call X.random_evaluate() to generate + evaluate 3 initial points X.random_evaluate(2)  # inspect the gathered data X.data Out[4]: x f xopt_runtime xopt_error 0 3.578319 -0.422975 0.000017 False 1 1.708847 0.990486 0.000002 False In\u00a0[5]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n\nn_steps = 5\n\n# test points for plotting\ntest_x = torch.linspace(*X.vocs.bounds.flatten(), 50).double()\n\nfor i in range(n_steps):\n    # get the Gaussian process model from the generator\n    model = X.generator.train_model()\n\n    # get acquisition function from generator\n    acq = X.generator.get_acquisition(model)\n\n    # calculate model posterior and acquisition function at each test point\n    # NOTE: need to add a dimension to the input tensor for evaluating the\n    # posterior and another for the acquisition function, see\n    # https://botorch.org/docs/batching for details\n    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n    # skipping calculations for backpropagation\n    with torch.no_grad():\n        posterior = model.posterior(test_x.unsqueeze(1))\n        acq_val = acq(test_x.reshape(-1, 1, 1))\n\n    # get mean function and confidence regions\n    mean = posterior.mean\n    l,u = posterior.mvn.confidence_region()\n\n    # plot model and acquisition function\n    fig,ax = plt.subplots(2, 1, sharex=\"all\")\n\n    # plot model posterior\n    ax[0].plot(test_x, mean, label=\"Posterior mean\")\n    ax[0].fill_between(test_x, l, u, alpha=0.25, label=\"Posterior confidence region\")\n\n    # add data to model plot\n    ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")\n\n    # plot true function\n    true_f = sin_function({\"x\": test_x})[\"f\"]\n    ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")\n\n    # add legend\n    ax[0].legend()\n\n    # plot acquisition function\n    ax[1].plot(test_x, acq_val.flatten())\n\n    ax[0].set_ylabel(\"f\")\n    ax[1].set_ylabel(r\"$\\alpha(x)$\")\n    ax[1].set_xlabel(\"x\")\n\n    # do the optimization step\n    X.step()\n</pre> import torch import matplotlib.pyplot as plt  n_steps = 5  # test points for plotting test_x = torch.linspace(*X.vocs.bounds.flatten(), 50).double()  for i in range(n_steps):     # get the Gaussian process model from the generator     model = X.generator.train_model()      # get acquisition function from generator     acq = X.generator.get_acquisition(model)      # calculate model posterior and acquisition function at each test point     # NOTE: need to add a dimension to the input tensor for evaluating the     # posterior and another for the acquisition function, see     # https://botorch.org/docs/batching for details     # NOTE: we use the `torch.no_grad()` environment to speed up computation by     # skipping calculations for backpropagation     with torch.no_grad():         posterior = model.posterior(test_x.unsqueeze(1))         acq_val = acq(test_x.reshape(-1, 1, 1))      # get mean function and confidence regions     mean = posterior.mean     l,u = posterior.mvn.confidence_region()      # plot model and acquisition function     fig,ax = plt.subplots(2, 1, sharex=\"all\")      # plot model posterior     ax[0].plot(test_x, mean, label=\"Posterior mean\")     ax[0].fill_between(test_x, l, u, alpha=0.25, label=\"Posterior confidence region\")      # add data to model plot     ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")      # plot true function     true_f = sin_function({\"x\": test_x})[\"f\"]     ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")      # add legend     ax[0].legend()      # plot acquisition function     ax[1].plot(test_x, acq_val.flatten())      ax[0].set_ylabel(\"f\")     ax[1].set_ylabel(r\"$\\alpha(x)$\")     ax[1].set_xlabel(\"x\")      # do the optimization step     X.step()  In\u00a0[6]: Copied! <pre># access the collected data\nX.data\n</pre> # access the collected data X.data Out[6]: x f xopt_runtime xopt_error 0 3.578319 -0.422975 0.000017 False 1 1.708847 0.990486 0.000002 False 2 6.142669 -0.140055 0.000008 False 3 4.698530 -0.999904 0.000007 False 4 4.413695 -0.955722 0.000007 False 5 4.805886 -0.995632 0.000007 False 6 4.686153 -0.999656 0.000007 False In\u00a0[7]: Copied! <pre>X.generator.get_optimum()\n</pre> X.generator.get_optimum() Out[7]: x 0 4.703172 In\u00a0[8]: Copied! <pre>X.generator.dict()\n</pre> X.generator.dict() Out[8]: <pre>{'model': ModelListGP(\n   (models): ModuleList(\n     (0): SingleTaskGP(\n       (likelihood): GaussianLikelihood(\n         (noise_covar): HomoskedasticNoise(\n           (noise_prior): GammaPrior()\n           (raw_noise_constraint): GreaterThan(1.000E-04)\n         )\n       )\n       (mean_module): ConstantMean()\n       (covar_module): ScaleKernel(\n         (base_kernel): MaternKernel(\n           (lengthscale_prior): GammaPrior()\n           (raw_lengthscale_constraint): Positive()\n         )\n         (outputscale_prior): GammaPrior()\n         (raw_outputscale_constraint): Positive()\n       )\n       (outcome_transform): Standardize()\n       (input_transform): Normalize()\n     )\n   )\n   (likelihood): LikelihoodList(\n     (likelihoods): ModuleList(\n       (0): GaussianLikelihood(\n         (noise_covar): HomoskedasticNoise(\n           (noise_prior): GammaPrior()\n           (raw_noise_constraint): GreaterThan(1.000E-04)\n         )\n       )\n     )\n   )\n ),\n 'n_monte_carlo_samples': 128,\n 'turbo_controller': None,\n 'use_cuda': False,\n 'gp_constructor': {'name': 'standard',\n  'use_low_noise_prior': True,\n  'covar_modules': {},\n  'mean_modules': {},\n  'trainable_mean_keys': [],\n  'transform_inputs': True},\n 'numerical_optimizer': {'name': 'LBFGS',\n  'n_restarts': 20,\n  'max_iter': 2000,\n  'max_time': None},\n 'max_travel_distances': None,\n 'fixed_features': None,\n 'computation_time':    training  acquisition_optimization\n 0  0.073235                  0.040914\n 1  0.067906                  0.026450\n 2  0.066214                  0.038205\n 3  0.072196                  0.030403\n 4  0.077389                  0.038384,\n 'log_transform_acquisition_function': False,\n 'n_interpolate_points': None,\n 'n_candidates': 1,\n 'beta': 2.0}</pre> In\u00a0[8]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#basic-bayesian-optimization","title":"Basic Bayesian Optimization\u00b6","text":"<p>In this tutorial we demonstrate the use of Xopt to preform Bayesian Optimization on a simple test problem.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize the sin function in the domian [0,2*pi]. Note that the function used to evaluate the objective function takes a dictionary as input and returns a dictionary as the output.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Upper Confidence Bound acquisition function to perform Bayesian Optimization.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>To perform optimization we simply call <code>X.step()</code> in a loop. This allows us to do intermediate tasks in between optimization steps, such as examining the model and acquisition function at each step (as we demonstrate here).</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#getting-the-optimization-result","title":"Getting the optimization result\u00b6","text":"<p>To get the best point (without evaluating it) we ask the generator to predict the optimum based on the posterior mean.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#customizing-optimization","title":"Customizing optimization\u00b6","text":"<p>Each generator has a set of options that can be modified to effect optimization behavior</p>"},{"location":"examples/single_objective_bayes_opt/constrained_bo_tutorial/","title":"Constrained optimization","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time\nimport math\n\nfrom xopt.vocs import VOCS\n\n# define variables, function objective and constraining function\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n    constraints={\"c\": [\"LESS_THAN\", 0]}\n)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\") import time import math  from xopt.vocs import VOCS  # define variables, function objective and constraining function vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"},     constraints={\"c\": [\"LESS_THAN\", 0]} ) In\u00a0[2]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\ndef test_function(input_dict):\n    return {\"f\": np.sin(input_dict[\"x\"]),\"c\": np.cos(input_dict[\"x\"])}\n</pre> # define a test function to optimize import numpy as np  def test_function(input_dict):     return {\"f\": np.sin(input_dict[\"x\"]),\"c\": np.cos(input_dict[\"x\"])} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import ExpectedImprovementGenerator\nfrom xopt import Xopt\n\nevaluator = Evaluator(function=test_function)\ngenerator = ExpectedImprovementGenerator(vocs=vocs)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import ExpectedImprovementGenerator from xopt import Xopt  evaluator = Evaluator(function=test_function) generator = ExpectedImprovementGenerator(vocs=vocs) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre># call X.random_evaluate(n_samples) to generate + evaluate initial points\nX.random_evaluate(n_samples=2)\n\n# inspect the gathered data\nX.data\n</pre> # call X.random_evaluate(n_samples) to generate + evaluate initial points X.random_evaluate(n_samples=2)  # inspect the gathered data X.data Out[4]: x f c xopt_runtime xopt_error 0 2.161437 0.830584 -0.556893 0.000008 False 1 3.805150 -0.615923 -0.787806 0.000001 False In\u00a0[5]: Copied! <pre>import time\n\nn_steps = 5\n\n# test points for plotting\ntest_x = np.linspace(*X.vocs.bounds.flatten(), 50)\n\nfor i in range(n_steps):\n    start = time.perf_counter()\n    model = X.generator.train_model()\n    fig, ax = X.generator.visualize_model(n_grid=100)\n    print(time.perf_counter() - start)\n\n    # add ground truth functions to plots\n    out = test_function({\"x\": test_x})\n    ax[0].plot(test_x, out[\"f\"], \"C0-.\")\n    ax[1].plot(test_x, out[\"c\"], \"C2-.\")\n\n    # do the optimization step\n    X.step()\n</pre> import time  n_steps = 5  # test points for plotting test_x = np.linspace(*X.vocs.bounds.flatten(), 50)  for i in range(n_steps):     start = time.perf_counter()     model = X.generator.train_model()     fig, ax = X.generator.visualize_model(n_grid=100)     print(time.perf_counter() - start)      # add ground truth functions to plots     out = test_function({\"x\": test_x})     ax[0].plot(test_x, out[\"f\"], \"C0-.\")     ax[1].plot(test_x, out[\"c\"], \"C2-.\")      # do the optimization step     X.step() <pre>0.473529958006111\n0.14257725000788923\n0.15709620900452137\n0.22288366699649487\n0.156100249994779\n</pre> In\u00a0[6]: Copied! <pre># access the collected data\nX.data\n</pre> # access the collected data X.data Out[6]: x f c xopt_runtime xopt_error 0 2.161437 0.830584 -0.556893 0.000008 False 1 3.805150 -0.615923 -0.787806 0.000001 False 3 5.723410 -0.530996 0.847374 0.000005 False 4 4.370528 -0.942133 -0.335241 0.000005 False 5 4.610993 -0.994864 -0.101222 0.000006 False 6 4.692337 -0.999799 -0.020050 0.000010 False 7 4.700659 -0.999931 -0.011730 0.000006 False In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/constrained_bo_tutorial/#constrained-bayesian-optimization","title":"Constrained Bayesian Optimization\u00b6","text":"<p>In this tutorial we demonstrate the use of Xopt to perform Bayesian Optimization on a simple test problem subject to a single constraint.</p>"},{"location":"examples/single_objective_bayes_opt/constrained_bo_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize the sin function in the domian [0,2*pi], subject to a cos constraining function.</p>"},{"location":"examples/single_objective_bayes_opt/constrained_bo_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Expected Improvement acquisition function to perform Bayesian Optimization.</p>"},{"location":"examples/single_objective_bayes_opt/constrained_bo_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/constrained_bo_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>To perform optimization we simply call <code>X.step()</code> in a loop. This allows us to do intermediate tasks in between optimization steps, such as examining the model and acquisition function at each step (as we demonstrate here).</p>"},{"location":"examples/single_objective_bayes_opt/custom_model/","title":"Custom GP modeling for BO","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom xopt.vocs import VOCS\n\nmy_vocs = VOCS(\n    variables = {\"x\":[0,1]},\n    objectives = {\"y\":\"MAXIMIZE\"},\n    constraints = {\"c\": [\"LESS_THAN\", 0]}\n)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import matplotlib.pyplot as plt import pandas as pd import torch from xopt.vocs import VOCS  my_vocs = VOCS(     variables = {\"x\":[0,1]},     objectives = {\"y\":\"MAXIMIZE\"},     constraints = {\"c\": [\"LESS_THAN\", 0]} )  In\u00a0[2]: Copied! <pre># define test functions\ndef y(x):\n    return torch.sin(2*3.14*x)\n\ndef c(x):\n    return 5.0*torch.cos(2*3.14*x + 0.25)\n\ntest_x = torch.linspace(*torch.tensor(my_vocs.bounds.flatten()), 100)\n\n# define training data to pass to the generator\ntrain_x = torch.tensor((0.2,0.5, 0.6))\ntrain_y = y(train_x)\ntrain_c = c(train_x)\n\ntraining_data = pd.DataFrame(\n    {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c}\n)\n\ndef plot_ground_truth():\n    fig,ax = plt.subplots()\n    ax.plot(test_x, y(test_x),'--C0')\n    ax.plot(test_x, c(test_x),'--C1')\n    ax.plot(train_x, train_y,'oC0')\n    ax.plot(train_x, train_c,'oC1')\n\n    return ax\nplot_ground_truth()\n</pre> # define test functions def y(x):     return torch.sin(2*3.14*x)  def c(x):     return 5.0*torch.cos(2*3.14*x + 0.25)  test_x = torch.linspace(*torch.tensor(my_vocs.bounds.flatten()), 100)  # define training data to pass to the generator train_x = torch.tensor((0.2,0.5, 0.6)) train_y = y(train_x) train_c = c(train_x)  training_data = pd.DataFrame(     {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c} )  def plot_ground_truth():     fig,ax = plt.subplots()     ax.plot(test_x, y(test_x),'--C0')     ax.plot(test_x, c(test_x),'--C1')     ax.plot(train_x, train_y,'oC0')     ax.plot(train_x, train_c,'oC1')      return ax plot_ground_truth() Out[2]: <pre>&lt;Axes: &gt;</pre> In\u00a0[3]: Copied! <pre>from xopt.generators.bayesian.expected_improvement import ExpectedImprovementGenerator\nfrom xopt.generators.bayesian.models.standard import StandardModelConstructor\nfrom gpytorch.kernels import PeriodicKernel, ScaleKernel\n\n\n# note the creation of options beforehand\n# specify a periodic kernel for each output (objectives and constraints)\ncovar_module = {\"y\": ScaleKernel(PeriodicKernel())}\ngp_constructor = StandardModelConstructor(\n    covar_modules=covar_module\n)\ngenerator = ExpectedImprovementGenerator(\n    vocs=my_vocs, gp_constructor=gp_constructor\n)\ngenerator\n</pre> from xopt.generators.bayesian.expected_improvement import ExpectedImprovementGenerator from xopt.generators.bayesian.models.standard import StandardModelConstructor from gpytorch.kernels import PeriodicKernel, ScaleKernel   # note the creation of options beforehand # specify a periodic kernel for each output (objectives and constraints) covar_module = {\"y\": ScaleKernel(PeriodicKernel())} gp_constructor = StandardModelConstructor(     covar_modules=covar_module ) generator = ExpectedImprovementGenerator(     vocs=my_vocs, gp_constructor=gp_constructor ) generator Out[3]: <pre>ExpectedImprovementGenerator(supports_batch_generation=True, supports_multi_objective=False, vocs=VOCS(variables={'x': [0.0, 1.0]}, constraints={'c': ['LESS_THAN', 0.0]}, objectives={'y': 'MAXIMIZE'}, constants={}, observables=[]), data=None, model=None, n_monte_carlo_samples=128, turbo_controller=None, use_cuda=False, gp_constructor=StandardModelConstructor(name='standard', use_low_noise_prior=True, covar_modules={'y': ScaleKernel(\n  (base_kernel): PeriodicKernel(\n    (raw_lengthscale_constraint): Positive()\n    (raw_period_length_constraint): Positive()\n  )\n  (raw_outputscale_constraint): Positive()\n)}, mean_modules={}, trainable_mean_keys=[], transform_inputs=True), numerical_optimizer=LBFGSOptimizer(name='LBFGS', n_restarts=20, max_iter=2000, max_time=None), max_travel_distances=None, fixed_features=None, computation_time=None, log_transform_acquisition_function=False, n_interpolate_points=None, n_candidates=1)</pre> In\u00a0[4]: Copied! <pre># view custom model from data\ngenerator.add_data(training_data)\nmodel = generator.train_model()\n\nfig,ax = plt.subplots(2,1, sharex=\"all\")\nfig.set_size_inches(6,6)\nwith torch.no_grad():\n    post = model.posterior(test_x.reshape(-1,1,1).double())\n\n    for i in range(post.event_shape[-1]):\n        mean = post.mean[...,i].squeeze()\n        l,u = post.mvn.confidence_region()\n        ax[0].plot(test_x, mean,f\"C{i}\", label=generator.vocs.output_names[i])\n        ax[0].fill_between(test_x, l[...,i].squeeze(), u[...,i].squeeze(), alpha=0.5)\n\n    # plot ground truth\n    ax[0].plot(test_x, y(test_x),'C0--', label=\"y ground truth\")\n    ax[0].plot(test_x, c(test_x),'C1--', label=\"c ground truth\")\n\n    # plot training data\n    ax[0].plot(train_x, train_y,\"C0o\", label=\"y data\")\n    ax[0].plot(train_x, train_c,\"C1o\", label=\"c data\")\n    ax[0].legend()\n\n\n    acq = generator.get_acquisition(model)(test_x.reshape(-1,1,1).double())\n\n    ax[1].plot(test_x, acq, label='Acquisition Function')\n    ax[1].legend()\n</pre> # view custom model from data generator.add_data(training_data) model = generator.train_model()  fig,ax = plt.subplots(2,1, sharex=\"all\") fig.set_size_inches(6,6) with torch.no_grad():     post = model.posterior(test_x.reshape(-1,1,1).double())      for i in range(post.event_shape[-1]):         mean = post.mean[...,i].squeeze()         l,u = post.mvn.confidence_region()         ax[0].plot(test_x, mean,f\"C{i}\", label=generator.vocs.output_names[i])         ax[0].fill_between(test_x, l[...,i].squeeze(), u[...,i].squeeze(), alpha=0.5)      # plot ground truth     ax[0].plot(test_x, y(test_x),'C0--', label=\"y ground truth\")     ax[0].plot(test_x, c(test_x),'C1--', label=\"c ground truth\")      # plot training data     ax[0].plot(train_x, train_y,\"C0o\", label=\"y data\")     ax[0].plot(train_x, train_c,\"C1o\", label=\"c data\")     ax[0].legend()       acq = generator.get_acquisition(model)(test_x.reshape(-1,1,1).double())      ax[1].plot(test_x, acq, label='Acquisition Function')     ax[1].legend() In\u00a0[5]: Copied! <pre>model\n</pre> model Out[5]: <pre>ModelListGP(\n  (models): ModuleList(\n    (0): SingleTaskGP(\n      (likelihood): GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n      (mean_module): ConstantMean()\n      (covar_module): ScaleKernel(\n        (base_kernel): PeriodicKernel(\n          (raw_lengthscale_constraint): Positive()\n          (raw_period_length_constraint): Positive()\n        )\n        (raw_outputscale_constraint): Positive()\n      )\n      (outcome_transform): Standardize()\n      (input_transform): Normalize()\n    )\n    (1): SingleTaskGP(\n      (likelihood): GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n      (mean_module): ConstantMean()\n      (covar_module): ScaleKernel(\n        (base_kernel): MaternKernel(\n          (lengthscale_prior): GammaPrior()\n          (raw_lengthscale_constraint): Positive()\n        )\n        (outputscale_prior): GammaPrior()\n        (raw_outputscale_constraint): Positive()\n      )\n      (outcome_transform): Standardize()\n      (input_transform): Normalize()\n    )\n  )\n  (likelihood): LikelihoodList(\n    (likelihoods): ModuleList(\n      (0-1): 2 x GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n    )\n  )\n)</pre> In\u00a0[6]: Copied! <pre># get the next point from the generator\ngenerator.generate(1)\n</pre> # get the next point from the generator generator.generate(1) Out[6]: <pre>[{'x': 0.2759158289948305}]</pre> In\u00a0[7]: Copied! <pre>class ConstraintPrior(torch.nn.Module):\n    def forward(self, X):\n        return c(X).squeeze(dim=-1)\n\ngp_constructor = StandardModelConstructor(\n    mean_modules={\"c\":ConstraintPrior()}\n)\ngenerator = ExpectedImprovementGenerator(\n    vocs=my_vocs, gp_constructor=gp_constructor)\n</pre> class ConstraintPrior(torch.nn.Module):     def forward(self, X):         return c(X).squeeze(dim=-1)  gp_constructor = StandardModelConstructor(     mean_modules={\"c\":ConstraintPrior()} ) generator = ExpectedImprovementGenerator(     vocs=my_vocs, gp_constructor=gp_constructor) In\u00a0[8]: Copied! <pre># view custom model from data\ngenerator.add_data(training_data)\nmodel = generator.train_model()\ntest_x = torch.linspace(0,5, 100)\n\n\nfig,ax = plt.subplots(2,1, sharex=\"all\")\nfig.set_size_inches(6,6)\nwith torch.no_grad():\n    post = model.posterior(test_x.reshape(-1,1,1).double())\n\n    for i in range(post.event_shape[-1]):\n        mean = post.mean[...,i].squeeze()\n        l,u = post.mvn.confidence_region()\n        ax[0].plot(test_x, mean,f\"C{i}\", label=generator.vocs.output_names[i])\n        ax[0].fill_between(test_x, l[...,i].squeeze(), u[...,i].squeeze(), alpha=0.5)\n\n    # plot ground truth\n    ax[0].plot(test_x, y(test_x),'C0--', label=\"y ground truth\")\n    ax[0].plot(test_x, c(test_x),'C1--', label=\"c ground truth\")\n\n    # plot training data\n    ax[0].plot(train_x, train_y,\"C0o\", label=\"y data\")\n    ax[0].plot(train_x, train_c,\"C1o\", label=\"c data\")\n    ax[0].legend()\n\n\n    acq = generator.get_acquisition(model)(test_x.reshape(-1,1,1).double())\n\n    ax[1].plot(test_x, acq, label='Acquisition Function')\n    ax[1].legend()\n</pre> # view custom model from data generator.add_data(training_data) model = generator.train_model() test_x = torch.linspace(0,5, 100)   fig,ax = plt.subplots(2,1, sharex=\"all\") fig.set_size_inches(6,6) with torch.no_grad():     post = model.posterior(test_x.reshape(-1,1,1).double())      for i in range(post.event_shape[-1]):         mean = post.mean[...,i].squeeze()         l,u = post.mvn.confidence_region()         ax[0].plot(test_x, mean,f\"C{i}\", label=generator.vocs.output_names[i])         ax[0].fill_between(test_x, l[...,i].squeeze(), u[...,i].squeeze(), alpha=0.5)      # plot ground truth     ax[0].plot(test_x, y(test_x),'C0--', label=\"y ground truth\")     ax[0].plot(test_x, c(test_x),'C1--', label=\"c ground truth\")      # plot training data     ax[0].plot(train_x, train_y,\"C0o\", label=\"y data\")     ax[0].plot(train_x, train_c,\"C1o\", label=\"c data\")     ax[0].legend()       acq = generator.get_acquisition(model)(test_x.reshape(-1,1,1).double())      ax[1].plot(test_x, acq, label='Acquisition Function')     ax[1].legend() In\u00a0[9]: Copied! <pre>model\n</pre> model Out[9]: <pre>ModelListGP(\n  (models): ModuleList(\n    (0): SingleTaskGP(\n      (likelihood): GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n      (mean_module): ConstantMean()\n      (covar_module): ScaleKernel(\n        (base_kernel): MaternKernel(\n          (lengthscale_prior): GammaPrior()\n          (raw_lengthscale_constraint): Positive()\n        )\n        (outputscale_prior): GammaPrior()\n        (raw_outputscale_constraint): Positive()\n      )\n      (outcome_transform): Standardize()\n      (input_transform): Normalize()\n    )\n    (1): SingleTaskGP(\n      (likelihood): GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n      (mean_module): CustomMean(\n        (_model): ConstraintPrior()\n        (input_transformer): Normalize()\n        (outcome_transformer): Standardize()\n      )\n      (covar_module): ScaleKernel(\n        (base_kernel): MaternKernel(\n          (lengthscale_prior): GammaPrior()\n          (raw_lengthscale_constraint): Positive()\n        )\n        (outputscale_prior): GammaPrior()\n        (raw_outputscale_constraint): Positive()\n      )\n      (outcome_transform): Standardize()\n      (input_transform): Normalize()\n    )\n  )\n  (likelihood): LikelihoodList(\n    (likelihoods): ModuleList(\n      (0-1): 2 x GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n    )\n  )\n)</pre> In\u00a0[10]: Copied! <pre>list(model.named_parameters())\n</pre> list(model.named_parameters()) Out[10]: <pre>[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-23.1611], dtype=torch.float64, requires_grad=True)),\n ('models.0.mean_module.raw_constant',\n  Parameter containing:\n  tensor(-0.2223, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(3.0724, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[-0.7308]], dtype=torch.float64, requires_grad=True)),\n ('models.1.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-23.2184], dtype=torch.float64, requires_grad=True)),\n ('models.1.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(-6.7635, dtype=torch.float64, requires_grad=True)),\n ('models.1.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[-0.5597]], dtype=torch.float64, requires_grad=True))]</pre> In\u00a0[10]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/custom_model/#custom-gp-modeling-for-bo","title":"Custom GP modeling for BO\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/custom_model/#custom-kernel-definition","title":"Custom kernel definition\u00b6","text":"<p>In this example we know that the target optimization function is periodic, so it makes sense to use a periodic kernel for the GP model with no noise. Here we define a function to create that model.</p>"},{"location":"examples/single_objective_bayes_opt/custom_model/#custom-prior-mean-function","title":"Custom prior mean function\u00b6","text":"<p>Here we assume we have some knowledge of the ground truth function, which we can take advantage of to speed up optimization. This \"prior mean\" function is specified by a pytorch module.</p>"},{"location":"examples/single_objective_bayes_opt/fixed_features/","title":"Bayesian optimization with fixed features","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt.resources.test_functions.rosenbrock import evaluate_rosenbrock,make_rosenbrock_vocs\n\n# make rosenbrock function vocs in 2D\nvocs = make_rosenbrock_vocs(2)\n\n# define a fixed value for the BO generator\nfixed_features = {\"x0\":-1.0}\ngenerator = UpperConfidenceBoundGenerator(\n    vocs=vocs, fixed_features=fixed_features\n)\ngenerator.numerical_optimizer.n_restarts = NUM_RESTARTS\ngenerator.n_monte_carlo_samples = NUM_MC_SAMPLES\n\nevaluator = Evaluator(function=evaluate_rosenbrock)\n\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") NUM_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  from xopt import Xopt, Evaluator from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt.resources.test_functions.rosenbrock import evaluate_rosenbrock,make_rosenbrock_vocs  # make rosenbrock function vocs in 2D vocs = make_rosenbrock_vocs(2)  # define a fixed value for the BO generator fixed_features = {\"x0\":-1.0} generator = UpperConfidenceBoundGenerator(     vocs=vocs, fixed_features=fixed_features ) generator.numerical_optimizer.n_restarts = NUM_RESTARTS generator.n_monte_carlo_samples = NUM_MC_SAMPLES  evaluator = Evaluator(function=evaluate_rosenbrock)  X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X Out[1]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock\n  function_kwargs:\n    dummy: 1\n    label: y\n  max_workers: 1\n  vectorized: false\ngenerator:\n  beta: 2.0\n  computation_time: null\n  fixed_features:\n    x0: -1.0\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: upper_confidence_bound\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  supports_batch_generation: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints: {}\n  objectives:\n    y: MINIMIZE\n  observables: []\n  variables:\n    x0:\n    - -2.0\n    - 2.0\n    x1:\n    - -2.0\n    - 2.0\n</pre> In\u00a0[2]: Copied! <pre>X.random_evaluate(10)\n</pre> X.random_evaluate(10) Out[2]: x0 x1 y xopt_runtime xopt_error 0 0.088243 -1.965051 390.040193 0.000010 False 1 -1.705828 0.285944 695.809249 0.000004 False 2 -0.542899 0.671296 16.560048 0.000003 False 3 0.123090 1.258190 155.283481 0.000002 False 4 -0.955720 -1.633507 652.498388 0.000002 False 5 1.943168 1.786736 396.567221 0.000002 False 6 0.337820 -1.029199 131.156773 0.000002 False 7 -1.353144 -0.052149 360.161657 0.000002 False 8 -1.919566 -1.809150 3026.800218 0.000002 False 9 -1.745366 -1.862789 2417.455487 0.000002 False In\u00a0[3]: Copied! <pre>for i in range(5):\n    X.step()\n</pre> for i in range(5):     X.step() In\u00a0[4]: Copied! <pre>X.data\n</pre> X.data Out[4]: x0 x1 y xopt_runtime xopt_error 0 0.088243 -1.965051 390.040193 0.000010 False 1 -1.705828 0.285944 695.809249 0.000004 False 2 -0.542899 0.671296 16.560048 0.000003 False 3 0.123090 1.258190 155.283481 0.000002 False 4 -0.955720 -1.633507 652.498388 0.000002 False 5 1.943168 1.786736 396.567221 0.000002 False 6 0.337820 -1.029199 131.156773 0.000002 False 7 -1.353144 -0.052149 360.161657 0.000002 False 8 -1.919566 -1.809150 3026.800218 0.000002 False 9 -1.745366 -1.862789 2417.455487 0.000002 False 10 -1.000000 2.000000 104.000000 0.000008 False 11 -1.000000 0.939984 4.360192 0.000007 False 12 -1.000000 0.229592 63.352893 0.000007 False 13 -1.000000 1.339756 15.543433 0.000007 False 14 -1.000000 0.864286 5.841836 0.000008 False In\u00a0[5]: Copied! <pre>from matplotlib import pyplot as plt  # plot model predictions\nimport torch\ndata = X.data\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.train_model(generator.data)\n\n# create mesh\nn = 100\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nxx,yy = xx.numpy(), yy.numpy()\n\noutputs = generator.vocs.output_names\nwith torch.no_grad():\n    post = model.posterior(pts)\n\n    for i in range(len(vocs.output_names)):\n        mean = post.mean[...,i]\n        fig, ax = plt.subplots()\n        ax.plot(*data[[\"x0\", \"x1\"]].to_numpy()[:10].T, \"+C1\",\n                label=\"random samples\",\n                zorder=10\n                )\n        ax.plot(*data[[\"x0\", \"x1\"]].to_numpy()[10:].T, \"+C3\",\n                label=\"GP samples\",\n                zorder=10)\n\n\n        c = ax.pcolor(\n            xx, yy, mean.squeeze().reshape(n, n)\n        )\n        fig.colorbar(c)\n        ax.set_title(f\"Posterior mean: {outputs[i]}\")\n        ax.axvline(-1.0,ls=\"--\")\n        ax.set_xlabel(\"x0\")\n        ax.set_ylabel(\"x1\")\n        ax.legend()\n</pre> from matplotlib import pyplot as plt  # plot model predictions import torch data = X.data  bounds = X.generator.vocs.bounds model = X.generator.train_model(generator.data)  # create mesh n = 100 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  xx,yy = xx.numpy(), yy.numpy()  outputs = generator.vocs.output_names with torch.no_grad():     post = model.posterior(pts)      for i in range(len(vocs.output_names)):         mean = post.mean[...,i]         fig, ax = plt.subplots()         ax.plot(*data[[\"x0\", \"x1\"]].to_numpy()[:10].T, \"+C1\",                 label=\"random samples\",                 zorder=10                 )         ax.plot(*data[[\"x0\", \"x1\"]].to_numpy()[10:].T, \"+C3\",                 label=\"GP samples\",                 zorder=10)           c = ax.pcolor(             xx, yy, mean.squeeze().reshape(n, n)         )         fig.colorbar(c)         ax.set_title(f\"Posterior mean: {outputs[i]}\")         ax.axvline(-1.0,ls=\"--\")         ax.set_xlabel(\"x0\")         ax.set_ylabel(\"x1\")         ax.legend() In\u00a0[5]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/fixed_features/#bayesian-optimization-with-fixed-features","title":"Bayesian optimization with fixed features\u00b6","text":"<p>In some contexts a variable/feature needs to be fixed during optimization. However, we can leverage previous measurements near the fixed variable value to potentially jump-start optimization using observed model covariances established by the GP kernel . In this example, we start with a number of random observations in 2D input space and then proceed with BO at a fixed value for one of the variables. This notebook uses the 2D Rosenbrock test function as an example.</p>"},{"location":"examples/single_objective_bayes_opt/fixed_features/#generate-some-initial-random-samples-in-2d-space","title":"Generate some initial random samples in 2D space\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/fixed_features/#run-bo-steps-with-fixed-features","title":"Run BO steps with fixed features\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/fixed_features/#visualize-model-and-evaluations","title":"Visualize model and evaluations\u00b6","text":"<p>Note that for the BO samples, they all are on the line $x_0=-1$</p>"},{"location":"examples/single_objective_bayes_opt/hessian_kernel/","title":"Bayesian Optimization with a Hessian","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom copy import deepcopy\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt.generators.bayesian.models.standard import StandardModelConstructor\nfrom xopt.generators.bayesian.custom_botorch.hessian_kernel import HessianRBF\nfrom gpytorch.kernels import ScaleKernel\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\n\nvocs = deepcopy(tnk_vocs)\nvocs.objectives = {\"y2\":\"MINIMIZE\"}\n\n# define a custom kernel and create the model constructor\nhessian_matrix = torch.tensor([[1, -0.8], [-0.8, 1]]).double()\nkernel = ScaleKernel(HessianRBF(hessian_matrix))\ngp_constructor = StandardModelConstructor(\n    covar_modules={\"y2\": kernel}\n)\n\n\ngenerator = UpperConfidenceBoundGenerator(\n    vocs=vocs, gp_constructor=gp_constructor\n)\ngenerator.numerical_optimizer.n_restarts = NUM_RESTARTS\ngenerator.n_monte_carlo_samples = NUM_MC_SAMPLES\n\nevaluator = Evaluator(function=evaluate_TNK)\n\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") NUM_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import torch from copy import deepcopy from xopt import Xopt, Evaluator from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt.generators.bayesian.models.standard import StandardModelConstructor from xopt.generators.bayesian.custom_botorch.hessian_kernel import HessianRBF from gpytorch.kernels import ScaleKernel from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs   vocs = deepcopy(tnk_vocs) vocs.objectives = {\"y2\":\"MINIMIZE\"}  # define a custom kernel and create the model constructor hessian_matrix = torch.tensor([[1, -0.8], [-0.8, 1]]).double() kernel = ScaleKernel(HessianRBF(hessian_matrix)) gp_constructor = StandardModelConstructor(     covar_modules={\"y2\": kernel} )   generator = UpperConfidenceBoundGenerator(     vocs=vocs, gp_constructor=gp_constructor ) generator.numerical_optimizer.n_restarts = NUM_RESTARTS generator.n_monte_carlo_samples = NUM_MC_SAMPLES  evaluator = Evaluator(function=evaluate_TNK)  X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X Out[1]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    raise_probability: 0\n    random_sleep: 0\n    sleep: 0\n  max_workers: 1\n  vectorized: false\ngenerator:\n  beta: 2.0\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: upper_confidence_bound\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  supports_batch_generation: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants:\n    a: dummy_constant\n  constraints:\n    c1:\n    - GREATER_THAN\n    - 0.0\n    c2:\n    - LESS_THAN\n    - 0.5\n  objectives:\n    y2: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 3.14159\n    x2:\n    - 0.0\n    - 3.14159\n</pre> In\u00a0[2]: Copied! <pre>X.evaluate_data({\"x1\":[1.0, 0.75],\"x2\":[1.0, 2.0]})\nX.generator.train_model()\nfig, ax = X.generator.visualize_model(show_feasibility=True, n_grid=100)\n</pre> X.evaluate_data({\"x1\":[1.0, 0.75],\"x2\":[1.0, 2.0]}) X.generator.train_model() fig, ax = X.generator.visualize_model(show_feasibility=True, n_grid=100)  In\u00a0[3]: Copied! <pre>X.data\n</pre> X.data Out[3]: x1 x2 a y1 y2 c1 c2 xopt_runtime xopt_error 0 1.00 1.0 dummy_constant 1.00 1.0 0.900000 0.5000 0.000034 False 1 0.75 2.0 dummy_constant 0.75 2.0 3.476876 2.3125 0.000008 False In\u00a0[3]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/hessian_kernel/#bayesian-optimization-with-a-hessian","title":"Bayesian Optimization with a Hessian\u00b6","text":"<p>Here we demonstrate the use of a Hessian matrix to estimate the kernel.</p>"},{"location":"examples/single_objective_bayes_opt/hessian_kernel/#specifiying-generator-options","title":"Specifiying generator options\u00b6","text":"<p>We start with the generator defaults and add a hessian kernel to the model. This also requires specifying that we will not normalize inputs to the GP model. Note: this can potentially mess up training of other hyperparameters.</p>"},{"location":"examples/single_objective_bayes_opt/hessian_kernel/#evaluate-explict-points-and-view-model","title":"Evaluate explict points and view model\u00b6","text":"<p>We start with evaluating 2 points that we know satisfy the constraints. Note the cross correlations between x1,x2 due to the Hessian kernel.</p>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/","title":"Heteroskedastic modeling","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\nimport math\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> from xopt.vocs import VOCS import math  # define variables and function objectives vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre># define a test function to optimize\n# the test function also returns an estimation of the variance, which is\n# used to create a Heteroskedastic noise model for the gp\nimport numpy as np\n\ndef sin_function(input_dict):\n    return {\"f\": np.sin(input_dict[\"x\"]), \"f_var\":0.001*input_dict[\"x\"]}\n</pre> # define a test function to optimize # the test function also returns an estimation of the variance, which is # used to create a Heteroskedastic noise model for the gp import numpy as np  def sin_function(input_dict):     return {\"f\": np.sin(input_dict[\"x\"]), \"f_var\":0.001*input_dict[\"x\"]} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt import Xopt\n\nevaluator = Evaluator(function=sin_function)\ngenerator = UpperConfidenceBoundGenerator(vocs=vocs)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt import Xopt  evaluator = Evaluator(function=sin_function) generator = UpperConfidenceBoundGenerator(vocs=vocs) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre># call X.random_evaluate() to generate + evaluate 3 initial points\nX.random_evaluate(4)\n\n# inspect the gathered data\nX.data\n</pre> # call X.random_evaluate() to generate + evaluate 3 initial points X.random_evaluate(4)  # inspect the gathered data X.data Out[4]: x f f_var xopt_runtime xopt_error 0 6.274476 -0.008710 0.006274 0.000018 False 1 3.822724 -0.629672 0.003823 0.000002 False 2 1.401506 0.985705 0.001402 0.000002 False 3 0.704802 0.647883 0.000705 0.000001 False In\u00a0[5]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n\nn_steps = 5\n\n# test points for plotting\ntest_x = torch.linspace(*X.vocs.bounds.flatten(), 50).double()\n\nfor i in range(n_steps):\n    # get the Gaussian process model from the generator\n\n    model = X.generator.train_model()\n\n    # get acquisition function from generator\n    acq = X.generator.get_acquisition(model)\n\n    # calculate model posterior and acquisition function at each test point\n    # NOTE: need to add a dimension to the input tensor for evaluating the\n    # posterior and another for the acquisition function, see\n    # https://botorch.org/docs/batching for details\n    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n    # skipping calculations for backpropagation\n    with torch.no_grad():\n        posterior = model.posterior(test_x.unsqueeze(1))\n        acq_val = acq(test_x.reshape(-1, 1, 1))\n\n    # get mean function and confidence regions\n    mean = posterior.mean\n    l,u = posterior.mvn.confidence_region()\n\n    # plot model and acquisition function\n    fig,ax = plt.subplots(2, 1, sharex=\"all\")\n\n    # plot model posterior\n    ax[0].plot(test_x, mean, label=\"Posterior mean\")\n    ax[0].fill_between(test_x, l, u, alpha=0.25, label=\"Posterior confidence region\")\n\n    # add data to model plot\n    ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")\n\n    # plot true function\n    true_f = sin_function({\"x\": test_x})[\"f\"]\n    ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")\n\n    # add legend\n    #ax[0].legend()\n\n    # plot acquisition function\n    ax[1].plot(test_x, acq_val.flatten())\n\n    ax[0].set_ylabel(\"f\")\n    ax[1].set_ylabel(r\"$\\alpha(x)$\")\n    ax[1].set_xlabel(\"x\")\n\n    # do the optimization step\n    X.step()\n</pre> import torch import matplotlib.pyplot as plt  n_steps = 5  # test points for plotting test_x = torch.linspace(*X.vocs.bounds.flatten(), 50).double()  for i in range(n_steps):     # get the Gaussian process model from the generator      model = X.generator.train_model()      # get acquisition function from generator     acq = X.generator.get_acquisition(model)      # calculate model posterior and acquisition function at each test point     # NOTE: need to add a dimension to the input tensor for evaluating the     # posterior and another for the acquisition function, see     # https://botorch.org/docs/batching for details     # NOTE: we use the `torch.no_grad()` environment to speed up computation by     # skipping calculations for backpropagation     with torch.no_grad():         posterior = model.posterior(test_x.unsqueeze(1))         acq_val = acq(test_x.reshape(-1, 1, 1))      # get mean function and confidence regions     mean = posterior.mean     l,u = posterior.mvn.confidence_region()      # plot model and acquisition function     fig,ax = plt.subplots(2, 1, sharex=\"all\")      # plot model posterior     ax[0].plot(test_x, mean, label=\"Posterior mean\")     ax[0].fill_between(test_x, l, u, alpha=0.25, label=\"Posterior confidence region\")      # add data to model plot     ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")      # plot true function     true_f = sin_function({\"x\": test_x})[\"f\"]     ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")      # add legend     #ax[0].legend()      # plot acquisition function     ax[1].plot(test_x, acq_val.flatten())      ax[0].set_ylabel(\"f\")     ax[1].set_ylabel(r\"$\\alpha(x)$\")     ax[1].set_xlabel(\"x\")      # do the optimization step     X.step()  In\u00a0[6]: Copied! <pre># access the collected data\nX.data\n</pre> # access the collected data X.data Out[6]: x f f_var xopt_runtime xopt_error 0 6.274476 -0.008710 0.006274 0.000018 False 1 3.822724 -0.629672 0.003823 0.000002 False 2 1.401506 0.985705 0.001402 0.000002 False 3 0.704802 0.647883 0.000705 0.000001 False 4 4.733631 -0.999774 0.004734 0.000007 False 5 4.499144 -0.977349 0.004499 0.000008 False 6 4.880438 -0.985913 0.004880 0.000008 False 7 4.652276 -0.998194 0.004652 0.000008 False 8 4.674442 -0.999280 0.004674 0.000007 False In\u00a0[7]: Copied! <pre>X.generator.get_optimum()\n</pre> X.generator.get_optimum() Out[7]: x 0 4.676551 In\u00a0[8]: Copied! <pre>X.generator.dict()\n</pre> X.generator.dict() Out[8]: <pre>{'model': ModelListGP(\n   (models): ModuleList(\n     (0): XoptHeteroskedasticSingleTaskGP(\n       (likelihood): _GaussianLikelihoodBase(\n         (noise_covar): HeteroskedasticNoise(\n           (noise_model): SingleTaskGP(\n             (likelihood): GaussianLikelihood(\n               (noise_covar): HomoskedasticNoise(\n                 (noise_prior): SmoothedBoxPrior()\n                 (raw_noise_constraint): GreaterThan(1.000E-04)\n               )\n             )\n             (mean_module): ConstantMean()\n             (covar_module): ScaleKernel(\n               (base_kernel): MaternKernel(\n                 (lengthscale_prior): GammaPrior()\n                 (raw_lengthscale_constraint): Positive()\n               )\n               (outputscale_prior): GammaPrior()\n               (raw_outputscale_constraint): Positive()\n             )\n             (outcome_transform): Log()\n             (input_transform): Normalize()\n           )\n           (_noise_constraint): GreaterThan(1.000E-04)\n         )\n       )\n       (mean_module): ConstantMean()\n       (covar_module): ScaleKernel(\n         (base_kernel): MaternKernel(\n           (lengthscale_prior): GammaPrior()\n           (raw_lengthscale_constraint): Positive()\n         )\n         (outputscale_prior): GammaPrior()\n         (raw_outputscale_constraint): Positive()\n       )\n       (input_transform): Normalize()\n       (outcome_transform): Standardize()\n     )\n   )\n   (likelihood): LikelihoodList(\n     (likelihoods): ModuleList(\n       (0): _GaussianLikelihoodBase(\n         (noise_covar): HeteroskedasticNoise(\n           (noise_model): SingleTaskGP(\n             (likelihood): GaussianLikelihood(\n               (noise_covar): HomoskedasticNoise(\n                 (noise_prior): SmoothedBoxPrior()\n                 (raw_noise_constraint): GreaterThan(1.000E-04)\n               )\n             )\n             (mean_module): ConstantMean()\n             (covar_module): ScaleKernel(\n               (base_kernel): MaternKernel(\n                 (lengthscale_prior): GammaPrior()\n                 (raw_lengthscale_constraint): Positive()\n               )\n               (outputscale_prior): GammaPrior()\n               (raw_outputscale_constraint): Positive()\n             )\n             (outcome_transform): Log()\n             (input_transform): Normalize()\n           )\n           (_noise_constraint): GreaterThan(1.000E-04)\n         )\n       )\n     )\n   )\n ),\n 'n_monte_carlo_samples': 128,\n 'turbo_controller': None,\n 'use_cuda': False,\n 'gp_constructor': {'name': 'standard',\n  'use_low_noise_prior': True,\n  'covar_modules': {},\n  'mean_modules': {},\n  'trainable_mean_keys': [],\n  'transform_inputs': True},\n 'numerical_optimizer': {'name': 'LBFGS',\n  'n_restarts': 20,\n  'max_iter': 2000,\n  'max_time': None},\n 'max_travel_distances': None,\n 'fixed_features': None,\n 'computation_time':    training  acquisition_optimization\n 0  0.360403                  0.030050\n 1  0.381346                  0.062795\n 2  0.440578                  0.039208\n 3  0.465335                  0.034649\n 4  0.322201                  0.040909,\n 'log_transform_acquisition_function': False,\n 'n_interpolate_points': None,\n 'n_candidates': 1,\n 'beta': 2.0}</pre> In\u00a0[8]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/#bayesian-optimization-with-heteroskedastic-noise-gp-modeling","title":"Bayesian Optimization with Heteroskedastic Noise GP Modeling\u00b6","text":"<p>In this tutorial we demonstrate the use of Xopt to preform Bayesian Optimization on a simple test problem. The problem exibits non-uniform (heteroskedastic) noise which we account for in the GP model. This requires explicit specification of the measurement variance.</p>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize the sin function in the domian [0,2*pi]. Note that the function used to evaluate the objective function takes a dictionary as input and returns a dictionary as the output.</p>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/#specifying-measurement-variance","title":"Specifying measurement variance\u00b6","text":"<p>We specify variance in the objective function by appending <code>_var</code> to it. This info will collected by the model constructor to make a heteroskedastic model.</p>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Upper Confidence Bound acquisition function to perform Bayesian Optimization.</p>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>To perform optimization we simply call <code>X.step()</code> in a loop. This allows us to do intermediate tasks in between optimization steps, such as examining the model and acquisition function at each step (as we demonstrate here).</p>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/#getting-the-optimization-result","title":"Getting the optimization result\u00b6","text":"<p>To get the best point (without evaluating it) we ask the generator to predict the optimum based on the posterior mean.</p>"},{"location":"examples/single_objective_bayes_opt/heteroskedastic_noise_tutorial/#customizing-optimization","title":"Customizing optimization\u00b6","text":"<p>Each generator has a set of options that can be modified to effect optimization behavior</p>"},{"location":"examples/single_objective_bayes_opt/interpolate_tutorial/","title":"Interpolated optimization","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x1\": [-1, 1], \"x2\": [-1, 1]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> from xopt.vocs import VOCS  # define variables and function objectives vocs = VOCS(     variables={\"x1\": [-1, 1], \"x2\": [-1, 1]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre># define a test function to optimize\n\ndef sphere_function(input_dict):\n    return {\"f\": input_dict[\"x1\"]**2 + input_dict[\"x2\"]**2}\n</pre> # define a test function to optimize  def sphere_function(input_dict):     return {\"f\": input_dict[\"x1\"]**2 + input_dict[\"x2\"]**2} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import ExpectedImprovementGenerator\nfrom xopt import Xopt\n\n# define a generator that uses 5 interpolation points during sampling\ngenerator = ExpectedImprovementGenerator(vocs=vocs, n_interpolate_points=5)\ngenerator.gp_constructor.use_low_noise_prior = True\n\n\nevaluator = Evaluator(function=sphere_function)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import ExpectedImprovementGenerator from xopt import Xopt  # define a generator that uses 5 interpolation points during sampling generator = ExpectedImprovementGenerator(vocs=vocs, n_interpolate_points=5) generator.gp_constructor.use_low_noise_prior = True   evaluator = Evaluator(function=sphere_function) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre># call X.random_evaluate() to generate + evaluate initial points\nX.random_evaluate(2)\n\n# inspect the gathered data\nX.data\n</pre> # call X.random_evaluate() to generate + evaluate initial points X.random_evaluate(2)  # inspect the gathered data X.data Out[4]: x1 x2 f xopt_runtime xopt_error 0 -0.160448 0.210720 0.070146 0.000004 False 1 -0.391007 0.594561 0.506389 0.000001 False In\u00a0[5]: Copied! <pre>X.generator.train_model()\nX.generator.visualize_model(n_grid=50)\n\nn_steps = 5\nfor i in range(n_steps):\n    print(i)\n    # do the optimization step\n    X.step()\n\n    # train the model and visualize\n    X.generator.train_model()\n    fig,ax = X.generator.visualize_model(n_grid=50)\n\n    # add the ground truth minimum location\n    for a in ax.flatten()[:-1]:\n        a.plot(0,0,\"x\",c=\"red\",ms=10)\n</pre> X.generator.train_model() X.generator.visualize_model(n_grid=50)  n_steps = 5 for i in range(n_steps):     print(i)     # do the optimization step     X.step()      # train the model and visualize     X.generator.train_model()     fig,ax = X.generator.visualize_model(n_grid=50)      # add the ground truth minimum location     for a in ax.flatten()[:-1]:         a.plot(0,0,\"x\",c=\"red\",ms=10)  <pre>0\n</pre> <pre>1\n</pre> <pre>2\n</pre> <pre>3\n</pre> <pre>4\n</pre> In\u00a0[6]: Copied! <pre># access the collected data\nX.data\n</pre> # access the collected data X.data Out[6]: x1 x2 f xopt_runtime xopt_error 0 -0.160448 0.210720 7.014645e-02 3.547000e-06 False 1 -0.391007 0.594561 5.063891e-01 1.132000e-06 False 2 -0.283536 0.415639 2.531482e-01 3.797000e-06 False 3 -0.176064 0.236717 8.703361e-02 1.213000e-06 False 4 -0.068592 0.057796 8.045230e-03 1.232000e-06 False 5 0.038880 -0.121126 1.618309e-02 8.710000e-07 False 6 0.146351 -0.300047 1.114472e-01 7.919999e-07 False 7 0.038546 -0.311617 9.859098e-02 3.416000e-06 False 8 -0.069260 -0.323187 1.092465e-01 9.619999e-07 False 9 -0.177065 -0.334756 1.434138e-01 8.119999e-07 False 10 -0.284870 -0.346326 2.010928e-01 7.920000e-07 False 11 -0.392676 -0.357895 2.822836e-01 6.119999e-07 False 12 -0.276294 -0.249372 1.385246e-01 3.006000e-06 False 13 -0.159911 -0.140849 4.540997e-02 1.102000e-06 False 14 -0.043529 -0.032325 2.939691e-03 8.810000e-07 False 15 0.072854 0.076198 1.111376e-02 9.720001e-07 False 16 0.189236 0.184721 6.993218e-02 7.620000e-07 False 17 0.351389 0.347777 2.444229e-01 2.826000e-06 False 18 0.513542 0.510833 5.246750e-01 8.720000e-07 False 19 0.675694 0.673888 9.106886e-01 6.210000e-07 False 20 0.837847 0.836944 1.402464e+00 6.220000e-07 False 21 1.000000 1.000000 2.000000e+00 5.110001e-07 False 22 0.799914 0.799972 1.279817e+00 3.146000e-06 False 23 0.599828 0.599944 7.197257e-01 8.910000e-07 False 24 0.399741 0.399916 3.197257e-01 5.710000e-07 False 25 0.199655 0.199888 7.981722e-02 5.610001e-07 False 26 -0.000431 -0.000140 2.056280e-07 1.022000e-06 False In\u00a0[7]: Copied! <pre>X.generator.get_optimum()\n</pre> X.generator.get_optimum() Out[7]: x1 x2 0 -0.001425 -0.000049"},{"location":"examples/single_objective_bayes_opt/interpolate_tutorial/#bayesian-optimization-w-interpolated-samples","title":"Bayesian Optimization w/ Interpolated Samples\u00b6","text":"<p>In some situations, the process of evaluating objectives and constraints consumes fewer resources compared to the computational demands associated with Bayesian Optimization (BO) decision-making. Particularly, when making subtle changes to parameters during optimization, the cost of assessing objectives becomes notably more affordable.</p> <p>Consider a practical example: the optimization of magnet parameters in an accelerator with the goal of either minimizing the beam spot size on a screen or maximizing the Free Electron Laser (FEL) pulse energy. In such cases, where adjustments to accelerator parameters are frequent, it proves beneficial to augment the dataset by implementing multiple smaller changes to the parameters. These adjustments are followed by quick measurements of the objective in between the parameter changes guided by BO.</p> <p>This approach, although introducing a slight slowdown to the BO process due to the addition of extra training points, it will expedite convergence for most problems. It offers a more efficient alternative compared to the strategy of measuring the same points multiple times in noisy environments. The rationale here is that the exploration of a broader parameter space through numerous smaller changes enhances the overall understanding of the system's behavior, potentially leading to a more efficient and effective optimization process.</p> <p>NOTE: This only works for serialized problems.</p> <p>WARNING: The interpolated points may violate constraints! Do not use this feature in problems where safety is critical.</p>"},{"location":"examples/single_objective_bayes_opt/interpolate_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize the sphere function.</p>"},{"location":"examples/single_objective_bayes_opt/interpolate_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Upper Confidence Bound acquisition function to perform Bayesian Optimization. We additionally specify <code>n_interpolate_points</code> to be non-zero such that the generator proposes interpolated points during generation.</p>"},{"location":"examples/single_objective_bayes_opt/interpolate_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/interpolate_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>To perform optimization we simply call <code>X.step()</code> in a loop. This allows us to do intermediate tasks in between optimization steps, such as examining the model and acquisition function at each step (as we demonstrate here).</p>"},{"location":"examples/single_objective_bayes_opt/interpolate_tutorial/#getting-the-optimization-result","title":"Getting the optimization result\u00b6","text":"<p>To get the best point (without evaluating it) we ask the generator to predict the optimum based on the posterior mean.</p>"},{"location":"examples/single_objective_bayes_opt/log_transformed_tutorial/","title":"Log-transformed optimization","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\nimport math\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> from xopt.vocs import VOCS import math  # define variables and function objectives vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\ndef sin_function(input_dict):\n    return {\"f\": np.sin(input_dict[\"x\"])}\n</pre> # define a test function to optimize import numpy as np  def sin_function(input_dict):     return {\"f\": np.sin(input_dict[\"x\"])} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import ExpectedImprovementGenerator\nfrom xopt import Xopt\n\nevaluator = Evaluator(function=sin_function)\ngenerator = ExpectedImprovementGenerator(vocs=vocs, log_transform_acquisition_function=True)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import ExpectedImprovementGenerator from xopt import Xopt  evaluator = Evaluator(function=sin_function) generator = ExpectedImprovementGenerator(vocs=vocs, log_transform_acquisition_function=True) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre># call X.random_evaluate() to generate + evaluate 3 initial points\nX.random_evaluate(2)\n\n# inspect the gathered data\nX.data\n</pre> # call X.random_evaluate() to generate + evaluate 3 initial points X.random_evaluate(2)  # inspect the gathered data X.data Out[4]: x f xopt_runtime xopt_error 0 4.521258 -0.981790 0.000017 False 1 0.033319 0.033313 0.000002 False In\u00a0[5]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n\nn_steps = 5\n\n# test points for plotting\ntest_x = torch.linspace(*X.vocs.bounds.flatten(), 50).double()\n\nfor i in range(n_steps):\n    # get the Gaussian process model from the generator\n    model = X.generator.train_model()\n\n    # get acquisition function from generator\n    acq = X.generator.get_acquisition(model)\n\n    # calculate model posterior and acquisition function at each test point\n    # NOTE: need to add a dimension to the input tensor for evaluating the\n    # posterior and another for the acquisition function, see\n    # https://botorch.org/docs/batching for details\n    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n    # skipping calculations for backpropagation\n    with torch.no_grad():\n        posterior = model.posterior(test_x.unsqueeze(1))\n        acq_val = acq(test_x.reshape(-1, 1, 1))\n\n    # get mean function and confidence regions\n    mean = posterior.mean\n    l,u = posterior.mvn.confidence_region()\n\n    # plot model and acquisition function\n    fig,ax = plt.subplots(2, 1, sharex=\"all\")\n\n    # plot model posterior\n    ax[0].plot(test_x, mean, label=\"Posterior mean\")\n    ax[0].fill_between(test_x, l, u, alpha=0.25, label=\"Posterior confidence region\")\n\n    # add data to model plot\n    ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")\n\n    # plot true function\n    true_f = sin_function({\"x\": test_x})[\"f\"]\n    ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")\n\n    # add legend\n    ax[0].legend()\n\n    # plot acquisition function\n    ax[1].plot(test_x, acq_val.flatten())\n\n    ax[0].set_ylabel(\"f\")\n    ax[1].set_ylabel(r\"$log(\\alpha(x))$\")\n    ax[1].set_xlabel(\"x\")\n\n    # do the optimization step\n    X.step()\n</pre> import torch import matplotlib.pyplot as plt  n_steps = 5  # test points for plotting test_x = torch.linspace(*X.vocs.bounds.flatten(), 50).double()  for i in range(n_steps):     # get the Gaussian process model from the generator     model = X.generator.train_model()      # get acquisition function from generator     acq = X.generator.get_acquisition(model)      # calculate model posterior and acquisition function at each test point     # NOTE: need to add a dimension to the input tensor for evaluating the     # posterior and another for the acquisition function, see     # https://botorch.org/docs/batching for details     # NOTE: we use the `torch.no_grad()` environment to speed up computation by     # skipping calculations for backpropagation     with torch.no_grad():         posterior = model.posterior(test_x.unsqueeze(1))         acq_val = acq(test_x.reshape(-1, 1, 1))      # get mean function and confidence regions     mean = posterior.mean     l,u = posterior.mvn.confidence_region()      # plot model and acquisition function     fig,ax = plt.subplots(2, 1, sharex=\"all\")      # plot model posterior     ax[0].plot(test_x, mean, label=\"Posterior mean\")     ax[0].fill_between(test_x, l, u, alpha=0.25, label=\"Posterior confidence region\")      # add data to model plot     ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")      # plot true function     true_f = sin_function({\"x\": test_x})[\"f\"]     ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")      # add legend     ax[0].legend()      # plot acquisition function     ax[1].plot(test_x, acq_val.flatten())      ax[0].set_ylabel(\"f\")     ax[1].set_ylabel(r\"$log(\\alpha(x))$\")     ax[1].set_xlabel(\"x\")      # do the optimization step     X.step()  In\u00a0[6]: Copied! <pre># access the collected data\nX.data\n</pre> # access the collected data X.data Out[6]: x f xopt_runtime xopt_error 0 4.521258 -9.817900e-01 0.000017 False 1 0.033319 3.331270e-02 0.000002 False 2 6.283185 -2.449294e-16 0.000007 False 3 3.471621 -3.240698e-01 0.000007 False 4 4.909622 -9.806126e-01 0.000007 False 5 4.714321 -9.999981e-01 0.000007 False 6 4.713122 -9.999997e-01 0.000007 False In\u00a0[7]: Copied! <pre>X.generator.get_optimum()\n</pre> X.generator.get_optimum() Out[7]: x 0 4.712621 In\u00a0[8]: Copied! <pre>X.generator.dict()\n</pre> X.generator.dict() Out[8]: <pre>{'model': ModelListGP(\n   (models): ModuleList(\n     (0): SingleTaskGP(\n       (likelihood): GaussianLikelihood(\n         (noise_covar): HomoskedasticNoise(\n           (noise_prior): GammaPrior()\n           (raw_noise_constraint): GreaterThan(1.000E-04)\n         )\n       )\n       (mean_module): ConstantMean()\n       (covar_module): ScaleKernel(\n         (base_kernel): MaternKernel(\n           (lengthscale_prior): GammaPrior()\n           (raw_lengthscale_constraint): Positive()\n         )\n         (outputscale_prior): GammaPrior()\n         (raw_outputscale_constraint): Positive()\n       )\n       (outcome_transform): Standardize()\n       (input_transform): Normalize()\n     )\n   )\n   (likelihood): LikelihoodList(\n     (likelihoods): ModuleList(\n       (0): GaussianLikelihood(\n         (noise_covar): HomoskedasticNoise(\n           (noise_prior): GammaPrior()\n           (raw_noise_constraint): GreaterThan(1.000E-04)\n         )\n       )\n     )\n   )\n ),\n 'n_monte_carlo_samples': 128,\n 'turbo_controller': None,\n 'use_cuda': False,\n 'gp_constructor': {'name': 'standard',\n  'use_low_noise_prior': True,\n  'covar_modules': {},\n  'mean_modules': {},\n  'trainable_mean_keys': [],\n  'transform_inputs': True},\n 'numerical_optimizer': {'name': 'LBFGS',\n  'n_restarts': 20,\n  'max_iter': 2000,\n  'max_time': None},\n 'max_travel_distances': None,\n 'fixed_features': None,\n 'computation_time':    training  acquisition_optimization\n 0  0.065896                  0.027058\n 1  0.073930                  0.109595\n 2  0.072786                  0.096491\n 3  0.059170                  0.099979\n 4  0.073808                  0.072278,\n 'log_transform_acquisition_function': True,\n 'n_interpolate_points': None,\n 'n_candidates': 1}</pre> In\u00a0[8]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/log_transformed_tutorial/#log-transformed-acqusition-functions","title":"Log-Transformed Acqusition Functions\u00b6","text":"<p>A common problem when using acquisition functions is vanishing gradients in regions of parameter space that are nearly zero. This happens often when using Expected Improvement or Constraint probability weighting. To address this, [1] suggests taking the log of the acquisition function to make numerical optimization more robust. We demonstrate this feature in Xopt. Compare acquisition function plots shown here to those in the constrained BO tutorial.</p> <p>[1] Ament, Sebastian, et al. \"Unexpected improvements to expected improvement for bayesian optimization.\" arXiv preprint arXiv:2310.20708 (2023).</p>"},{"location":"examples/single_objective_bayes_opt/log_transformed_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize the sin function in the domian [0,2*pi]. Note that the function used to evaluate the objective function takes a dictionary as input and returns a dictionary as the output.</p>"},{"location":"examples/single_objective_bayes_opt/log_transformed_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Upper Confidence Bound acquisition function to perform Bayesian Optimization.</p>"},{"location":"examples/single_objective_bayes_opt/log_transformed_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/log_transformed_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>To perform optimization we simply call <code>X.step()</code> in a loop. This allows us to do intermediate tasks in between optimization steps, such as examining the model and acquisition function at each step (as we demonstrate here).</p>"},{"location":"examples/single_objective_bayes_opt/log_transformed_tutorial/#getting-the-optimization-result","title":"Getting the optimization result\u00b6","text":"<p>To get the best point (without evaluating it) we ask the generator to predict the optimum based on the posterior mean.</p>"},{"location":"examples/single_objective_bayes_opt/log_transformed_tutorial/#customizing-optimization","title":"Customizing optimization\u00b6","text":"<p>Each generator has a set of options that can be modified to effect optimization behavior</p>"},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/","title":"Multi-fidelity BO","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nN_MC_SAMPLES = 1 if SMOKE_TEST else 128\nN_RESTARTS = 1 if SMOKE_TEST else 20\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\nimport pandas as pd\nimport torch\n\ndef test_function(input_dict):\n    x = input_dict[\"x\"]\n    s = input_dict[\"s\"]\n    return {\"f\":np.sin(x + (1.0 - s)) * np.exp((-s+1)/2)}\n\n\n# define vocs\nfrom xopt import VOCS\nvocs = VOCS(\n    variables={\n        \"x\": [0, 2*math.pi],\n    },\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") N_MC_SAMPLES = 1 if SMOKE_TEST else 128 N_RESTARTS = 1 if SMOKE_TEST else 20  import matplotlib.pyplot as plt import numpy as np import math  import pandas as pd import torch  def test_function(input_dict):     x = input_dict[\"x\"]     s = input_dict[\"s\"]     return {\"f\":np.sin(x + (1.0 - s)) * np.exp((-s+1)/2)}   # define vocs from xopt import VOCS vocs = VOCS(     variables={         \"x\": [0, 2*math.pi],     },     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre>test_x = np.linspace(*vocs.bounds, 1000)\nfidelities = [0.0,0.5,1.0]\n\nfig,ax = plt.subplots()\nfor ele in fidelities:\n    f = test_function({\"x\":test_x, \"s\":ele})[\"f\"]\n    ax.plot(test_x, f,label=f\"s:{ele}\")\n\nax.legend()\n</pre> test_x = np.linspace(*vocs.bounds, 1000) fidelities = [0.0,0.5,1.0]  fig,ax = plt.subplots() for ele in fidelities:     f = test_function({\"x\":test_x, \"s\":ele})[\"f\"]     ax.plot(test_x, f,label=f\"s:{ele}\")  ax.legend() Out[2]: <pre>&lt;matplotlib.legend.Legend at 0x7fc79b225760&gt;</pre> In\u00a0[3]: Copied! <pre># create xopt object\nfrom xopt.generators.bayesian import MultiFidelityGenerator\nfrom xopt import Evaluator, Xopt\n\n# get and modify default generator options\ngenerator = MultiFidelityGenerator(vocs=vocs)\n\n# specify a custom cost function based on the fidelity parameter\ngenerator.cost_function = lambda s: s + 0.001\n\ngenerator.numerical_optimizer.n_restarts = N_RESTARTS\ngenerator.n_monte_carlo_samples = N_MC_SAMPLES\n\n# pass options to the generator\nevaluator = Evaluator(function=test_function)\n\nX = Xopt(vocs=vocs, generator=generator, evaluator=evaluator)\nX\n</pre> # create xopt object from xopt.generators.bayesian import MultiFidelityGenerator from xopt import Evaluator, Xopt  # get and modify default generator options generator = MultiFidelityGenerator(vocs=vocs)  # specify a custom cost function based on the fidelity parameter generator.cost_function = lambda s: s + 0.001  generator.numerical_optimizer.n_restarts = N_RESTARTS generator.n_monte_carlo_samples = N_MC_SAMPLES  # pass options to the generator evaluator = Evaluator(function=test_function)  X = Xopt(vocs=vocs, generator=generator, evaluator=evaluator) X Out[3]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: __main__.test_function\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: multi_fidelity\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  reference_point:\n    f: 100.0\n    s: 0.0\n  supports_batch_generation: true\n  supports_multi_objective: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints: {}\n  objectives:\n    f: MINIMIZE\n    s: MAXIMIZE\n  observables: []\n  variables:\n    s:\n    - 0\n    - 1\n    x:\n    - 0.0\n    - 6.283185307179586\n</pre> In\u00a0[4]: Copied! <pre># evaluate initial points at mixed fidelities to seed optimization\nX.evaluate_data(pd.DataFrame({\n    \"x\":[math.pi / 4, math.pi / 2., math.pi],\"s\":[0.0, 0.25, 0.0]\n}))\n</pre> # evaluate initial points at mixed fidelities to seed optimization X.evaluate_data(pd.DataFrame({     \"x\":[math.pi / 4, math.pi / 2., math.pi],\"s\":[0.0, 0.25, 0.0] })) Out[4]: x s f xopt_runtime xopt_error 0 0.785398 0.00 1.610902 0.000014 False 1 1.570796 0.25 1.064601 0.000003 False 2 3.141593 0.00 -1.387351 0.000002 False In\u00a0[5]: Copied! <pre># get the total cost of previous observations based on the cost function\nX.generator.calculate_total_cost()\n</pre> # get the total cost of previous observations based on the cost function X.generator.calculate_total_cost() Out[5]: <pre>tensor(0.2530, dtype=torch.float64)</pre> In\u00a0[6]: Copied! <pre># run optimization until the cost budget is exhausted\n# we subtract one unit to make sure we don't go over our eval budget\nbudget = 10\nwhile X.generator.calculate_total_cost() &lt; budget - 1:\n    X.step()\n    print(f\"n_samples: {len(X.data)} \"\n          f\"budget used: {X.generator.calculate_total_cost():.4} \"\n          f\"hypervolume: {X.generator.calculate_hypervolume():.4}\")\n</pre> # run optimization until the cost budget is exhausted # we subtract one unit to make sure we don't go over our eval budget budget = 10 while X.generator.calculate_total_cost() &lt; budget - 1:     X.step()     print(f\"n_samples: {len(X.data)} \"           f\"budget used: {X.generator.calculate_total_cost():.4} \"           f\"hypervolume: {X.generator.calculate_hypervolume():.4}\")  <pre>n_samples: 4 budget used: 0.6253 hypervolume: 36.72\n</pre> <pre>n_samples: 5 budget used: 1.184 hypervolume: 55.23\n</pre> <pre>n_samples: 6 budget used: 1.959 hypervolume: 76.79\n</pre> <pre>n_samples: 7 budget used: 2.96 hypervolume: 100.0\n</pre> <pre>n_samples: 8 budget used: 3.961 hypervolume: 100.7\n</pre> <pre>n_samples: 9 budget used: 4.962 hypervolume: 100.9\n</pre> <pre>n_samples: 10 budget used: 5.963 hypervolume: 100.9\n</pre> <pre>n_samples: 11 budget used: 6.964 hypervolume: 100.9\n</pre> <pre>n_samples: 12 budget used: 7.255 hypervolume: 101.0\n</pre> <pre>n_samples: 13 budget used: 7.819 hypervolume: 101.1\n</pre> <pre>n_samples: 14 budget used: 8.581 hypervolume: 101.2\n</pre> <pre>n_samples: 15 budget used: 8.732 hypervolume: 101.2\n</pre> <pre>n_samples: 16 budget used: 9.733 hypervolume: 101.2\n</pre> In\u00a0[7]: Copied! <pre>X.data\n</pre> X.data Out[7]: x s f xopt_runtime xopt_error 0 0.785398 0.000000 1.610902e+00 0.000014 False 1 1.570796 0.250000 1.064601e+00 0.000003 False 2 3.141593 0.000000 -1.387351e+00 0.000002 False 3 1.476014 0.371257 1.178775e+00 0.000011 False 4 1.706002 0.558164 1.045271e+00 0.000012 False 5 2.151211 0.773879 7.748358e-01 0.000012 False 6 3.144071 1.000000 -2.478116e-03 0.000011 False 7 5.465887 1.000000 -7.293002e-01 0.000012 False 8 4.317866 1.000000 -9.231801e-01 0.000011 False 9 0.000000 1.000000 0.000000e+00 0.000011 False 10 1.270579 1.000000 9.552724e-01 0.000011 False 11 4.326236 0.289770 -1.352096e+00 0.000011 False 12 4.290795 0.563172 -1.243957e+00 0.000012 False 13 4.534718 0.760350 -1.125135e+00 0.000012 False 14 3.802442 0.150627 -1.526307e+00 0.000011 False 15 6.283185 1.000000 -2.449294e-16 0.000016 False In\u00a0[8]: Copied! <pre># augment the bounds to add the fidelity parameter\nbounds = X.generator.vocs.bounds[::-1]\n\nmodel = X.generator.model\n\n# create mesh\nn = 50\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nxx, yy = xx.numpy(), yy.numpy()\n\nacq_func = X.generator.get_acquisition(model)\n\nvariable_names = X.generator.vocs.variable_names\nwith torch.no_grad():\n    print(pts.shape)\n    # get the model posterior\n    post = model.posterior(pts.unsqueeze(-2))\n    f_mean = post.mean[..., 0]\n    s_mean = post.mean[..., 1]\n\n\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax0 = plt.subplots()\n    c = ax0.pcolor(xx, yy, f_mean.reshape(n, n))\n    fig.colorbar(c)\n    ax0.set_title(\"f-Mean prediction\")\n\n    fig, ax1 = plt.subplots()\n    c = ax1.pcolor(xx, yy, s_mean.reshape(n, n))\n    fig.colorbar(c)\n    ax1.set_title(\"s-Mean prediction\")\n\n    fig, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n    fig.colorbar(c)\n    ax2.set_title(\"Acquisition function\")\n\n    X.data.plot(x=variable_names[0], y=variable_names[1],ax=ax2,style=\"oC1\")\n\n    # mark the next observation\n    next_pt = pts[torch.argmax(acq)]\n    ax2.plot(*next_pt,\"*r\",ms=10)\n\n    # mark the optimum at the max fidelity\n    best_loc = [1.0, 1.5*np.pi]\n    ax2.plot(*best_loc, \"*\",c=\"C4\")\n\n    for a in [ax0,ax1,ax2]:\n        a.set_xlabel(variable_names[0])\n        a.set_ylabel(variable_names[1])\n</pre> # augment the bounds to add the fidelity parameter bounds = X.generator.vocs.bounds[::-1]  model = X.generator.model  # create mesh n = 50 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  xx, yy = xx.numpy(), yy.numpy()  acq_func = X.generator.get_acquisition(model)  variable_names = X.generator.vocs.variable_names with torch.no_grad():     print(pts.shape)     # get the model posterior     post = model.posterior(pts.unsqueeze(-2))     f_mean = post.mean[..., 0]     s_mean = post.mean[..., 1]       acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax0 = plt.subplots()     c = ax0.pcolor(xx, yy, f_mean.reshape(n, n))     fig.colorbar(c)     ax0.set_title(\"f-Mean prediction\")      fig, ax1 = plt.subplots()     c = ax1.pcolor(xx, yy, s_mean.reshape(n, n))     fig.colorbar(c)     ax1.set_title(\"s-Mean prediction\")      fig, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")     fig.colorbar(c)     ax2.set_title(\"Acquisition function\")      X.data.plot(x=variable_names[0], y=variable_names[1],ax=ax2,style=\"oC1\")      # mark the next observation     next_pt = pts[torch.argmax(acq)]     ax2.plot(*next_pt,\"*r\",ms=10)      # mark the optimum at the max fidelity     best_loc = [1.0, 1.5*np.pi]     ax2.plot(*best_loc, \"*\",c=\"C4\")      for a in [ax0,ax1,ax2]:         a.set_xlabel(variable_names[0])         a.set_ylabel(variable_names[1])  <pre>torch.Size([2500, 2])\n</pre> <pre>/usr/share/miniconda3/envs/xopt-dev/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706712279749/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/usr/share/miniconda3/envs/xopt-dev/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n  warnings.warn(\n</pre> In\u00a0[9]: Copied! <pre>X.data.plot(x=\"f\", y=\"s\", style=\"o-\")\n</pre> X.data.plot(x=\"f\", y=\"s\", style=\"o-\") Out[9]: <pre>&lt;Axes: xlabel='f'&gt;</pre> In\u00a0[10]: Copied! <pre>X.data\n</pre> X.data Out[10]: x s f xopt_runtime xopt_error 0 0.785398 0.000000 1.610902e+00 0.000014 False 1 1.570796 0.250000 1.064601e+00 0.000003 False 2 3.141593 0.000000 -1.387351e+00 0.000002 False 3 1.476014 0.371257 1.178775e+00 0.000011 False 4 1.706002 0.558164 1.045271e+00 0.000012 False 5 2.151211 0.773879 7.748358e-01 0.000012 False 6 3.144071 1.000000 -2.478116e-03 0.000011 False 7 5.465887 1.000000 -7.293002e-01 0.000012 False 8 4.317866 1.000000 -9.231801e-01 0.000011 False 9 0.000000 1.000000 0.000000e+00 0.000011 False 10 1.270579 1.000000 9.552724e-01 0.000011 False 11 4.326236 0.289770 -1.352096e+00 0.000011 False 12 4.290795 0.563172 -1.243957e+00 0.000012 False 13 4.534718 0.760350 -1.125135e+00 0.000012 False 14 3.802442 0.150627 -1.526307e+00 0.000011 False 15 6.283185 1.000000 -2.449294e-16 0.000016 False In\u00a0[11]: Copied! <pre># get optimal value at max fidelity, note that the actual maximum is 4.71\nX.generator.get_optimum().to_dict()\n</pre> # get optimal value at max fidelity, note that the actual maximum is 4.71 X.generator.get_optimum().to_dict() Out[11]: <pre>{'x': {0: 4.600375090344051}, 's': {0: 1.0}}</pre>"},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/#multi-fidelity-bo","title":"Multi-fidelity BO\u00b6","text":"<p>Here we demonstrate how Multi-Fidelity Bayesian Optimization can be used to reduce the computational cost of optimization by using lower fidelity surrogate models. The goal is to learn functional dependance of the objective on input variables at low fidelities (which are cheap to compute) and use that information to quickly find the best objective value at higher fidelities (which are more expensive to compute). This assumes that there is some learnable correlation between the objective values at different fidelities.</p> <p>Xopt implements the MOMF (https://botorch.org/tutorials/Multi_objective_multi_fidelity_BO) algorithm which can be used to solve both single (this notebook) and multi-objective (see multi-objective BO section) multi-fidelity problems. Under the hood this algorithm attempts to solve a multi-objective optimization problem, where one objective is the function objective and the other is a simple fidelity objective, weighted by the <code>cost_function</code> of evaluating the objective at a given fidelity.</p>"},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/#plot-the-test-function-in-input-fidelity-space","title":"plot the test function in input + fidelity space\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/#plot-the-model-prediction-and-acquisition-function-inside-the-optimization-space","title":"Plot the model prediction and acquisition function inside the optimization space\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/#plot-the-pareto-front","title":"Plot the Pareto front\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/noisy_bo_tutorial/","title":"Noisy bo tutorial","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\nimport math\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> from xopt.vocs import VOCS import math  # define variables and function objectives vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\ndef sin_function(input_dict):\n    # return multiple noisy measurements\n    return {\"f\": np.sin(input_dict[\"x\"]) + 0.5*np.random.randn(5),\n            \"dummy\": np.random.randn(5)}\n</pre> # define a test function to optimize import numpy as np  def sin_function(input_dict):     # return multiple noisy measurements     return {\"f\": np.sin(input_dict[\"x\"]) + 0.5*np.random.randn(5),             \"dummy\": np.random.randn(5)} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt.generators.bayesian.models.standard import StandardModelConstructor\nfrom xopt import Xopt\n\nevaluator = Evaluator(function=sin_function)\ngp_constructor = StandardModelConstructor(use_low_noise_prior=False)\ngenerator = UpperConfidenceBoundGenerator(vocs=vocs, gp_constructor=gp_constructor)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt.generators.bayesian.models.standard import StandardModelConstructor from xopt import Xopt  evaluator = Evaluator(function=sin_function) gp_constructor = StandardModelConstructor(use_low_noise_prior=False) generator = UpperConfidenceBoundGenerator(vocs=vocs, gp_constructor=gp_constructor) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre># call X.random_evaluate() to generate + evaluate 3 initial points\nX.random_evaluate(2)\n\n# inspect the gathered data\nX.data\n</pre> # call X.random_evaluate() to generate + evaluate 3 initial points X.random_evaluate(2)  # inspect the gathered data X.data Out[4]: x f dummy xopt_runtime xopt_error 0 4.619338 -0.011513 -0.240742 0.000059 False 1 4.619338 -1.435161 0.829753 0.000059 False 2 4.619338 -1.476094 0.164653 0.000059 False 3 4.619338 -0.778552 -1.198242 0.000059 False 4 4.619338 -1.260215 0.00147 0.000059 False 5 0.759449 0.098461 -0.324 0.000148 False 6 0.759449 1.567489 -0.311139 0.000148 False 7 0.759449 0.164153 0.022538 0.000148 False 8 0.759449 0.566114 -1.265313 0.000148 False 9 0.759449 0.379249 1.293033 0.000148 False In\u00a0[5]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n\nn_steps = 5\n\n# test points for plotting\ntest_x = torch.linspace(*X.vocs.bounds.flatten(), 50).double()\n\nfor i in range(n_steps):\n    # get the Gaussian process model from the generator\n    model = X.generator.train_model()\n\n    # get acquisition function from generator\n    acq = X.generator.get_acquisition(model)\n\n    # calculate model posterior and acquisition function at each test point\n    # NOTE: need to add a dimension to the input tensor for evaluating the\n    # posterior and another for the acquisition function, see\n    # https://botorch.org/docs/batching for details\n    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n    # skipping calculations for backpropagation\n    with torch.no_grad():\n        posterior = model.posterior(test_x.unsqueeze(1))\n        acq_val = acq(test_x.reshape(-1, 1, 1))\n\n    # get mean function and confidence regions\n    mean = posterior.mean\n    l,u = posterior.mvn.confidence_region()\n\n    # plot model and acquisition function\n    fig,ax = plt.subplots(2, 1, sharex=\"all\")\n\n    # plot model posterior\n    ax[0].plot(test_x, mean, label=\"Posterior mean\")\n    ax[0].fill_between(test_x, l, u, alpha=0.25, label=\"Posterior confidence region\")\n\n    # add data to model plot\n    ax[0].plot(X.data[\"x\"], X.data[\"f\"], \"C1+\", label=\"Training data\")\n\n    # plot true function\n    true_f = np.sin(test_x)\n    ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")\n\n    # add legend\n    ax[0].legend()\n\n    # plot acquisition function\n    ax[1].plot(test_x, acq_val.flatten())\n\n    ax[0].set_ylabel(\"f\")\n    ax[1].set_ylabel(r\"$\\alpha(x)$\")\n    ax[1].set_xlabel(\"x\")\n\n    # do the optimization step\n    X.step()\n</pre> import torch import matplotlib.pyplot as plt  n_steps = 5  # test points for plotting test_x = torch.linspace(*X.vocs.bounds.flatten(), 50).double()  for i in range(n_steps):     # get the Gaussian process model from the generator     model = X.generator.train_model()      # get acquisition function from generator     acq = X.generator.get_acquisition(model)      # calculate model posterior and acquisition function at each test point     # NOTE: need to add a dimension to the input tensor for evaluating the     # posterior and another for the acquisition function, see     # https://botorch.org/docs/batching for details     # NOTE: we use the `torch.no_grad()` environment to speed up computation by     # skipping calculations for backpropagation     with torch.no_grad():         posterior = model.posterior(test_x.unsqueeze(1))         acq_val = acq(test_x.reshape(-1, 1, 1))      # get mean function and confidence regions     mean = posterior.mean     l,u = posterior.mvn.confidence_region()      # plot model and acquisition function     fig,ax = plt.subplots(2, 1, sharex=\"all\")      # plot model posterior     ax[0].plot(test_x, mean, label=\"Posterior mean\")     ax[0].fill_between(test_x, l, u, alpha=0.25, label=\"Posterior confidence region\")      # add data to model plot     ax[0].plot(X.data[\"x\"], X.data[\"f\"], \"C1+\", label=\"Training data\")      # plot true function     true_f = np.sin(test_x)     ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")      # add legend     ax[0].legend()      # plot acquisition function     ax[1].plot(test_x, acq_val.flatten())      ax[0].set_ylabel(\"f\")     ax[1].set_ylabel(r\"$\\alpha(x)$\")     ax[1].set_xlabel(\"x\")      # do the optimization step     X.step()  In\u00a0[6]: Copied! <pre># access the collected data\nX.data\n</pre> # access the collected data X.data Out[6]: x f dummy xopt_runtime xopt_error 0 4.619338 -0.011513 -0.240742 0.000059 False 1 4.619338 -1.435161 0.829753 0.000059 False 2 4.619338 -1.476094 0.164653 0.000059 False 3 4.619338 -0.778552 -1.198242 0.000059 False 4 4.619338 -1.260215 0.00147 0.000059 False 5 0.759449 0.098461 -0.324 0.000148 False 6 0.759449 1.567489 -0.311139 0.000148 False 7 0.759449 0.164153 0.022538 0.000148 False 8 0.759449 0.566114 -1.265313 0.000148 False 9 0.759449 0.379249 1.293033 0.000148 False 10 6.283185 -0.379428 0.022686 0.000026 False 11 6.283185 -0.950938 1.781889 0.000026 False 12 6.283185 -0.217578 1.103808 0.000026 False 13 6.283185 -0.49596 0.664795 0.000026 False 14 6.283185 0.495998 0.102054 0.000026 False 15 3.518793 -0.039908 1.050478 0.000024 False 16 3.518793 -0.305294 -1.451714 0.000024 False 17 3.518793 -0.118421 -0.343997 0.000024 False 18 3.518793 0.237909 -0.975853 0.000024 False 19 3.518793 -0.252254 0.356771 0.000024 False 20 5.088307 0.497992 1.191994 0.000020 False 21 5.088307 -0.81322 0.479879 0.000020 False 22 5.088307 -0.590264 -0.02449 0.000020 False 23 5.088307 -0.194415 0.165368 0.000020 False 24 5.088307 -0.831174 1.86811 0.000020 False 25 4.650385 -1.00841 1.160027 0.000021 False 26 4.650385 -0.580665 0.59192 0.000021 False 27 4.650385 -1.230013 1.190219 0.000021 False 28 4.650385 -1.815493 -0.075334 0.000021 False 29 4.650385 -0.962873 -0.103079 0.000021 False 30 4.331677 -0.804609 0.160299 0.000019 False 31 4.331677 -0.381751 -0.097682 0.000019 False 32 4.331677 0.177958 0.100419 0.000019 False 33 4.331677 -0.983582 0.611655 0.000019 False 34 4.331677 0.090901 -0.912519 0.000019 False In\u00a0[7]: Copied! <pre>X.generator.get_optimum()\n</pre> X.generator.get_optimum() Out[7]: x 0 4.496265 In\u00a0[8]: Copied! <pre>X.generator.dict()\n</pre> X.generator.dict() Out[8]: <pre>{'model': ModelListGP(\n   (models): ModuleList(\n     (0): SingleTaskGP(\n       (likelihood): GaussianLikelihood(\n         (noise_covar): HomoskedasticNoise(\n           (noise_prior): GammaPrior()\n           (raw_noise_constraint): GreaterThan(1.000E-04)\n         )\n       )\n       (mean_module): ConstantMean()\n       (covar_module): ScaleKernel(\n         (base_kernel): MaternKernel(\n           (lengthscale_prior): GammaPrior()\n           (raw_lengthscale_constraint): Positive()\n         )\n         (outputscale_prior): GammaPrior()\n         (raw_outputscale_constraint): Positive()\n       )\n       (outcome_transform): Standardize()\n       (input_transform): Normalize()\n     )\n   )\n   (likelihood): LikelihoodList(\n     (likelihoods): ModuleList(\n       (0): GaussianLikelihood(\n         (noise_covar): HomoskedasticNoise(\n           (noise_prior): GammaPrior()\n           (raw_noise_constraint): GreaterThan(1.000E-04)\n         )\n       )\n     )\n   )\n ),\n 'n_monte_carlo_samples': 128,\n 'turbo_controller': None,\n 'use_cuda': False,\n 'gp_constructor': {'name': 'standard',\n  'use_low_noise_prior': False,\n  'covar_modules': {},\n  'mean_modules': {},\n  'trainable_mean_keys': [],\n  'transform_inputs': True},\n 'numerical_optimizer': {'name': 'LBFGS',\n  'n_restarts': 20,\n  'max_iter': 2000,\n  'max_time': None},\n 'max_travel_distances': None,\n 'fixed_features': None,\n 'computation_time':    training  acquisition_optimization\n 0  0.039017                  0.023556\n 1  0.041212                  0.030764\n 2  0.037589                  0.048881\n 3  0.034953                  0.040178\n 4  0.039619                  0.046087,\n 'log_transform_acquisition_function': False,\n 'n_interpolate_points': None,\n 'n_candidates': 1,\n 'beta': 2.0}</pre> In\u00a0[8]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/noisy_bo_tutorial/#basic-bayesian-optimization","title":"Basic Bayesian Optimization\u00b6","text":"<p>In this tutorial we demonstrate the use of Xopt to preform Bayesian Optimization on a simple test problem.</p>"},{"location":"examples/single_objective_bayes_opt/noisy_bo_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize the sin function in the domian [0,2*pi]. Note that the function used to evaluate the objective function takes a dictionary as input and returns a dictionary as the output.</p>"},{"location":"examples/single_objective_bayes_opt/noisy_bo_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Upper Confidence Bound acquisition function to perform Bayesian Optimization.</p>"},{"location":"examples/single_objective_bayes_opt/noisy_bo_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/noisy_bo_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>To perform optimization we simply call <code>X.step()</code> in a loop. This allows us to do intermediate tasks in between optimization steps, such as examining the model and acquisition function at each step (as we demonstrate here).</p>"},{"location":"examples/single_objective_bayes_opt/noisy_bo_tutorial/#getting-the-optimization-result","title":"Getting the optimization result\u00b6","text":"<p>To get the best point (without evaluating it) we ask the generator to predict the optimum based on the posterior mean.</p>"},{"location":"examples/single_objective_bayes_opt/noisy_bo_tutorial/#customizing-optimization","title":"Customizing optimization\u00b6","text":"<p>Each generator has a set of options that can be modified to effect optimization behavior</p>"},{"location":"examples/single_objective_bayes_opt/time_dependent_bo/","title":"Time dependent upper confidence bound","text":"In\u00a0[1]: Copied! <pre># set values if testing\nimport os\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nN_MC_SAMPLES = 1 if SMOKE_TEST else 128\nNUM_RESTARTS = 1 if SMOKE_TEST else 20\n\nfrom xopt.generators.bayesian.upper_confidence_bound import TDUpperConfidenceBoundGenerator\nfrom xopt.vocs import VOCS\nfrom xopt.evaluator import Evaluator\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> # set values if testing import os SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") N_MC_SAMPLES = 1 if SMOKE_TEST else 128 NUM_RESTARTS = 1 if SMOKE_TEST else 20  from xopt.generators.bayesian.upper_confidence_bound import TDUpperConfidenceBoundGenerator from xopt.vocs import VOCS from xopt.evaluator import Evaluator import warnings warnings.filterwarnings(\"ignore\") In\u00a0[2]: Copied! <pre># test evaluate function and vocs\nimport time\nfrom xopt import Xopt\n\nstart_time = time.time()\ndef f(inputs):\n    x_ = inputs[\"x\"]\n    current_time = time.time()\n    t_ = current_time - start_time\n    y_ = 5*(x_ - t_*1e-2)**2\n    return {\"y\":y_, \"time\":current_time}\n\nvariables = {\"x\":[-1,1]}\nobjectives = {\"y\": \"MINIMIZE\"}\n\nvocs = VOCS(variables=variables, objectives=objectives)\nprint(vocs)\n\nevaluator = Evaluator(function=f)\ngenerator = TDUpperConfidenceBoundGenerator(vocs=vocs)\ngenerator.added_time=1.0\ngenerator.beta = 2.0\ngenerator.n_monte_carlo_samples = N_MC_SAMPLES\ngenerator.numerical_optimizer.n_restarts = NUM_RESTARTS\n\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\nX\n</pre> # test evaluate function and vocs import time from xopt import Xopt  start_time = time.time() def f(inputs):     x_ = inputs[\"x\"]     current_time = time.time()     t_ = current_time - start_time     y_ = 5*(x_ - t_*1e-2)**2     return {\"y\":y_, \"time\":current_time}  variables = {\"x\":[-1,1]} objectives = {\"y\": \"MINIMIZE\"}  vocs = VOCS(variables=variables, objectives=objectives) print(vocs)  evaluator = Evaluator(function=f) generator = TDUpperConfidenceBoundGenerator(vocs=vocs) generator.added_time=1.0 generator.beta = 2.0 generator.n_monte_carlo_samples = N_MC_SAMPLES generator.numerical_optimizer.n_restarts = NUM_RESTARTS  X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) X <pre>variables={'x': [-1.0, 1.0]} constraints={} objectives={'y': 'MINIMIZE'} constants={} observables=[]\n</pre> Out[2]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: __main__.f\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  added_time: 1.0\n  beta: 2.0\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: time_dependent\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: time_dependent_upper_confidence_bound\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  target_prediction_time: null\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints: {}\n  objectives:\n    y: MINIMIZE\n  observables: []\n  variables:\n    x:\n    - -1.0\n    - 1.0\n</pre> In\u00a0[3]: Copied! <pre>X.random_evaluate(1)\n\nfor _ in range(20):\n    # note that in this example we can ignore warnings if computation time is greater\n    # than added time\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n        X.step()\n        time.sleep(0.1)\n\nprint(X.generator.generate(1))\n</pre> X.random_evaluate(1)  for _ in range(20):     # note that in this example we can ignore warnings if computation time is greater     # than added time     with warnings.catch_warnings():         warnings.filterwarnings(\"ignore\", category=RuntimeWarning)         X.step()         time.sleep(0.1)  print(X.generator.generate(1)) <pre>[{'x': 0.24034043536729968}]\n</pre> In\u00a0[4]: Copied! <pre>X.data\n</pre> X.data Out[4]: x y time xopt_runtime xopt_error 0 0.229101 0.261327 1.708726e+09 0.000017 False 1 -1.000000 5.105710 1.708726e+09 0.000006 False 2 1.000000 4.786874 1.708726e+09 0.000005 False 3 -0.036694 0.023990 1.708726e+09 0.000006 False 4 0.249797 0.212584 1.708726e+09 0.000006 False 5 0.030560 0.002897 1.708726e+09 0.000006 False 6 -0.170842 0.279655 1.708726e+09 0.000006 False 7 0.123094 0.010769 1.708726e+09 0.000007 False 8 0.003691 0.035299 1.708726e+09 0.000005 False 9 0.153595 0.015045 1.708726e+09 0.000005 False 10 0.078224 0.004977 1.708726e+09 0.000006 False 11 0.160882 0.008032 1.708726e+09 0.000005 False 12 0.084230 0.011330 1.708726e+09 0.000006 False 13 0.175963 0.005480 1.708726e+09 0.000005 False 14 0.124028 0.004459 1.708726e+09 0.000006 False 15 0.201264 0.006605 1.708726e+09 0.000005 False 16 0.143060 0.005410 1.708726e+09 0.000006 False 17 0.206223 0.001850 1.708726e+09 0.000005 False 18 0.165679 0.005228 1.708726e+09 0.000006 False 19 0.243277 0.005859 1.708726e+09 0.000005 False 20 0.195550 0.003008 1.708726e+09 0.000006 False In\u00a0[5]: Copied! <pre># plot model\nimport torch\nfrom matplotlib import pyplot as plt  # plot model predictions\ndata = X.data\n\nxbounds = generator.vocs.bounds\ntbounds = [data[\"time\"].min(), data[\"time\"].max()]\n\ndef gt(inpts):\n    return 5*(inpts[:,1] - (inpts[:,0] - start_time)*1e-2)**2\n\nmodel = X.generator.model\nn = 200\nt = torch.linspace(*tbounds, n, dtype=torch.double)\nx = torch.linspace(*xbounds.flatten(), n, dtype=torch.double)\ntt, xx = torch.meshgrid(t, x)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (tt, xx)]).double()\n\ntt, xx = tt.numpy(), xx.numpy()\n\n#NOTE: the model inputs are such that t is the last dimension\ngp_pts = torch.flip(pts, dims=[-1])\n\ngt_vals = gt(pts)\n\nwith torch.no_grad():\n    post = model.posterior(gp_pts)\n\n    mean = post.mean\n    std = torch.sqrt(post.variance)\n\n    fig, ax = plt.subplots()\n    ax.set_title(\"model mean\")\n    ax.set_xlabel(\"unix time\")\n    ax.set_ylabel(\"x\")\n    c = ax.pcolor(tt, xx, mean.reshape(n,n))\n    fig.colorbar(c)\n\n    fig2, ax2 = plt.subplots()\n    ax2.set_title(\"model uncertainty\")\n    ax2.set_xlabel(\"unix time\")\n    ax2.set_ylabel(\"x\")\n    c = ax2.pcolor(tt, xx, std.reshape(n,n))\n    fig2.colorbar(c)\n\n    ax.plot(data[\"time\"].to_numpy(), data[\"x\"].to_numpy(),\"oC1\")\n    ax2.plot(data[\"time\"].to_numpy(), data[\"x\"].to_numpy(),\"oC1\")\n\n    fig3, ax3 = plt.subplots()\n    ax3.set_title(\"ground truth value\")\n    ax3.set_xlabel(\"unix time\")\n    ax3.set_ylabel(\"x\")\n    c = ax3.pcolor(tt, xx, gt_vals.reshape(n,n))\n    fig3.colorbar(c)\n</pre> # plot model import torch from matplotlib import pyplot as plt  # plot model predictions data = X.data  xbounds = generator.vocs.bounds tbounds = [data[\"time\"].min(), data[\"time\"].max()]  def gt(inpts):     return 5*(inpts[:,1] - (inpts[:,0] - start_time)*1e-2)**2  model = X.generator.model n = 200 t = torch.linspace(*tbounds, n, dtype=torch.double) x = torch.linspace(*xbounds.flatten(), n, dtype=torch.double) tt, xx = torch.meshgrid(t, x) pts = torch.hstack([ele.reshape(-1, 1) for ele in (tt, xx)]).double()  tt, xx = tt.numpy(), xx.numpy()  #NOTE: the model inputs are such that t is the last dimension gp_pts = torch.flip(pts, dims=[-1])  gt_vals = gt(pts)  with torch.no_grad():     post = model.posterior(gp_pts)      mean = post.mean     std = torch.sqrt(post.variance)      fig, ax = plt.subplots()     ax.set_title(\"model mean\")     ax.set_xlabel(\"unix time\")     ax.set_ylabel(\"x\")     c = ax.pcolor(tt, xx, mean.reshape(n,n))     fig.colorbar(c)      fig2, ax2 = plt.subplots()     ax2.set_title(\"model uncertainty\")     ax2.set_xlabel(\"unix time\")     ax2.set_ylabel(\"x\")     c = ax2.pcolor(tt, xx, std.reshape(n,n))     fig2.colorbar(c)      ax.plot(data[\"time\"].to_numpy(), data[\"x\"].to_numpy(),\"oC1\")     ax2.plot(data[\"time\"].to_numpy(), data[\"x\"].to_numpy(),\"oC1\")      fig3, ax3 = plt.subplots()     ax3.set_title(\"ground truth value\")     ax3.set_xlabel(\"unix time\")     ax3.set_ylabel(\"x\")     c = ax3.pcolor(tt, xx, gt_vals.reshape(n,n))     fig3.colorbar(c) In\u00a0[6]: Copied! <pre>list(model.named_parameters())\n</pre> list(model.named_parameters()) Out[6]: <pre>[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-26.2528], dtype=torch.float64, requires_grad=True)),\n ('models.0.mean_module.raw_constant',\n  Parameter containing:\n  tensor(2.4626, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(2.3099, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[-0.5298,  1.3817]], dtype=torch.float64, requires_grad=True))]</pre> In\u00a0[7]: Copied! <pre># plot the acquisition function\n# note that target time is only updated during the generate call\ntarget_time = X.generator.target_prediction_time\nprint(target_time-start_time)\nmy_acq_func = X.generator.get_acquisition(model)\n\nwith torch.no_grad():\n    acq_pts = x.unsqueeze(-1).unsqueeze(-1)\n    full_acq = my_acq_func.acq_func(gp_pts.unsqueeze(1))\n    fixed_acq = my_acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(tt, xx, full_acq.reshape(n,n))\n    fig.colorbar(c)\n\n    fi2, ax2 = plt.subplots()\n    ax2.plot(x.flatten(), fixed_acq.flatten())\n</pre> # plot the acquisition function # note that target time is only updated during the generate call target_time = X.generator.target_prediction_time print(target_time-start_time) my_acq_func = X.generator.get_acquisition(model)  with torch.no_grad():     acq_pts = x.unsqueeze(-1).unsqueeze(-1)     full_acq = my_acq_func.acq_func(gp_pts.unsqueeze(1))     fixed_acq = my_acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(tt, xx, full_acq.reshape(n,n))     fig.colorbar(c)      fi2, ax2 = plt.subplots()     ax2.plot(x.flatten(), fixed_acq.flatten()) <pre>23.109293460845947\n</pre> In\u00a0[7]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/time_dependent_bo/#time-dependent-bayesian-optimization","title":"Time dependent Bayesian Optimization\u00b6","text":"<p>In this example we demonstrate time dependent optimization. In this case we are not only interested in finding an optimum point in input space, but also maintain the ideal point over time.</p>"},{"location":"examples/single_objective_bayes_opt/time_dependent_bo/#time-dependent-test-problem","title":"Time dependent test problem\u00b6","text":"<p>Optimization is carried out over a single variable <code>x</code>. The test function is a simple quadratic, with a minimum location that drifts in the positive <code>x</code> direction over (real) time.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/","title":"TuRBO Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\nimport math\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> from xopt.vocs import VOCS import math  # define variables and function objectives vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\ndef sin_function(input_dict):\n    x = input_dict[\"x\"]\n    return {\"f\": -10*np.exp(-(x - np.pi)**2 / 0.01) + 0.5*np.sin(5*x)}\n</pre> # define a test function to optimize import numpy as np  def sin_function(input_dict):     x = input_dict[\"x\"]     return {\"f\": -10*np.exp(-(x - np.pi)**2 / 0.01) + 0.5*np.sin(5*x)} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt import Xopt\n\n\nevaluator = Evaluator(function=sin_function)\ngenerator = UpperConfidenceBoundGenerator(\n    vocs=vocs, turbo_controller=\"optimize\"\n)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt import Xopt   evaluator = Evaluator(function=sin_function) generator = UpperConfidenceBoundGenerator(     vocs=vocs, turbo_controller=\"optimize\" ) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre>X\n</pre> X Out[4]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: __main__.sin_function\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  beta: 2.0\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: upper_confidence_bound\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  supports_batch_generation: true\n  turbo_controller:\n    batch_size: 1\n    best_value: null\n    center_x: null\n    dim: 1\n    failure_counter: 0\n    failure_tolerance: 2\n    length: 0.25\n    length_max: 2.0\n    length_min: 0.0078125\n    name: optimize\n    scale_factor: 2.0\n    success_counter: 0\n    success_tolerance: 2\n    tkwargs:\n      dtype: torch.float64\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints: {}\n  objectives:\n    f: MINIMIZE\n  observables: []\n  variables:\n    x:\n    - 0.0\n    - 6.283185307179586\n</pre> In\u00a0[5]: Copied! <pre>import pandas as pd\nX.evaluate_data(pd.DataFrame({\"x\":[3.0, 1.75, 2.0]}))\n\n# inspect the gathered data\nX.data\n</pre> import pandas as pd X.evaluate_data(pd.DataFrame({\"x\":[3.0, 1.75, 2.0]}))  # inspect the gathered data X.data Out[5]: x f xopt_runtime xopt_error 0 3.00 -1.021664 0.000033 False 1 1.75 0.312362 0.000006 False 2 2.00 -0.272011 0.000005 False In\u00a0[6]: Copied! <pre># determine trust region from gathered data\nX.generator.train_model()\nX.generator.turbo_controller.update_state(X.generator.data)\nX.generator.turbo_controller.get_trust_region(X.generator.model)\n</pre> # determine trust region from gathered data X.generator.train_model() X.generator.turbo_controller.update_state(X.generator.data) X.generator.turbo_controller.get_trust_region(X.generator.model) Out[6]: <pre>tensor([[2.2146],\n        [3.7854]], dtype=torch.float64)</pre> In\u00a0[7]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n\n# test points for plotting\ntest_x = torch.linspace(*X.vocs.bounds.flatten(),500).double()\n\nfor i in range(10):\n\n    # get the Gaussian process model from the generator\n    model = X.generator.train_model()\n\n    # get trust region\n    trust_region = X.generator.turbo_controller.get_trust_region(model)\\\n        .squeeze()\n    scale_factor = X.generator.turbo_controller.length\n    region_width = trust_region[1] - trust_region[0]\n    best_value = X.generator.turbo_controller.best_value\n\n    # get number of successes and failures\n    n_successes = X.generator.turbo_controller.success_counter\n    n_failures = X.generator.turbo_controller.failure_counter\n\n    # get acquisition function from generator\n    acq = X.generator.get_acquisition(model)\n\n    # calculate model posterior and acquisition function at each test point\n    # NOTE: need to add a dimension to the input tensor for evaluating the\n    # posterior and another for the acquisition function, see\n    # https://botorch.org/docs/batching for details\n    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n    # skipping calculations for backpropagation\n    with torch.no_grad():\n        posterior = model.posterior(test_x.unsqueeze(1))\n        acq_val = acq(test_x.reshape(-1,1,1))\n\n    # get mean function and confidence regions\n    mean = posterior.mean\n    l,u = posterior.mvn.confidence_region()\n\n    # plot model and acquisition function\n    fig,ax = plt.subplots(2,1,sharex=\"all\")\n\n    # add title for successes and failures\n    ax[0].set_title(f\"n_successes: {n_successes}, n_failures: {n_failures}, \"\n                    f\"scale_factor: {scale_factor}, region_width: {region_width:.2}, \"\n                    f\"best_value: {best_value:.4}\")\n\n    # plot model posterior\n    ax[0].plot(test_x, mean, label=\"Posterior mean\")\n    ax[0].fill_between(test_x, l, u,alpha=0.25, label=\"Confidence region\")\n\n    # add data to model plot\n    ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")\n\n    # plot true function\n    true_f = sin_function({\"x\": test_x})[\"f\"]\n    ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")\n\n    # add legend\n    ax[0].legend(fontsize=\"x-small\")\n\n    # plot acquisition function\n    ax[1].plot(test_x, acq_val.flatten())\n\n    ax[0].set_ylabel(\"f\")\n    ax[0].set_ylim(-12,10)\n    ax[1].set_ylabel(r\"$\\alpha(x)$\")\n    ax[1].set_xlabel(\"x\")\n\n    # plot trust region\n    for a in ax:\n        a.axvline(trust_region[0],c=\"r\")\n        a.axvline(trust_region[1],c=\"r\")\n\n    fig.tight_layout()\n    X.step()\n</pre> import torch import matplotlib.pyplot as plt  # test points for plotting test_x = torch.linspace(*X.vocs.bounds.flatten(),500).double()  for i in range(10):      # get the Gaussian process model from the generator     model = X.generator.train_model()      # get trust region     trust_region = X.generator.turbo_controller.get_trust_region(model)\\         .squeeze()     scale_factor = X.generator.turbo_controller.length     region_width = trust_region[1] - trust_region[0]     best_value = X.generator.turbo_controller.best_value      # get number of successes and failures     n_successes = X.generator.turbo_controller.success_counter     n_failures = X.generator.turbo_controller.failure_counter      # get acquisition function from generator     acq = X.generator.get_acquisition(model)      # calculate model posterior and acquisition function at each test point     # NOTE: need to add a dimension to the input tensor for evaluating the     # posterior and another for the acquisition function, see     # https://botorch.org/docs/batching for details     # NOTE: we use the `torch.no_grad()` environment to speed up computation by     # skipping calculations for backpropagation     with torch.no_grad():         posterior = model.posterior(test_x.unsqueeze(1))         acq_val = acq(test_x.reshape(-1,1,1))      # get mean function and confidence regions     mean = posterior.mean     l,u = posterior.mvn.confidence_region()      # plot model and acquisition function     fig,ax = plt.subplots(2,1,sharex=\"all\")      # add title for successes and failures     ax[0].set_title(f\"n_successes: {n_successes}, n_failures: {n_failures}, \"                     f\"scale_factor: {scale_factor}, region_width: {region_width:.2}, \"                     f\"best_value: {best_value:.4}\")      # plot model posterior     ax[0].plot(test_x, mean, label=\"Posterior mean\")     ax[0].fill_between(test_x, l, u,alpha=0.25, label=\"Confidence region\")      # add data to model plot     ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")      # plot true function     true_f = sin_function({\"x\": test_x})[\"f\"]     ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")      # add legend     ax[0].legend(fontsize=\"x-small\")      # plot acquisition function     ax[1].plot(test_x, acq_val.flatten())      ax[0].set_ylabel(\"f\")     ax[0].set_ylim(-12,10)     ax[1].set_ylabel(r\"$\\alpha(x)$\")     ax[1].set_xlabel(\"x\")      # plot trust region     for a in ax:         a.axvline(trust_region[0],c=\"r\")         a.axvline(trust_region[1],c=\"r\")      fig.tight_layout()     X.step()  In\u00a0[8]: Copied! <pre># access the collected data\nX.generator.turbo_controller\n</pre> # access the collected data X.generator.turbo_controller Out[8]: <pre>OptimizeTurboController(vocs=VOCS(variables={'x': [0.0, 6.283185307179586]}, constraints={}, objectives={'f': 'MINIMIZE'}, constants={}, observables=[]), dim=1, batch_size=1, length=0.0625, length_min=0.0078125, length_max=2.0, failure_counter=0, failure_tolerance=2, success_counter=1, success_tolerance=2, center_x={'x': 3.1421063677657464}, scale_factor=2.0, tkwargs={'dtype': torch.float64}, name='optimize', best_value=-10.001020385255307)</pre> In\u00a0[9]: Copied! <pre>X.data\n</pre> X.data Out[9]: x f xopt_runtime xopt_error 0 3.000000 -1.021664 0.000033 False 1 1.750000 0.312362 0.000006 False 2 2.000000 -0.272011 0.000005 False 3 3.392699 -0.493621 0.000011 False 4 2.628499 0.272390 0.000011 False 5 3.141806 -10.000489 0.000011 False 6 3.205876 -6.773020 0.000011 False 7 3.114312 -9.214808 0.000011 False 8 3.149690 -9.954879 0.000033 False 9 3.140919 -9.997862 0.000010 False 10 3.141708 -10.000276 0.000010 False 11 3.142106 -10.001020 0.000010 False 12 3.142392 -10.001360 0.000010 False In\u00a0[10]: Copied! <pre>list(model.named_parameters())\n</pre> list(model.named_parameters()) Out[10]: <pre>[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-25.0732], dtype=torch.float64, requires_grad=True)),\n ('models.0.mean_module.raw_constant',\n  Parameter containing:\n  tensor(0.9513, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(0.5314, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[-3.7730]], dtype=torch.float64, requires_grad=True))]</pre>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#turbo-bayesian-optimization","title":"TuRBO Bayesian Optimization\u00b6","text":"<p>In this tutorial we demonstrate the use of Xopt to preform Trust Region Bayesian Optimization (TuRBO) on a simple test problem. During optimization of high dimensional input spaces off the shelf BO tends to over-emphasize exploration which severely degrades optimization performance. TuRBO attempts to prevent this by maintaining a surrogate model over a local (trust) region centered on the best observation so far and restricting optimization inside that local region. The trust region is expanded and contracted based on the number of <code>successful</code> (observations that improve over the best observed point) or <code>unsuccessful</code> (no improvement) observations in a row. See https://botorch.org/tutorials/turbo_1 for details.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize a function in the domian [0,2*pi]. Note that the function used to evaluate the objective function takes a dictionary as input and returns a dictionary as the output.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Upper Confidence Bound acquisition function to perform Bayesian Optimization.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>Notice that when the number of successive successes or failures reaches 2 the trust region expands or contracts and counters are reset to zero. Counters are also reset to zero during alternate successes/failures. Finally, the model is most accurate inside the trust region, which supports our goal of local optimization.</p>"},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/","title":"Upper Confidence Bound BO","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\n\n\nimport matplotlib.pyplot as plt\nfrom xopt import Xopt\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import torch   import matplotlib.pyplot as plt from xopt import Xopt <p>The <code>Xopt</code> object can be instantiated from a JSON or YAML file, or a dict, with the proper structure.</p> <p>Here we will make one</p> In\u00a0[2]: Copied! <pre># Make a proper input file.\nYAML = \"\"\"\ngenerator:\n  name: upper_confidence_bound\n  beta: 0.1\n\nevaluator:\n  function: xopt.resources.test_functions.sinusoid_1d.evaluate_sinusoid\n\nvocs:\n  variables:\n    x1: [0, 6.28]\n  objectives:\n    y1: 'MINIMIZE'\n\"\"\"\n</pre> # Make a proper input file. YAML = \"\"\" generator:   name: upper_confidence_bound   beta: 0.1  evaluator:   function: xopt.resources.test_functions.sinusoid_1d.evaluate_sinusoid  vocs:   variables:     x1: [0, 6.28]   objectives:     y1: 'MINIMIZE' \"\"\" In\u00a0[3]: Copied! <pre>X = Xopt.from_yaml(YAML)\nX\n</pre> X = Xopt.from_yaml(YAML) X Out[3]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g7a7eff9\nData size: 0\nConfig as YAML:\ndump_file: null\nevaluator:\n  function: xopt.resources.test_functions.sinusoid_1d.evaluate_sinusoid\n  function_kwargs: {}\n  max_workers: 1\n  vectorized: false\ngenerator:\n  beta: 0.1\n  computation_time: null\n  fixed_features: null\n  gp_constructor:\n    covar_modules: {}\n    mean_modules: {}\n    name: standard\n    trainable_mean_keys: []\n    transform_inputs: true\n    use_low_noise_prior: true\n  log_transform_acquisition_function: false\n  max_travel_distances: null\n  model: null\n  n_candidates: 1\n  n_interpolate_points: null\n  n_monte_carlo_samples: 128\n  name: upper_confidence_bound\n  numerical_optimizer:\n    max_iter: 2000\n    max_time: null\n    n_restarts: 20\n    name: LBFGS\n  supports_batch_generation: true\n  turbo_controller: null\n  use_cuda: false\nmax_evaluations: null\nserialize_inline: false\nserialize_torch: false\nstrict: true\nvocs:\n  constants: {}\n  constraints: {}\n  objectives:\n    y1: MINIMIZE\n  observables: []\n  variables:\n    x1:\n    - 0.0\n    - 6.28\n</pre> In\u00a0[4]: Copied! <pre>X.random_evaluate(3)\nfor i in range(5):\n    print(i)\n    X.step()\n</pre> X.random_evaluate(3) for i in range(5):     print(i)     X.step() <pre>0\n1\n</pre> <pre>2\n3\n</pre> <pre>4\n</pre> In\u00a0[5]: Copied! <pre>X.data\n</pre> X.data Out[5]: x1 y1 c1 xopt_runtime xopt_error 0 0.822846 0.733084 -2.668864 0.000018 False 1 5.810552 -0.455233 -13.886258 0.000004 False 2 1.846439 0.962250 0.473564 0.000003 False 3 6.280000 -0.003185 -9.554148 0.000010 False 4 5.103761 -0.924386 -19.664150 0.000010 False 5 4.778741 -0.997800 -18.583933 0.000010 False 6 4.697842 -0.999894 -18.504122 0.000010 False 7 4.710486 -0.999998 -18.500071 0.000009 False In\u00a0[6]: Copied! <pre>model = X.generator.model\nbounds = X.vocs.bounds\n\ntest_x = torch.linspace(*bounds.flatten(), 100).double()\n\ntrain_x = torch.tensor(X.data[\"x1\"].to_numpy())\ntrain_y = torch.tensor(X.data[\"y1\"].to_numpy())\n\nfig, ax = plt.subplots()\nwith torch.no_grad():\n    post = model.models[0].posterior(test_x.reshape(-1,1,1))\n    mean = post.mean.flatten()\n    std = post.variance.sqrt().flatten()\n\n    lower = mean - std\n    upper = mean + std\n\nax.plot(test_x, mean)\nax.fill_between(test_x, lower, upper, alpha=0.5)\nax.plot(\n    train_x.flatten(),\n    train_y.flatten(),\n    \"+\"\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"y1\")\n</pre> model = X.generator.model bounds = X.vocs.bounds  test_x = torch.linspace(*bounds.flatten(), 100).double()  train_x = torch.tensor(X.data[\"x1\"].to_numpy()) train_y = torch.tensor(X.data[\"y1\"].to_numpy())  fig, ax = plt.subplots() with torch.no_grad():     post = model.models[0].posterior(test_x.reshape(-1,1,1))     mean = post.mean.flatten()     std = post.variance.sqrt().flatten()      lower = mean - std     upper = mean + std  ax.plot(test_x, mean) ax.fill_between(test_x, lower, upper, alpha=0.5) ax.plot(     train_x.flatten(),     train_y.flatten(),     \"+\" ) ax.set_xlabel(\"x1\") ax.set_ylabel(\"y1\")  Out[6]: <pre>Text(0, 0.5, 'y1')</pre> In\u00a0[7]: Copied! <pre># Cleanup\n!rm dump.yaml\n</pre> # Cleanup !rm dump.yaml <pre>rm: cannot remove 'dump.yaml': No such file or directory\r\n</pre> In\u00a0[7]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#upper-confidence-bound-bo","title":"Upper Confidence Bound BO\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#run-optimization","title":"Run Optimization\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#view-output-data","title":"View output data\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#visualize-model-used-by-upper-confidence-bound","title":"Visualize model used by upper confidence bound\u00b6","text":"<p>Models are kept in a list, in this case that list has one element, the model created for the objective <code>y1</code>.</p>"}]}