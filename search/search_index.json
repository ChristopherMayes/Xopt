{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#xopt","title":"Xopt","text":"<p>Flexible optimization of arbitrary problems in Python.</p> <p>The goal of this package is to provide advanced algorithmic support for arbitrary  simulations/control systems with minimal required coding. Users can easily connect  arbitrary evaluation functions to advanced algorithms with minimal coding with  support for multi-threaded or MPI-enabled execution.</p> <p>Currenty Xopt provides:</p> <ul> <li>optimization algorithms:<ul> <li><code>cnsga</code> Continuous NSGA-II with constraints.</li> <li><code>upper_confidence_bound</code> Single objective Bayesian optimization (w/ or w/o     constraints, serial or parallel).</li> <li><code>mobo</code> Multi-objective Bayesian optimization (w/ or w/o constraints, serial or parallel).</li> <li><code>bayesian_exploration</code> Bayesian exploration.</li> </ul> </li> <li>sampling algorithms:<ul> <li><code>random sampler</code></li> </ul> </li> <li>Convenient YAML/JSON based input format.</li> <li>Driver programs:<ul> <li><code>xopt.mpi.run</code> Parallel MPI execution using this input format.</li> </ul> </li> </ul> <p>Xopt does not provide:  - your custom simulation via an <code>evaluate</code> function.</p> <p>Rather, Xopt asks you to define this function.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Xopt Overview PDF gives an overview of Xopt's design and  usage.</p> <p>Xopt Built-In Generators provides a list of available algorithms  implemented in the Xopt <code>Generator</code> framework.</p> <p>Simple Bayesian Optimization Example shows  Xopt usage for a simple optimization problem.</p>"},{"location":"#configuring-an-xopt-run","title":"Configuring an Xopt run","text":"<p>Xopt runs are specified via a dictionary that can be directly imported from a YAML  file.</p> <pre><code>xopt:\nmax_evaluations: 6400\n\ngenerator:\nname: cnsga\npopulation_size: 64\npopulation_file: test.csv\noutput_path: .\n\nevaluator:\nfunction: xopt.resources.test_functions.tnk.evaluate_TNK\nfunction_kwargs:\nraise_probability: 0.1\n\nvocs:\nvariables:\nx1: [0, 3.14159]\nx2: [0, 3.14159]\nobjectives: {y1: MINIMIZE, y2: MINIMIZE}\nconstraints:\nc1: [GREATER_THAN, 0]\nc2: [LESS_THAN, 0.5]\nconstants: {a: dummy_constant}\n</code></pre>"},{"location":"#defining-evaluation-function","title":"Defining evaluation function","text":"<p>Xopt can interface with arbitrary evaluate functions (defined in Python) with the  following form: <pre><code>evaluate(inputs: dict) -&gt; dict\n</code></pre> Evaluate functions must accept a dictionary object that at least has the keys  specified in <code>variables, constants, linked_variables</code> and returns a dictionary  containing at least the  keys contained in <code>objectives, constraints</code>. Extra dictionary keys are tracked and  used in the evaluate function but are not modified by xopt.</p>"},{"location":"#using-mpi","title":"Using MPI","text":"<p>Example MPI run, with <code>xopt.yaml</code> as the only user-defined file: <pre><code>mpirun -n 64 python -m mpi4py.futures -m xopt.mpi.run xopt.yaml\n</code></pre></p>"},{"location":"algorithms/","title":"Pre-Configured Generators in Xopt","text":"<p>A number of algorithms are implemented in Xopt using the <code>Generator</code> class for  off-the-shelf usage.  Below is a  description of the different generators that are available in Xopt and their target  use cases.</p>"},{"location":"algorithms/#randomgenerator","title":"RandomGenerator","text":"<p>Generates random points in the input space according to <code>VOCS</code>.</p>"},{"location":"algorithms/#bayesian-generators","title":"Bayesian Generators","text":"<p>All of the generators here use Bayesian optimization (BO) type methods to solve single  objective, multi objective and characterization problems. Bayesian generators  incorperate unknown constrianing functions into optimization based on what is  specified in <code>VOCS</code></p> <ul> <li><code>ExpectedImprovementGenerator</code>: implements Expected Improvement single    objective BO. Automatically balances trade-offs between exploration and    exploitation and is thus useful for general purpose optimization. </li> <li><code>UpperConfidenceBoundGenerator</code>: implements Upper Confidence Bound single    objective BO. Requires a hyperparameter <code>beta</code> that explicitly sets the tradeoff    between exploration and exploitation. Default value of <code>beta=2</code> is a good    starting point. Increase $\\beta$ to prioritize exploration and decrease <code>beta</code> to    prioritize exploitation.</li> <li><code>BayesianExplorationGenerator</code>: implements the Bayesian Exploration algorithm    for function characterization. This algorithm selects observation points that    maximize model uncertainty, thus picking points that maximize the information gain    about the target function at each iteration. If the target function is found to be    more sensative to one parameter this generator will adjust sampling frequency to    adapt. Note: specifying <code>vocs.objective[1]</code>   to <code>MAXIMIZE</code> or <code>MINIMIZE</code> does not change the behavior of this generator.</li> <li><code>MOBOGenerator</code>: implements Multi-Objective BO using the    Expected Hypervolume Improvement (EHVI) acquisition function. This is an ideal    general purpose multi-objective optimizer when objective evaluations cannot be    massively parallelized (&lt; 10 parallel evaluations).</li> <li><code>MGGPOGenerator</code>: implements Multi-Generation Gaussian Process Optimization using    the    Expected Hypervolume Improvement (EHVI) acquisition function. This is an ideal    general purpose multi-objective optimizer when objective evaluations can be    massively parallelized (&gt; 10 parallel evaluations) .</li> <li><code>MultiFidelityGenerator</code>: implements Multi-Fidelity BO which can take    advantage of lower fidelity evaluations of objectives and constraints to reduce    the computational cost of solving single or multi-objective optimization problems    in sequential or small scale parallel (&lt; 10 parallel evaluations)    contexts. </li> </ul>"},{"location":"algorithms/#evolutionary-generators","title":"Evolutionary Generators","text":"<ul> <li><code>CNSGAGenerator</code>: implements Continuous Non-dominated Sorted Genetic Algorithm    which as a good general purpose evolutionary algorithm used for solving    multi-objective optimization problems where evaluating the objective is relatively    cheap and massively parallelizable (above 5-10 parallel evaluations).</li> </ul>"},{"location":"algorithms/#extremum-seeking-generators","title":"Extremum Seeking Generators","text":"<ul> <li><code>ExtremumSeekingGenerator</code>: implements the Extremum Seeking algorithm which is    ideal for solving optimization problems that are suceptable to drifts.</li> </ul>"},{"location":"algorithms/#scipy-generators","title":"Scipy Generators","text":"<p>These generators serve as wrappers for algorithms implemented in scipy. - <code>NelderMeadGenerator</code>: implements Nelder-Mead (simplex) optimization.</p>"},{"location":"algorithms/#rcds-generators","title":"RCDS Generators","text":"<ul> <li><code>RCDSGenerator</code>: implements the RCDS algorithm. RCDS could be applied in noisy   online optimization scenarios</li> </ul>"},{"location":"algorithms/#custom-generators","title":"Custom Generators","text":"<p>Any general algorithm can be implemented by subclassing the abstract <code>Generator</code> class and used in the Xopt framework. If you implement a generator for your use case please consider opening a pull request so that we can add it to Xopt!</p>"},{"location":"installation/","title":"Installing Xopt","text":"<p>Installing <code>xopt</code> from the <code>conda-forge</code> channel can be achieved by adding <code>conda-forge</code> to your channels with:</p> <pre><code>conda config --add channels conda-forge\n</code></pre> <p>Once the <code>conda-forge</code> channel has been enabled, <code>xopt</code> can be installed with:</p> <pre><code>conda install xopt\n</code></pre> <p>It is possible to list all of the versions of <code>xopt</code> available on your platform with:</p> <pre><code>conda search xopt --channel conda-forge\n</code></pre>"},{"location":"installation/#developers","title":"Developers","text":"<p>Clone this repository: <pre><code>git clone https://github.com/ChristopherMayes/Xopt.git\n</code></pre></p> <p>Create an environment <code>xopt-dev</code> with all the dependencies: <pre><code>conda env create -f environment.yml\n</code></pre></p> <p>Install as editable: <pre><code>conda activate xopt-dev\npip install --no-dependencies -e .\n</code></pre></p>"},{"location":"installation/#cori-nersc-setup","title":"Cori (NERSC) setup","text":"<p><pre><code>conda install -c conda-forge xopt\n</code></pre> Follow instructions to build mpi4py: https://docs.nersc.gov/programming/high-level-environments/python/ Note that there is a bug in Jupyterhub terminals. Type: <pre><code>module swap PrgEnv-gnu PrgEnv-gnu\n</code></pre> to get the C compiler activated. </p>"},{"location":"api/evaluator/","title":"Evaluator","text":""},{"location":"api/evaluator/#xopt.evaluator.Evaluator","title":"<code>xopt.evaluator.Evaluator</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Xopt Evaluator for handling the parallel execution of an evaluate function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to evaluate.</p> required <code>function_kwargs</code> <code>dict, default</code> <p>Any kwargs to pass on to this function.</p> required <code>max_workers</code> <code>int, default</code> <p>Maximum number of workers.</p> required <code>executor</code> <code>NormalExecutor</code> <p>NormalExecutor or any instantiated Executor object</p> required <code>vectorized</code> <code>bool, default</code> <p>If true,</p> required Source code in <code>xopt/evaluator.py</code> <pre><code>class Evaluator(BaseModel):\n\"\"\"\n    Xopt Evaluator for handling the parallel execution of an evaluate function.\n\n    Parameters\n    ----------\n    function : Callable\n        Function to evaluate.\n    function_kwargs : dict, default={}\n        Any kwargs to pass on to this function.\n    max_workers : int, default=1\n        Maximum number of workers.\n    executor : NormalExecutor\n        NormalExecutor or any instantiated Executor object\n    vectorized : bool, default=False\n        If true,\n    \"\"\"\n\n    function: Callable\n    max_workers: int = 1\n    executor: NormalExecutor = Field(exclude=True)  # Do not serialize\n    function_kwargs: dict = {}\n    vectorized: bool = False\n\n    class Config:\n\"\"\"config\"\"\"\n\n        arbitrary_types_allowed = True\n        # validate_assignment = True # Broken in 1.9.0.\n        # Trying to fix in https://github.com/samuelcolvin/pydantic/pull/4194\n        json_encoders = JSON_ENCODERS\n        extra = \"forbid\"\n        # copy_on_model_validation = False\n\n    @root_validator(pre=True)\n    def validate_all(cls, values):\n        f = get_function(values[\"function\"])\n        kwargs = values.get(\"function_kwargs\", {})\n        kwargs = {**get_function_defaults(f), **kwargs}\n        values[\"function\"] = f\n        values[\"function_kwargs\"] = kwargs\n\n        max_workers = values.pop(\"max_workers\", 1)\n\n        executor = values.pop(\"executor\", None)\n        if not executor:\n            if max_workers &gt; 1:\n                executor = ProcessPoolExecutor(max_workers=max_workers)\n            else:\n                executor = DummyExecutor()\n\n        # Cast as a NormalExecutor\n        values[\"executor\"] = NormalExecutor[type(executor)](executor=executor)\n        values[\"max_workers\"] = max_workers\n\n        return values\n\n    def evaluate(self, input: Dict, **kwargs):\n\"\"\"\n        Evaluate a single input dict using Evaluator.function with\n        Evaluator.function_kwargs.\n\n        Further kwargs are passed to the function.\n\n        Inputs:\n            inputs: dict of inputs to be evaluated\n            **kwargs: additional kwargs to pass to the function\n\n        Returns:\n            function(input, **function_kwargs_updated)\n\n        \"\"\"\n        return self.safe_function(input, **{**self.function_kwargs, **kwargs})\n\n    def evaluate_data(self, input_data: pd.DataFrame):\n\"\"\"evaluate dataframe of inputs\"\"\"\n        input_data = pd.DataFrame(input_data)\n\n        if self.vectorized:\n            output_data = self.safe_function(input_data, **self.function_kwargs)\n        else:\n            # This construction is needed to avoid a pickle error\n            inputs = input_data.to_dict(\"records\")\n\n            funcs = [self.function] * len(inputs)\n            kwargs = [self.function_kwargs] * len(inputs)\n\n            output_data = self.executor.map(\n                safe_function1_for_map,\n                funcs,\n                inputs,\n                kwargs,\n            )\n\n        return pd.DataFrame(output_data, index=input_data.index)\n\n    def safe_function(self, *args, **kwargs):\n\"\"\"\n        Safely call the function, handling exceptions.\n\n        Note that this should not be submitted to fuu\n        \"\"\"\n        return safe_function(self.function, *args, **kwargs)\n\n    def submit(self, input: Dict):\n\"\"\"submit a single input to the executor\n\n        Parameters\n        ----------\n        input : dict\n\n        Returns\n        -------\n        Future  : Future object\n        \"\"\"\n        if not isinstance(input, dict):\n            raise ValueError(\"input must be a dictionary\")\n        # return self.executor.submit(self.function, input, **self.function_kwargs)\n        # Must call a function outside of the classs\n        # See: https://stackoverflow.com/questions/44144584/typeerror-cant-pickle-thread-lock-objects\n        return self.executor.submit(\n            safe_function, self.function, input, **self.function_kwargs\n        )\n\n    def submit_data(self, input_data: pd.DataFrame):\n\"\"\"submit dataframe of inputs to executor\"\"\"\n        input_data = pd.DataFrame(input_data)  # cast to dataframe for consistency\n\n        if self.vectorized:\n            # Single submission, cast to numpy array\n            inputs = input_data.to_dict(orient=\"list\")\n            for key, value in inputs.items():\n                inputs[key] = np.array(value)\n            futures = [self.submit(inputs)]  # Single item\n        else:\n            # Do not use iterrows or itertuples.\n            futures = [self.submit(inputs) for inputs in input_data.to_dict(\"records\")]\n\n        return futures\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.Config","title":"<code>Config</code>","text":"<p>config</p> Source code in <code>xopt/evaluator.py</code> <pre><code>class Config:\n\"\"\"config\"\"\"\n\n    arbitrary_types_allowed = True\n    # validate_assignment = True # Broken in 1.9.0.\n    # Trying to fix in https://github.com/samuelcolvin/pydantic/pull/4194\n    json_encoders = JSON_ENCODERS\n    extra = \"forbid\"\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.evaluate","title":"<code>evaluate(input, **kwargs)</code>","text":"<p>Evaluate a single input dict using Evaluator.function with Evaluator.function_kwargs.</p> <p>Further kwargs are passed to the function.</p> <p>Inputs:     inputs: dict of inputs to be evaluated     **kwargs: additional kwargs to pass to the function</p> <p>Returns:     function(input, **function_kwargs_updated)</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def evaluate(self, input: Dict, **kwargs):\n\"\"\"\n    Evaluate a single input dict using Evaluator.function with\n    Evaluator.function_kwargs.\n\n    Further kwargs are passed to the function.\n\n    Inputs:\n        inputs: dict of inputs to be evaluated\n        **kwargs: additional kwargs to pass to the function\n\n    Returns:\n        function(input, **function_kwargs_updated)\n\n    \"\"\"\n    return self.safe_function(input, **{**self.function_kwargs, **kwargs})\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.evaluate_data","title":"<code>evaluate_data(input_data)</code>","text":"<p>evaluate dataframe of inputs</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def evaluate_data(self, input_data: pd.DataFrame):\n\"\"\"evaluate dataframe of inputs\"\"\"\n    input_data = pd.DataFrame(input_data)\n\n    if self.vectorized:\n        output_data = self.safe_function(input_data, **self.function_kwargs)\n    else:\n        # This construction is needed to avoid a pickle error\n        inputs = input_data.to_dict(\"records\")\n\n        funcs = [self.function] * len(inputs)\n        kwargs = [self.function_kwargs] * len(inputs)\n\n        output_data = self.executor.map(\n            safe_function1_for_map,\n            funcs,\n            inputs,\n            kwargs,\n        )\n\n    return pd.DataFrame(output_data, index=input_data.index)\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.safe_function","title":"<code>safe_function(*args, **kwargs)</code>","text":"<p>Safely call the function, handling exceptions.</p> <p>Note that this should not be submitted to fuu</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def safe_function(self, *args, **kwargs):\n\"\"\"\n    Safely call the function, handling exceptions.\n\n    Note that this should not be submitted to fuu\n    \"\"\"\n    return safe_function(self.function, *args, **kwargs)\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.submit","title":"<code>submit(input)</code>","text":"<p>submit a single input to the executor</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>dict</code> required <p>Returns:</p> Name Type Description <code>Future</code> <code>Future object</code> Source code in <code>xopt/evaluator.py</code> <pre><code>def submit(self, input: Dict):\n\"\"\"submit a single input to the executor\n\n    Parameters\n    ----------\n    input : dict\n\n    Returns\n    -------\n    Future  : Future object\n    \"\"\"\n    if not isinstance(input, dict):\n        raise ValueError(\"input must be a dictionary\")\n    # return self.executor.submit(self.function, input, **self.function_kwargs)\n    # Must call a function outside of the classs\n    # See: https://stackoverflow.com/questions/44144584/typeerror-cant-pickle-thread-lock-objects\n    return self.executor.submit(\n        safe_function, self.function, input, **self.function_kwargs\n    )\n</code></pre>"},{"location":"api/evaluator/#xopt.evaluator.Evaluator.submit_data","title":"<code>submit_data(input_data)</code>","text":"<p>submit dataframe of inputs to executor</p> Source code in <code>xopt/evaluator.py</code> <pre><code>def submit_data(self, input_data: pd.DataFrame):\n\"\"\"submit dataframe of inputs to executor\"\"\"\n    input_data = pd.DataFrame(input_data)  # cast to dataframe for consistency\n\n    if self.vectorized:\n        # Single submission, cast to numpy array\n        inputs = input_data.to_dict(orient=\"list\")\n        for key, value in inputs.items():\n            inputs[key] = np.array(value)\n        futures = [self.submit(inputs)]  # Single item\n    else:\n        # Do not use iterrows or itertuples.\n        futures = [self.submit(inputs) for inputs in input_data.to_dict(\"records\")]\n\n    return futures\n</code></pre>"},{"location":"api/generators/","title":"Base generator class","text":""},{"location":"api/generators/#xopt.generator","title":"<code>xopt.generator</code>","text":""},{"location":"api/generators/#xopt.generator.Generator","title":"<code>Generator</code>","text":"<p>         Bases: <code>ABC</code></p> Source code in <code>xopt/generator.py</code> <pre><code>class Generator(ABC):\n    alias = None\n\n    def __init__(self, vocs: VOCS, options: GeneratorOptions = None):\n\"\"\"\n        Initialize the generator.\n\n        Args:\n            vocs: The vocs to use.\n            options: The options to use.\n        \"\"\"\n        logger.info(f\"Initializing generator {self.alias},\")\n        options = options or GeneratorOptions()\n        if not isinstance(options, GeneratorOptions):\n            raise TypeError(\"options must be of type GeneratorOptions\")\n\n        self._vocs = vocs.copy()\n        self._options = options.copy()\n        self._is_done = False\n        self._data = pd.DataFrame()\n        self._check_options(self._options)\n\n    @abstractmethod\n    def generate(self, n_candidates) -&gt; pd.DataFrame:\n\"\"\"\n        generate `n_candidates` candidates\n\n        \"\"\"\n        pass\n\n    def add_data(self, new_data: pd.DataFrame):\n\"\"\"\n        update dataframe with results from new evaluations.\n\n        This is intended for generators that maintain their own data.\n\n        \"\"\"\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def default_options() -&gt; Type[GeneratorOptions]:\n\"\"\"\n        Get the default options for the generator.\n        \"\"\"\n        pass\n\n    def _check_options(self, options: XoptBaseModel):\n\"\"\"\n        Raise error if options are not compatable, overwrite in each generator if needed\n        \"\"\"\n        pass\n\n    @property\n    def is_done(self):\n        return self._is_done\n\n    @property\n    def data(self):\n        return self._data\n\n    @data.setter\n    def data(self, value: pd.DataFrame):\n        self._data = pd.DataFrame(value)\n\n    @property\n    def vocs(self):\n        return self._vocs\n\n    @property\n    def options(self):\n        return self._options\n</code></pre>"},{"location":"api/generators/#xopt.generator.Generator.__init__","title":"<code>__init__(vocs, options=None)</code>","text":"<p>Initialize the generator.</p> <p>Args:     vocs: The vocs to use.     options: The options to use.</p> Source code in <code>xopt/generator.py</code> <pre><code>def __init__(self, vocs: VOCS, options: GeneratorOptions = None):\n\"\"\"\n    Initialize the generator.\n\n    Args:\n        vocs: The vocs to use.\n        options: The options to use.\n    \"\"\"\n    logger.info(f\"Initializing generator {self.alias},\")\n    options = options or GeneratorOptions()\n    if not isinstance(options, GeneratorOptions):\n        raise TypeError(\"options must be of type GeneratorOptions\")\n\n    self._vocs = vocs.copy()\n    self._options = options.copy()\n    self._is_done = False\n    self._data = pd.DataFrame()\n    self._check_options(self._options)\n</code></pre>"},{"location":"api/generators/#xopt.generator.Generator.add_data","title":"<code>add_data(new_data)</code>","text":"<p>update dataframe with results from new evaluations.</p> <p>This is intended for generators that maintain their own data.</p> Source code in <code>xopt/generator.py</code> <pre><code>def add_data(self, new_data: pd.DataFrame):\n\"\"\"\n    update dataframe with results from new evaluations.\n\n    This is intended for generators that maintain their own data.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#xopt.generator.Generator.default_options","title":"<code>default_options()</code>  <code>staticmethod</code> <code>abstractmethod</code>","text":"<p>Get the default options for the generator.</p> Source code in <code>xopt/generator.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef default_options() -&gt; Type[GeneratorOptions]:\n\"\"\"\n    Get the default options for the generator.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#xopt.generator.Generator.generate","title":"<code>generate(n_candidates)</code>  <code>abstractmethod</code>","text":"<p>generate <code>n_candidates</code> candidates</p> Source code in <code>xopt/generator.py</code> <pre><code>@abstractmethod\ndef generate(self, n_candidates) -&gt; pd.DataFrame:\n\"\"\"\n    generate `n_candidates` candidates\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/generators/#xopt.generator.GeneratorOptions","title":"<code>GeneratorOptions</code>","text":"<p>         Bases: <code>XoptBaseModel</code></p> <p>Options for the generator.</p> Source code in <code>xopt/generator.py</code> <pre><code>class GeneratorOptions(XoptBaseModel):\n\"\"\"\n    Options for the generator.\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n        json_encoders = JSON_ENCODERS\n        extra = \"forbid\"\n        allow_mutation = True\n</code></pre>"},{"location":"api/vocs/","title":"Vocs","text":""},{"location":"api/vocs/#xopt.vocs.VOCS","title":"<code>xopt.vocs.VOCS</code>","text":"<p>         Bases: <code>XoptBaseModel</code></p> <p>Variables, Objectives, Constraints, and other Settings (VOCS) data structure to describe optimization problems.</p> Source code in <code>xopt/vocs.py</code> <pre><code>class VOCS(XoptBaseModel):\n\"\"\"\n    Variables, Objectives, Constraints, and other Settings (VOCS) data structure\n    to describe optimization problems.\n    \"\"\"\n\n    variables: Dict[str, conlist(float, min_items=2, max_items=2)] = {}\n    constraints: Dict[\n        str, conlist(Union[float, ConstraintEnum], min_items=2, max_items=2)\n    ] = {}\n    objectives: Dict[str, ObjectiveEnum] = {}\n    constants: Dict[str, Any] = {}\n    linked_variables: Dict[str, str] = {}\n\n    class Config:\n        validate_assignment = True  # Not sure this helps in this case\n        use_enum_values = True\n\n    @classmethod\n    def from_yaml(cls, yaml_text):\n        return cls.parse_obj(yaml.safe_load(yaml_text))\n\n    def as_yaml(self):\n        return yaml.dump(self.dict(), default_flow_style=None, sort_keys=False)\n\n    @property\n    def bounds(self):\n\"\"\"\n        Returns a bounds array (mins, maxs) of shape (2, n_variables)\n        Arrays of lower and upper bounds can be extracted by:\n            mins, maxs = vocs.bounds\n        \"\"\"\n        return np.array([v for _, v in sorted(self.variables.items())]).T\n\n    @property\n    def variable_names(self):\n\"\"\"Returns a sorted list of variable names\"\"\"\n        return list(sorted(self.variables.keys()))\n\n    @property\n    def objective_names(self):\n\"\"\"Returns a sorted list of objective names\"\"\"\n        return list(sorted(self.objectives.keys()))\n\n    @property\n    def constraint_names(self):\n\"\"\"Returns a sorted list of constraint names\"\"\"\n        if self.constraints is None:\n            return []\n        return list(sorted(self.constraints.keys()))\n\n    @property\n    def output_names(self):\n\"\"\"\n        Returns a sorted list of objective and constraint names (objectives first\n        then constraints)\n        \"\"\"\n        return self.objective_names + self.constraint_names\n\n    @property\n    def constant_names(self):\n\"\"\"Returns a sorted list of constraint names\"\"\"\n        if self.constants is None:\n            return []\n        return list(sorted(self.constants.keys()))\n\n    @property\n    def all_names(self):\n\"\"\"Returns all vocs names (variables, constants, objectives, constraints\"\"\"\n        return (\n            self.variable_names\n            + self.constant_names\n            + self.objective_names\n            + self.constraint_names\n        )\n\n    @property\n    def n_variables(self):\n\"\"\"Returns the number of variables\"\"\"\n        return len(self.variables)\n\n    @property\n    def n_constants(self):\n\"\"\"Returns the number of constants\"\"\"\n        return len(self.constants)\n\n    @property\n    def n_inputs(self):\n\"\"\"Returns the number of inputs (variables and constants)\"\"\"\n        return self.n_variables + self.n_constants\n\n    @property\n    def n_objectives(self):\n\"\"\"Returns the number of objectives\"\"\"\n        return len(self.objectives)\n\n    @property\n    def n_constraints(self):\n\"\"\"Returns the number of constraints\"\"\"\n        return len(self.constraints)\n\n    @property\n    def n_outputs(self):\n\"\"\"Returns the number of outputs (objectives and constraints)\"\"\"\n        return self.n_objectives + self.n_constraints\n\n    def random_inputs(\n        self, n=None, include_constants=True, include_linked_variables=True\n    ):\n\"\"\"\n        Uniform sampling of the variables.\n\n        Returns a dict of inputs.\n\n        If include_constants, the vocs.constants are added to the dict.\n\n        Optional:\n            n (integer) to make arrays of inputs, of size n.\n\n        \"\"\"\n        inputs = {}\n        for key, val in self.variables.items():  # No need to sort here\n            a, b = val\n            x = np.random.random(n)\n            inputs[key] = x * a + (1 - x) * b\n\n        # Constants\n        if include_constants and self.constants is not None:\n            inputs.update(self.constants)\n\n        # Handle linked variables\n        if include_linked_variables and self.linked_variables is not None:\n            for k, v in self.linked_variables.items():\n                inputs[k] = inputs[v]\n\n        # return pd.DataFrame(inputs, index=range(n))\n        return inputs\n\n    def convert_dataframe_to_inputs(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Extracts only inputs from a dataframe.\n        This will add constants.\n        \"\"\"\n        # make sure that the df keys contain the vocs variables\n        if not set(self.variable_names).issubset(set(data.keys())):\n            raise RuntimeError(\n                \"input dataframe must at least contain the vocs variables\"\n            )\n\n        # only keep the variables\n        in_copy = data[self.variable_names].copy()\n\n        # append constants\n        constants = self.constants\n        if constants is not None:\n            for name, val in constants.items():\n                in_copy[name] = val\n\n        return in_copy\n\n    def convert_numpy_to_inputs(self, inputs: np.ndarray) -&gt; pd.DataFrame:\n\"\"\"\n        convert 2D numpy array to list of dicts (inputs) for evaluation\n        Assumes that the columns of the array match correspond to\n        `sorted(self.vocs.variables.keys())\n\n        \"\"\"\n        df = pd.DataFrame(inputs, columns=self.variable_names)\n        return self.convert_dataframe_to_inputs(df)\n\n    # Extract optimization data (in correct column order)\n    def variable_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"variable_\",\n    ) -&gt; pd.DataFrame:\n\"\"\"\n        Returns a dataframe containing variables according to `vocs.variables` in sorted\n        order\n\n        Args:\n            data: Data to be processed.\n            prefix: Prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_variable_data(self.variables, data, prefix=prefix)\n\n    def objective_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"objective_\",\n        return_raw=False,\n    ) -&gt; pd.DataFrame:\n\"\"\"\n        Returns a dataframe containing objective data transformed according to\n        `vocs.objectives` such that we always assume minimization.\n\n        Args:\n            data: data to be processed.\n            prefix: prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_objective_data(self.objectives, data, prefix, return_raw)\n\n    def constraint_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"constraint_\",\n    ) -&gt; pd.DataFrame:\n\"\"\"\n        Returns a dataframe containing constraint data transformed according to\n        `vocs.constraints` such that values that satisfy each constraint are negative.\n\n        Args:\n            data: data to be processed.\n            prefix: prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_constraint_data(self.constraints, data, prefix)\n\n    def feasibility_data(\n        self,\n        data: Union[pd.DataFrame, List[Dict], List[Dict]],\n        prefix: str = \"feasible_\",\n    ) -&gt; pd.DataFrame:\n\"\"\"\n        Returns a dataframe containing booleans denoting if a constraint is satisfied or\n        not. Returned dataframe also contains a column `feasibility` which denotes if\n        all constraints are satisfied.\n\n        Args:\n            data: data to be processed.\n            prefix: prefix added to column names.\n\n        Returns:\n            result: processed Dataframe\n        \"\"\"\n        return form_feasibility_data(self.constraints, data, prefix)\n\n    def validate_input_data(self, input_points: pd.DataFrame) -&gt; None:\n\"\"\"\n        Validates input data. Raises an error if the input data does not satisfy\n        requirements given by vocs.\n\n        Args:\n            input_points: input data to be validated.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: if input data does not satisfy requirements.\n        \"\"\"\n        validate_input_data(self, input_points)\n\n    def extract_data(self, data: pd.DataFrame, return_raw=False):\n\"\"\"\n        split dataframe into seperate dataframes for variables, objectives and\n        constraints based on vocs - objective data is transformed based on\n        `vocs.objectives` properties\n\n        Args:\n            data: dataframe to be split\n            return_raw: if True, return untransformed objective data\n\n        Returns:\n            variable_data: dataframe containing variable data\n            objective_data: dataframe containing objective data\n            constraint_data: dataframe containing constraint data\n        \"\"\"\n        variable_data = self.variable_data(data, \"\")\n        objective_data = self.objective_data(data, \"\", return_raw)\n        constraint_data = self.constraint_data(data, \"\")\n        return variable_data, objective_data, constraint_data\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.all_names","title":"<code>all_names</code>  <code>property</code>","text":"<p>Returns all vocs names (variables, constants, objectives, constraints</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.bounds","title":"<code>bounds</code>  <code>property</code>","text":"<p>Returns a bounds array (mins, maxs) of shape (2, n_variables) Arrays of lower and upper bounds can be extracted by:     mins, maxs = vocs.bounds</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.constant_names","title":"<code>constant_names</code>  <code>property</code>","text":"<p>Returns a sorted list of constraint names</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.constraint_names","title":"<code>constraint_names</code>  <code>property</code>","text":"<p>Returns a sorted list of constraint names</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_constants","title":"<code>n_constants</code>  <code>property</code>","text":"<p>Returns the number of constants</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_constraints","title":"<code>n_constraints</code>  <code>property</code>","text":"<p>Returns the number of constraints</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_inputs","title":"<code>n_inputs</code>  <code>property</code>","text":"<p>Returns the number of inputs (variables and constants)</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_objectives","title":"<code>n_objectives</code>  <code>property</code>","text":"<p>Returns the number of objectives</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_outputs","title":"<code>n_outputs</code>  <code>property</code>","text":"<p>Returns the number of outputs (objectives and constraints)</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.n_variables","title":"<code>n_variables</code>  <code>property</code>","text":"<p>Returns the number of variables</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.objective_names","title":"<code>objective_names</code>  <code>property</code>","text":"<p>Returns a sorted list of objective names</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.output_names","title":"<code>output_names</code>  <code>property</code>","text":"<p>Returns a sorted list of objective and constraint names (objectives first then constraints)</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.variable_names","title":"<code>variable_names</code>  <code>property</code>","text":"<p>Returns a sorted list of variable names</p>"},{"location":"api/vocs/#xopt.vocs.VOCS.constraint_data","title":"<code>constraint_data(data, prefix='constraint_')</code>","text":"<p>Returns a dataframe containing constraint data transformed according to <code>vocs.constraints</code> such that values that satisfy each constraint are negative.</p> <p>Args:     data: data to be processed.     prefix: prefix added to column names.</p> <p>Returns:     result: processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def constraint_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"constraint_\",\n) -&gt; pd.DataFrame:\n\"\"\"\n    Returns a dataframe containing constraint data transformed according to\n    `vocs.constraints` such that values that satisfy each constraint are negative.\n\n    Args:\n        data: data to be processed.\n        prefix: prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_constraint_data(self.constraints, data, prefix)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.convert_dataframe_to_inputs","title":"<code>convert_dataframe_to_inputs(data)</code>","text":"<p>Extracts only inputs from a dataframe. This will add constants.</p> Source code in <code>xopt/vocs.py</code> <pre><code>def convert_dataframe_to_inputs(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Extracts only inputs from a dataframe.\n    This will add constants.\n    \"\"\"\n    # make sure that the df keys contain the vocs variables\n    if not set(self.variable_names).issubset(set(data.keys())):\n        raise RuntimeError(\n            \"input dataframe must at least contain the vocs variables\"\n        )\n\n    # only keep the variables\n    in_copy = data[self.variable_names].copy()\n\n    # append constants\n    constants = self.constants\n    if constants is not None:\n        for name, val in constants.items():\n            in_copy[name] = val\n\n    return in_copy\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.convert_numpy_to_inputs","title":"<code>convert_numpy_to_inputs(inputs)</code>","text":"<p>convert 2D numpy array to list of dicts (inputs) for evaluation Assumes that the columns of the array match correspond to `sorted(self.vocs.variables.keys())</p> Source code in <code>xopt/vocs.py</code> <pre><code>def convert_numpy_to_inputs(self, inputs: np.ndarray) -&gt; pd.DataFrame:\n\"\"\"\n    convert 2D numpy array to list of dicts (inputs) for evaluation\n    Assumes that the columns of the array match correspond to\n    `sorted(self.vocs.variables.keys())\n\n    \"\"\"\n    df = pd.DataFrame(inputs, columns=self.variable_names)\n    return self.convert_dataframe_to_inputs(df)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.extract_data","title":"<code>extract_data(data, return_raw=False)</code>","text":"<p>split dataframe into seperate dataframes for variables, objectives and constraints based on vocs - objective data is transformed based on <code>vocs.objectives</code> properties</p> <p>Args:     data: dataframe to be split     return_raw: if True, return untransformed objective data</p> <p>Returns:     variable_data: dataframe containing variable data     objective_data: dataframe containing objective data     constraint_data: dataframe containing constraint data</p> Source code in <code>xopt/vocs.py</code> <pre><code>def extract_data(self, data: pd.DataFrame, return_raw=False):\n\"\"\"\n    split dataframe into seperate dataframes for variables, objectives and\n    constraints based on vocs - objective data is transformed based on\n    `vocs.objectives` properties\n\n    Args:\n        data: dataframe to be split\n        return_raw: if True, return untransformed objective data\n\n    Returns:\n        variable_data: dataframe containing variable data\n        objective_data: dataframe containing objective data\n        constraint_data: dataframe containing constraint data\n    \"\"\"\n    variable_data = self.variable_data(data, \"\")\n    objective_data = self.objective_data(data, \"\", return_raw)\n    constraint_data = self.constraint_data(data, \"\")\n    return variable_data, objective_data, constraint_data\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.feasibility_data","title":"<code>feasibility_data(data, prefix='feasible_')</code>","text":"<p>Returns a dataframe containing booleans denoting if a constraint is satisfied or not. Returned dataframe also contains a column <code>feasibility</code> which denotes if all constraints are satisfied.</p> <p>Args:     data: data to be processed.     prefix: prefix added to column names.</p> <p>Returns:     result: processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def feasibility_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"feasible_\",\n) -&gt; pd.DataFrame:\n\"\"\"\n    Returns a dataframe containing booleans denoting if a constraint is satisfied or\n    not. Returned dataframe also contains a column `feasibility` which denotes if\n    all constraints are satisfied.\n\n    Args:\n        data: data to be processed.\n        prefix: prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_feasibility_data(self.constraints, data, prefix)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.objective_data","title":"<code>objective_data(data, prefix='objective_', return_raw=False)</code>","text":"<p>Returns a dataframe containing objective data transformed according to <code>vocs.objectives</code> such that we always assume minimization.</p> <p>Args:     data: data to be processed.     prefix: prefix added to column names.</p> <p>Returns:     result: processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def objective_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"objective_\",\n    return_raw=False,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Returns a dataframe containing objective data transformed according to\n    `vocs.objectives` such that we always assume minimization.\n\n    Args:\n        data: data to be processed.\n        prefix: prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_objective_data(self.objectives, data, prefix, return_raw)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.random_inputs","title":"<code>random_inputs(n=None, include_constants=True, include_linked_variables=True)</code>","text":"<p>Uniform sampling of the variables.</p> <p>Returns a dict of inputs.</p> <p>If include_constants, the vocs.constants are added to the dict.</p> <p>Optional:     n (integer) to make arrays of inputs, of size n.</p> Source code in <code>xopt/vocs.py</code> <pre><code>def random_inputs(\n    self, n=None, include_constants=True, include_linked_variables=True\n):\n\"\"\"\n    Uniform sampling of the variables.\n\n    Returns a dict of inputs.\n\n    If include_constants, the vocs.constants are added to the dict.\n\n    Optional:\n        n (integer) to make arrays of inputs, of size n.\n\n    \"\"\"\n    inputs = {}\n    for key, val in self.variables.items():  # No need to sort here\n        a, b = val\n        x = np.random.random(n)\n        inputs[key] = x * a + (1 - x) * b\n\n    # Constants\n    if include_constants and self.constants is not None:\n        inputs.update(self.constants)\n\n    # Handle linked variables\n    if include_linked_variables and self.linked_variables is not None:\n        for k, v in self.linked_variables.items():\n            inputs[k] = inputs[v]\n\n    # return pd.DataFrame(inputs, index=range(n))\n    return inputs\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.validate_input_data","title":"<code>validate_input_data(input_points)</code>","text":"<p>Validates input data. Raises an error if the input data does not satisfy requirements given by vocs.</p> <p>Args:     input_points: input data to be validated.</p> <p>Returns:     None</p> <p>Raises:     ValueError: if input data does not satisfy requirements.</p> Source code in <code>xopt/vocs.py</code> <pre><code>def validate_input_data(self, input_points: pd.DataFrame) -&gt; None:\n\"\"\"\n    Validates input data. Raises an error if the input data does not satisfy\n    requirements given by vocs.\n\n    Args:\n        input_points: input data to be validated.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: if input data does not satisfy requirements.\n    \"\"\"\n    validate_input_data(self, input_points)\n</code></pre>"},{"location":"api/vocs/#xopt.vocs.VOCS.variable_data","title":"<code>variable_data(data, prefix='variable_')</code>","text":"<p>Returns a dataframe containing variables according to <code>vocs.variables</code> in sorted order</p> <p>Args:     data: Data to be processed.     prefix: Prefix added to column names.</p> <p>Returns:     result: processed Dataframe</p> Source code in <code>xopt/vocs.py</code> <pre><code>def variable_data(\n    self,\n    data: Union[pd.DataFrame, List[Dict], List[Dict]],\n    prefix: str = \"variable_\",\n) -&gt; pd.DataFrame:\n\"\"\"\n    Returns a dataframe containing variables according to `vocs.variables` in sorted\n    order\n\n    Args:\n        data: Data to be processed.\n        prefix: Prefix added to column names.\n\n    Returns:\n        result: processed Dataframe\n    \"\"\"\n    return form_variable_data(self.variables, data, prefix=prefix)\n</code></pre>"},{"location":"api/xopt/","title":"Xopt","text":""},{"location":"api/xopt/#xopt.Xopt","title":"<code>xopt.Xopt</code>","text":"<p>Object to handle a single optimization problem.</p> Source code in <code>xopt/base.py</code> <pre><code>class Xopt:\n\"\"\"\n\n    Object to handle a single optimization problem.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        config: dict = None,\n        *,\n        generator: Generator = None,\n        evaluator: Evaluator = None,\n        vocs: VOCS = None,\n        options: XoptOptions = None,\n        data: pd.DataFrame = None,\n    ):\n\"\"\"\n        Initialize Xopt object using either a config dictionary or explicitly\n\n        Args:\n            config: dict, or YAML or JSON str or file. This overrides all other arguments.\n\n            generator: Generator object\n            evaluator: Evaluator object\n            vocs: VOCS object\n            options: XoptOptions object\n            data: initial data to use\n\n        \"\"\"\n        logger.info(\"Initializing Xopt object\")\n\n        # if config is provided, load it and re-init. Otherwise, init normally.\n        if config is not None:\n            self.__init__(**parse_config(config))\n            # TODO: Allow overrides\n            return\n\n        # initialize Xopt object\n        self._generator = generator\n        self._evaluator = evaluator\n        self._vocs = vocs\n\n        logger.debug(f\"Xopt initialized with generator: {self._generator}\")\n        logger.debug(f\"Xopt initialized with evaluator: {self._evaluator}\")\n\n        self.options = options or XoptOptions()\n        logger.debug(f\"Xopt initialized with options: {self.options.dict()}\")\n\n        # add data to xopt object and generator\n        self._new_data = pd.DataFrame()\n        self._data = pd.DataFrame()\n        if data is not None:\n            self.add_data(data)\n\n        self._futures = {}  # unfinished futures\n        self._input_data = None  # dataframe for unfinished futures inputs\n        self._ix_last = len(self.data)  # index of last sample generated\n        self._is_done = False\n        self.n_unfinished_futures = 0\n\n        # check internals\n        self.check_components()\n        logger.info(\"Xopt object initialized\")\n\n    def run(self):\n\"\"\"run until either xopt is done or the generator is done\"\"\"\n        while not self.is_done:\n            # Stopping criteria\n            if self.options.max_evaluations:\n                if len(self.data) &gt;= self.options.max_evaluations:\n                    self._is_done = True\n                    logger.info(\n                        \"Xopt is done. \"\n                        f\"Max evaluations {self.options.max_evaluations} reached.\"\n                    )\n                    break\n\n            self.step()\n\n    def evaluate_data(self, input_data: pd.DataFrame):\n\"\"\"\n        Evaluate data using the evaluator.\n        Adds to the internal dataframe.\n        \"\"\"\n        logger.debug(f\"Evaluating {len(input_data)} inputs\")\n        input_data = self.prepare_input_data(input_data)\n        output_data = self.evaluator.evaluate_data(input_data)\n\n        if self.options.strict:\n            validate_outputs(output_data)\n        new_data = pd.concat([input_data, output_data], axis=1)\n\n        self.add_data(new_data)\n        return new_data\n\n    def submit_data(self, input_data: pd.DataFrame):\n\"\"\"\n        Submit data to evaluator and return futures indexed to internal futures list.\n\n        Args:\n            input_data: dataframe containing input data\n\n        \"\"\"\n        logger.debug(f\"Submitting {len(input_data)} inputs\")\n        input_data = self.prepare_input_data(input_data)\n\n        # submit data to evaluator. Futures are keyed on the index of the input data.\n        futures = self.evaluator.submit_data(input_data)\n        index = input_data.index\n        # Special handling for vectorized evaluations\n        if self.evaluator.vectorized:\n            assert len(futures) == 1\n            new_futures = {tuple(index): futures[0]}\n        else:\n            new_futures = dict(zip(index, futures))\n\n        # add futures to internal list\n        for key, future in new_futures.items():\n            assert key not in self._futures\n            self._futures[key] = future\n        # self._futures.update(new_futures)\n        return futures\n\n    def prepare_input_data(self, input_data: pd.DataFrame):\n\"\"\"\n        re-index and validate input data.\n        \"\"\"\n        input_data = pd.DataFrame(input_data, copy=True)  # copy for reindexing\n\n        # Reindex input dataframe\n        input_data.index = np.arange(\n            self._ix_last + 1, self._ix_last + 1 + len(input_data)\n        )\n        self._ix_last += len(input_data)\n        self._input_data = pd.concat([self._input_data, input_data])\n\n        # validate data before submission\n        self.vocs.validate_input_data(self._input_data)\n\n        return input_data\n\n    def step(self):\n\"\"\"\n        run one optimization cycle\n\n        - determine the number of candidates to request from the generator\n        - pass candidate request to generator\n        - submit candidates to evaluator\n        - wait until all (asynch == False) or at least one (asynch == True) evaluation\n            is finished\n        - update data storage and generator data storage (if applicable)\n\n        \"\"\"\n        logger.info(\"Running Xopt step\")\n\n        # check if Xopt is set up to step\n        self.check_components()\n\n        if self.is_done:\n            logger.debug(\"Xopt is done, will not step.\")\n            return\n\n        # get number of candidates to generate\n        if self.options.asynch:\n            n_generate = self.evaluator.max_workers - self.n_unfinished_futures\n        else:\n            n_generate = self.evaluator.max_workers\n\n        # generate samples and submit to evaluator\n        logger.debug(f\"Generating {n_generate} candidates\")\n        new_samples = pd.DataFrame(self.generator.generate(n_generate))\n\n        # generator is done when it returns no new samples\n        if len(new_samples) == 0:\n            logger.debug(\"Generator returned 0 samples =&gt; optimization is done.\")\n            assert self.generator.is_done\n            return\n\n        #  Blocking submission/evaluation\n        if self.options.asynch:\n            # Submit data\n            self.submit_data(new_samples)\n            # Process futures\n            self.n_unfinished_futures = self.process_futures()\n        else:\n            # Evaluate data\n            self.evaluate_data(new_samples)\n\n        # dump data to file if specified\n        self.dump_state()\n\n    def process_futures(self):\n\"\"\"\n        wait for futures to finish (specified by asynch) and then internal dataframes\n        of Xopt and generator, finally return the number of unfinished futures\n\n        \"\"\"\n        if self.options.asynch:\n            logger.debug(\"Waiting for at least one future to complete\")\n            return_when = concurrent.futures.FIRST_COMPLETED\n        else:\n            logger.debug(\"Waiting for all futures to complete\")\n            return_when = concurrent.futures.ALL_COMPLETED\n        logger.debug(f\"done. {self.n_unfinished_futures} futures remaining\")\n\n        # wait for futures to finish (depending on return_when)\n        finished_futures, unfinished_futures = concurrent.futures.wait(\n            self._futures.values(), None, return_when\n        )\n\n        # Get done indexes.\n        ix_done = [ix for ix, future in self._futures.items() if future.done()]\n\n        # Get results from futures\n        output_data = []\n        for ix in ix_done:\n            future = self._futures.pop(ix)  # remove from futures\n            outputs = future.result()  # Exceptions are already handled by the evaluator\n            if self.options.strict:\n                if future.exception() is not None:\n                    raise future.exception()\n                validate_outputs(outputs)\n            output_data.append(outputs)\n\n        # Special handling of a vectorized futures.\n        # Dict keys have all indexes of the input data.\n        if self.evaluator.vectorized:\n            output_data = pd.concat([pd.DataFrame([output]) for output in output_data])\n            index = []\n            for ix in ix_done:\n                index.extend(list(ix))\n        else:\n            index = ix_done\n\n        # Collect done inputs and outputs\n        input_data_done = self._input_data.loc[index]\n        output_data = pd.DataFrame(output_data, index=index)\n\n        # Form completed evaluation\n        new_data = pd.concat([input_data_done, output_data], axis=1)\n\n        # Add to internal dataframes\n        self.add_data(new_data)\n\n        # Cleanup\n        self._input_data.drop(index, inplace=True)\n\n        return len(unfinished_futures)\n\n    def check_components(self):\n\"\"\"check to make sure everything is in place to step\"\"\"\n        if not isinstance(self.options, XoptOptions):\n            raise ValueError(\"options must of type `XoptOptions`\")\n\n        if self.generator is None:\n            raise XoptError(\"Xopt generator not specified\")\n\n        if self.evaluator is None:\n            raise XoptError(\"Xopt evaluator not specified\")\n\n        if self.vocs is None:\n            raise XoptError(\"Xopt VOCS is not specified\")\n\n    def dump_state(self):\n\"\"\"dump data to file\"\"\"\n        if self.options.dump_file is not None:\n            output = state_to_dict(self)\n            with open(self.options.dump_file, \"w\") as f:\n                yaml.dump(output, f)\n            logger.debug(f\"Dumped state to YAML file: {self.options.dump_file}\")\n\n    @property\n    def data(self):\n        return self._data\n\n    @data.setter\n    def data(self, data: pd.DataFrame):\n        # Replace xopt dataframe\n        self._data = pd.DataFrame(data)\n\n        # do not do anything with generator.\n        # Generator data should be handled with add_data.\n\n    def add_data(self, new_data: pd.DataFrame):\n\"\"\"\n        Concatenate new data to internal dataframe,\n        and also adds this data to the generator if it exists.\n        \"\"\"\n        logger.debug(f\"Adding {len(new_data)} new data to internal dataframes\")\n\n        # Set internal dataframe. Don't use self.data =\n        new_data = pd.DataFrame(new_data)\n        self._data = pd.concat([self._data, new_data], axis=0)\n        self._new_data = new_data\n\n        if self.generator is not None:\n            self.generator.add_data(new_data)\n\n    @property\n    def is_done(self):\n        return self._is_done or self.generator.is_done\n\n    @property\n    def new_data(self):\n        return self._new_data\n\n    @property\n    def vocs(self):\n        return self._vocs\n\n    @property\n    def evaluator(self):\n        return self._evaluator\n\n    @property\n    def generator(self):\n        return self._generator\n\n    @classmethod\n    def from_dict(cls, config_dict):\n        pass\n        # return cls(**xopt_kwargs_from_dict(config_dict))\n\n    @classmethod\n    def from_yaml(cls, yaml_str):\n        if os.path.exists(yaml_str):\n            yaml_str = open(yaml_str)\n        return cls.from_dict(yaml.safe_load(yaml_str))\n\n    def yaml(self, filename=None, *, include_data=False):\n\"\"\"\n        YAML representation of the Xopt object.\n        \"\"\"\n        config = state_to_dict(self, include_data=include_data)\n        s = yaml.dump(config, default_flow_style=None, sort_keys=False)\n\n        if filename:\n            with open(filename, \"w\") as f:\n                f.write(s)\n\n        return s\n\n    def __repr__(self):\n\"\"\"\n        Returns infor about the Xopt object, including the YAML representation without data.\n        \"\"\"\n        return f\"\"\"\n            Xopt\n________________________________\nVersion: {__version__}\nData size: {len(self.data)}\nConfig as YAML:\n{self.yaml()}\n\"\"\"\n\n    def __str__(self):\n        return self.__repr__()\n\n    # Convenience methods\n\n    def random_inputs(self, *args, **kwargs):\n\"\"\"\n        Convenence method to call vocs.random_inputs\n        \"\"\"\n        return self.vocs.random_inputs(*args, **kwargs)\n\n    def evaluate(self, inputs: Dict, **kwargs):\n\"\"\"\n        Convenience method to call evaluator.evaluate\n        \"\"\"\n        return self.evaluator.evaluate(inputs, **kwargs)\n\n    def random_evaluate(self, *args, **kwargs):\n\"\"\"\n        Convenience method to generate random inputs using vocs\n        and evaluate them using evaluator.evaluate.\n        \"\"\"\n        result = self.evaluate(self.random_inputs(*args, **kwargs))\n        return result\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.__init__","title":"<code>__init__(config=None, *, generator=None, evaluator=None, vocs=None, options=None, data=None)</code>","text":"<p>Initialize Xopt object using either a config dictionary or explicitly</p> <p>Args:     config: dict, or YAML or JSON str or file. This overrides all other arguments.</p> <pre><code>generator: Generator object\nevaluator: Evaluator object\nvocs: VOCS object\noptions: XoptOptions object\ndata: initial data to use\n</code></pre> Source code in <code>xopt/base.py</code> <pre><code>def __init__(\n    self,\n    config: dict = None,\n    *,\n    generator: Generator = None,\n    evaluator: Evaluator = None,\n    vocs: VOCS = None,\n    options: XoptOptions = None,\n    data: pd.DataFrame = None,\n):\n\"\"\"\n    Initialize Xopt object using either a config dictionary or explicitly\n\n    Args:\n        config: dict, or YAML or JSON str or file. This overrides all other arguments.\n\n        generator: Generator object\n        evaluator: Evaluator object\n        vocs: VOCS object\n        options: XoptOptions object\n        data: initial data to use\n\n    \"\"\"\n    logger.info(\"Initializing Xopt object\")\n\n    # if config is provided, load it and re-init. Otherwise, init normally.\n    if config is not None:\n        self.__init__(**parse_config(config))\n        # TODO: Allow overrides\n        return\n\n    # initialize Xopt object\n    self._generator = generator\n    self._evaluator = evaluator\n    self._vocs = vocs\n\n    logger.debug(f\"Xopt initialized with generator: {self._generator}\")\n    logger.debug(f\"Xopt initialized with evaluator: {self._evaluator}\")\n\n    self.options = options or XoptOptions()\n    logger.debug(f\"Xopt initialized with options: {self.options.dict()}\")\n\n    # add data to xopt object and generator\n    self._new_data = pd.DataFrame()\n    self._data = pd.DataFrame()\n    if data is not None:\n        self.add_data(data)\n\n    self._futures = {}  # unfinished futures\n    self._input_data = None  # dataframe for unfinished futures inputs\n    self._ix_last = len(self.data)  # index of last sample generated\n    self._is_done = False\n    self.n_unfinished_futures = 0\n\n    # check internals\n    self.check_components()\n    logger.info(\"Xopt object initialized\")\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.add_data","title":"<code>add_data(new_data)</code>","text":"<p>Concatenate new data to internal dataframe, and also adds this data to the generator if it exists.</p> Source code in <code>xopt/base.py</code> <pre><code>def add_data(self, new_data: pd.DataFrame):\n\"\"\"\n    Concatenate new data to internal dataframe,\n    and also adds this data to the generator if it exists.\n    \"\"\"\n    logger.debug(f\"Adding {len(new_data)} new data to internal dataframes\")\n\n    # Set internal dataframe. Don't use self.data =\n    new_data = pd.DataFrame(new_data)\n    self._data = pd.concat([self._data, new_data], axis=0)\n    self._new_data = new_data\n\n    if self.generator is not None:\n        self.generator.add_data(new_data)\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.check_components","title":"<code>check_components()</code>","text":"<p>check to make sure everything is in place to step</p> Source code in <code>xopt/base.py</code> <pre><code>def check_components(self):\n\"\"\"check to make sure everything is in place to step\"\"\"\n    if not isinstance(self.options, XoptOptions):\n        raise ValueError(\"options must of type `XoptOptions`\")\n\n    if self.generator is None:\n        raise XoptError(\"Xopt generator not specified\")\n\n    if self.evaluator is None:\n        raise XoptError(\"Xopt evaluator not specified\")\n\n    if self.vocs is None:\n        raise XoptError(\"Xopt VOCS is not specified\")\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.dump_state","title":"<code>dump_state()</code>","text":"<p>dump data to file</p> Source code in <code>xopt/base.py</code> <pre><code>def dump_state(self):\n\"\"\"dump data to file\"\"\"\n    if self.options.dump_file is not None:\n        output = state_to_dict(self)\n        with open(self.options.dump_file, \"w\") as f:\n            yaml.dump(output, f)\n        logger.debug(f\"Dumped state to YAML file: {self.options.dump_file}\")\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.evaluate","title":"<code>evaluate(inputs, **kwargs)</code>","text":"<p>Convenience method to call evaluator.evaluate</p> Source code in <code>xopt/base.py</code> <pre><code>def evaluate(self, inputs: Dict, **kwargs):\n\"\"\"\n    Convenience method to call evaluator.evaluate\n    \"\"\"\n    return self.evaluator.evaluate(inputs, **kwargs)\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.evaluate_data","title":"<code>evaluate_data(input_data)</code>","text":"<p>Evaluate data using the evaluator. Adds to the internal dataframe.</p> Source code in <code>xopt/base.py</code> <pre><code>def evaluate_data(self, input_data: pd.DataFrame):\n\"\"\"\n    Evaluate data using the evaluator.\n    Adds to the internal dataframe.\n    \"\"\"\n    logger.debug(f\"Evaluating {len(input_data)} inputs\")\n    input_data = self.prepare_input_data(input_data)\n    output_data = self.evaluator.evaluate_data(input_data)\n\n    if self.options.strict:\n        validate_outputs(output_data)\n    new_data = pd.concat([input_data, output_data], axis=1)\n\n    self.add_data(new_data)\n    return new_data\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.prepare_input_data","title":"<code>prepare_input_data(input_data)</code>","text":"<p>re-index and validate input data.</p> Source code in <code>xopt/base.py</code> <pre><code>def prepare_input_data(self, input_data: pd.DataFrame):\n\"\"\"\n    re-index and validate input data.\n    \"\"\"\n    input_data = pd.DataFrame(input_data, copy=True)  # copy for reindexing\n\n    # Reindex input dataframe\n    input_data.index = np.arange(\n        self._ix_last + 1, self._ix_last + 1 + len(input_data)\n    )\n    self._ix_last += len(input_data)\n    self._input_data = pd.concat([self._input_data, input_data])\n\n    # validate data before submission\n    self.vocs.validate_input_data(self._input_data)\n\n    return input_data\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.process_futures","title":"<code>process_futures()</code>","text":"<p>wait for futures to finish (specified by asynch) and then internal dataframes of Xopt and generator, finally return the number of unfinished futures</p> Source code in <code>xopt/base.py</code> <pre><code>def process_futures(self):\n\"\"\"\n    wait for futures to finish (specified by asynch) and then internal dataframes\n    of Xopt and generator, finally return the number of unfinished futures\n\n    \"\"\"\n    if self.options.asynch:\n        logger.debug(\"Waiting for at least one future to complete\")\n        return_when = concurrent.futures.FIRST_COMPLETED\n    else:\n        logger.debug(\"Waiting for all futures to complete\")\n        return_when = concurrent.futures.ALL_COMPLETED\n    logger.debug(f\"done. {self.n_unfinished_futures} futures remaining\")\n\n    # wait for futures to finish (depending on return_when)\n    finished_futures, unfinished_futures = concurrent.futures.wait(\n        self._futures.values(), None, return_when\n    )\n\n    # Get done indexes.\n    ix_done = [ix for ix, future in self._futures.items() if future.done()]\n\n    # Get results from futures\n    output_data = []\n    for ix in ix_done:\n        future = self._futures.pop(ix)  # remove from futures\n        outputs = future.result()  # Exceptions are already handled by the evaluator\n        if self.options.strict:\n            if future.exception() is not None:\n                raise future.exception()\n            validate_outputs(outputs)\n        output_data.append(outputs)\n\n    # Special handling of a vectorized futures.\n    # Dict keys have all indexes of the input data.\n    if self.evaluator.vectorized:\n        output_data = pd.concat([pd.DataFrame([output]) for output in output_data])\n        index = []\n        for ix in ix_done:\n            index.extend(list(ix))\n    else:\n        index = ix_done\n\n    # Collect done inputs and outputs\n    input_data_done = self._input_data.loc[index]\n    output_data = pd.DataFrame(output_data, index=index)\n\n    # Form completed evaluation\n    new_data = pd.concat([input_data_done, output_data], axis=1)\n\n    # Add to internal dataframes\n    self.add_data(new_data)\n\n    # Cleanup\n    self._input_data.drop(index, inplace=True)\n\n    return len(unfinished_futures)\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.random_evaluate","title":"<code>random_evaluate(*args, **kwargs)</code>","text":"<p>Convenience method to generate random inputs using vocs and evaluate them using evaluator.evaluate.</p> Source code in <code>xopt/base.py</code> <pre><code>def random_evaluate(self, *args, **kwargs):\n\"\"\"\n    Convenience method to generate random inputs using vocs\n    and evaluate them using evaluator.evaluate.\n    \"\"\"\n    result = self.evaluate(self.random_inputs(*args, **kwargs))\n    return result\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.random_inputs","title":"<code>random_inputs(*args, **kwargs)</code>","text":"<p>Convenence method to call vocs.random_inputs</p> Source code in <code>xopt/base.py</code> <pre><code>def random_inputs(self, *args, **kwargs):\n\"\"\"\n    Convenence method to call vocs.random_inputs\n    \"\"\"\n    return self.vocs.random_inputs(*args, **kwargs)\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.run","title":"<code>run()</code>","text":"<p>run until either xopt is done or the generator is done</p> Source code in <code>xopt/base.py</code> <pre><code>def run(self):\n\"\"\"run until either xopt is done or the generator is done\"\"\"\n    while not self.is_done:\n        # Stopping criteria\n        if self.options.max_evaluations:\n            if len(self.data) &gt;= self.options.max_evaluations:\n                self._is_done = True\n                logger.info(\n                    \"Xopt is done. \"\n                    f\"Max evaluations {self.options.max_evaluations} reached.\"\n                )\n                break\n\n        self.step()\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.step","title":"<code>step()</code>","text":"<p>run one optimization cycle</p> <ul> <li>determine the number of candidates to request from the generator</li> <li>pass candidate request to generator</li> <li>submit candidates to evaluator</li> <li>wait until all (asynch == False) or at least one (asynch == True) evaluation     is finished</li> <li>update data storage and generator data storage (if applicable)</li> </ul> Source code in <code>xopt/base.py</code> <pre><code>def step(self):\n\"\"\"\n    run one optimization cycle\n\n    - determine the number of candidates to request from the generator\n    - pass candidate request to generator\n    - submit candidates to evaluator\n    - wait until all (asynch == False) or at least one (asynch == True) evaluation\n        is finished\n    - update data storage and generator data storage (if applicable)\n\n    \"\"\"\n    logger.info(\"Running Xopt step\")\n\n    # check if Xopt is set up to step\n    self.check_components()\n\n    if self.is_done:\n        logger.debug(\"Xopt is done, will not step.\")\n        return\n\n    # get number of candidates to generate\n    if self.options.asynch:\n        n_generate = self.evaluator.max_workers - self.n_unfinished_futures\n    else:\n        n_generate = self.evaluator.max_workers\n\n    # generate samples and submit to evaluator\n    logger.debug(f\"Generating {n_generate} candidates\")\n    new_samples = pd.DataFrame(self.generator.generate(n_generate))\n\n    # generator is done when it returns no new samples\n    if len(new_samples) == 0:\n        logger.debug(\"Generator returned 0 samples =&gt; optimization is done.\")\n        assert self.generator.is_done\n        return\n\n    #  Blocking submission/evaluation\n    if self.options.asynch:\n        # Submit data\n        self.submit_data(new_samples)\n        # Process futures\n        self.n_unfinished_futures = self.process_futures()\n    else:\n        # Evaluate data\n        self.evaluate_data(new_samples)\n\n    # dump data to file if specified\n    self.dump_state()\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.submit_data","title":"<code>submit_data(input_data)</code>","text":"<p>Submit data to evaluator and return futures indexed to internal futures list.</p> <p>Args:     input_data: dataframe containing input data</p> Source code in <code>xopt/base.py</code> <pre><code>def submit_data(self, input_data: pd.DataFrame):\n\"\"\"\n    Submit data to evaluator and return futures indexed to internal futures list.\n\n    Args:\n        input_data: dataframe containing input data\n\n    \"\"\"\n    logger.debug(f\"Submitting {len(input_data)} inputs\")\n    input_data = self.prepare_input_data(input_data)\n\n    # submit data to evaluator. Futures are keyed on the index of the input data.\n    futures = self.evaluator.submit_data(input_data)\n    index = input_data.index\n    # Special handling for vectorized evaluations\n    if self.evaluator.vectorized:\n        assert len(futures) == 1\n        new_futures = {tuple(index): futures[0]}\n    else:\n        new_futures = dict(zip(index, futures))\n\n    # add futures to internal list\n    for key, future in new_futures.items():\n        assert key not in self._futures\n        self._futures[key] = future\n    # self._futures.update(new_futures)\n    return futures\n</code></pre>"},{"location":"api/xopt/#xopt.base.Xopt.yaml","title":"<code>yaml(filename=None, *, include_data=False)</code>","text":"<p>YAML representation of the Xopt object.</p> Source code in <code>xopt/base.py</code> <pre><code>def yaml(self, filename=None, *, include_data=False):\n\"\"\"\n    YAML representation of the Xopt object.\n    \"\"\"\n    config = state_to_dict(self, include_data=include_data)\n    s = yaml.dump(config, default_flow_style=None, sort_keys=False)\n\n    if filename:\n        with open(filename, \"w\") as f:\n            f.write(s)\n\n    return s\n</code></pre>"},{"location":"api/generators/bayesian/","title":"Bayesian generators","text":""},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator","title":"<code>xopt.generators.bayesian.bayesian_generator.BayesianGenerator</code>","text":"<p>         Bases: <code>Generator</code>, <code>ABC</code></p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>class BayesianGenerator(Generator, ABC):\n    def __init__(\n        self,\n        vocs: VOCS,\n        options: BayesianOptions = None,\n        supports_batch_generation=False,\n    ):\n        options = options or BayesianOptions()\n        if not isinstance(options, BayesianOptions):\n            raise ValueError(\"options must be of type BayesianOptions\")\n\n        super().__init__(vocs, options)\n\n        self._model = None\n        self._acquisition = None\n        self._trust_region = None\n        self.supports_batch_generation = supports_batch_generation\n        self.sampler = SobolQMCNormalSampler(self.options.acq.monte_carlo_samples)\n        self.model_constructor = get_model_constructor(options.model)(\n            self.vocs, options.model\n        )\n\n        # set up turbo if requested\n        if self.options.optim.use_turbo:\n            self.turbo_state = TurboState(self.vocs.n_variables, 1)\n            self.first_call = True\n\n    @staticmethod\n    def default_options() -&gt; BayesianOptions:\n        return BayesianOptions()\n\n    def add_data(self, new_data: pd.DataFrame):\n        self.data = pd.concat([self.data, new_data], axis=0)\n\n    def generate(self, n_candidates: int) -&gt; List[Dict]:\n        if n_candidates &gt; 1 and not self.supports_batch_generation:\n            raise NotImplementedError(\n                \"This Bayesian algorithm does not currently support parallel candidate \"\n                \"generation\"\n            )\n\n        # if no data exists use random generator to generate candidates\n        if self.data.empty:\n            return self.vocs.random_inputs(self.options.n_initial)\n\n        else:\n            # update internal model with internal data\n            model = self.train_model(self.data)\n\n            # calculate optimization bounds\n            bounds = self._get_optimization_bounds()\n\n            # get acquisition function\n            acq_funct = self.get_acquisition(model)\n\n            # get candidates\n            candidates = self.optimize_acqf(acq_funct, bounds, n_candidates)\n\n            # post process candidates\n            result = self._process_candidates(candidates)\n            return result\n\n    def train_model(self, data: pd.DataFrame = None, update_internal=True) -&gt; Module:\n\"\"\"\n        Returns a ModelListGP containing independent models for the objectives and\n        constraints\n\n        \"\"\"\n        if data is None:\n            data = self.data\n        if data.empty:\n            raise ValueError(\"no data available to build model\")\n\n        _model = self.model_constructor.build_model(data, self._tkwargs)\n\n        # validate returned model\n        self._validate_model(_model)\n\n        if update_internal:\n            self._model = _model\n        return _model\n\n    def get_input_data(self, data):\n        return torch.tensor(\n            self.vocs.variable_data(data, \"\").to_numpy(), **self._tkwargs\n        )\n\n    def get_acquisition(self, model):\n\"\"\"\n        Returns a function that can be used to evaluate the acquisition function\n        \"\"\"\n\n        # need a sampler for botorch &gt; 0.8\n        # get input data\n        input_data = self.get_input_data(self.data)\n        self.sampler = get_sampler(\n            model.posterior(input_data),\n            sample_shape=torch.Size([self.options.acq.monte_carlo_samples]),\n        )\n\n        # get base acquisition function\n        acq = self._get_acquisition(model)\n\n        # add proximal biasing if requested\n        if self.options.acq.proximal_lengthscales is not None:\n            acq = ProximalAcquisitionFunction(\n                acq,\n                torch.tensor(self.options.acq.proximal_lengthscales, **self._tkwargs),\n                transformed_weighting=self.options.acq.use_transformed_proximal_weights,\n                beta=10.0,\n            )\n\n        return acq\n\n    def optimize_acqf(self, acq_funct, bounds, n_candidates):\n        # get candidates in real domain\n        candidates, out = optimize_acqf(\n            acq_function=acq_funct,\n            bounds=bounds,\n            q=n_candidates,\n            raw_samples=self.options.optim.raw_samples,\n            num_restarts=self.options.optim.num_restarts,\n        )\n        return candidates\n\n    def get_optimum(self):\n\"\"\"select the best point(s) (for multi-objective generators, given by the\n        model using the Posterior mean\"\"\"\n        c_posterior_mean = ConstrainedMCAcquisitionFunction(\n            self.model,\n            qUpperConfidenceBound(\n                model=self.model, beta=0.0, objective=self._get_objective()\n            ),\n            self._get_constraint_callables(),\n        )\n\n        result, out = optimize_acqf(\n            acq_function=c_posterior_mean,\n            bounds=self._get_bounds(),\n            q=1,\n            raw_samples=self.options.optim.raw_samples * 5,\n            num_restarts=self.options.optim.num_restarts * 5,\n        )\n\n        return self._process_candidates(result)\n\n    def _process_candidates(self, candidates):\n        logger.debug(\"Best candidate from optimize\", candidates)\n        return self.vocs.convert_numpy_to_inputs(candidates.detach().cpu().numpy())\n\n    @abstractmethod\n    def _get_acquisition(self, model):\n        pass\n\n    def _get_objective(self):\n\"\"\"return default objective (scalar objective) determined by vocs\"\"\"\n        return create_mc_objective(self.vocs, self._tkwargs)\n\n    def _get_constraint_callables(self):\n\"\"\"return default objective (scalar objective) determined by vocs\"\"\"\n        constraint_callables = create_constraint_callables(self.vocs)\n        if len(constraint_callables) == 0:\n            constraint_callables = None\n        return constraint_callables\n\n    @property\n    def model(self):\n        if self._model is None:\n            self.train_model(self.data)\n        return self._model\n\n    @property\n    def _tkwargs(self):\n        # set device and data type for generator\n        device = \"cpu\"\n        if self.options.use_cuda:\n            if torch.cuda.is_available():\n                device = \"cuda\"\n            else:\n                warnings.warn(\n                    \"Cuda requested in generator options but not found on \"\n                    \"machine! Using CPU instead\"\n                )\n\n        return {\"dtype\": torch.double, \"device\": device}\n\n    def _get_bounds(self):\n\"\"\"convert bounds from vocs to torch tensors\"\"\"\n        return torch.tensor(self.vocs.bounds, **self._tkwargs)\n\n    def _get_optimization_bounds(self):\n\"\"\"\n        gets optimization bounds based on the union of several domains\n        - if use_turbo is True include trust region\n        - if max_travel_distances is not None limit max travel distance\n\n        \"\"\"\n        bounds = self._get_bounds()\n\n        # if using turbo, update turbo state and set bounds according to turbo state\n        if self.options.optim.use_turbo:\n            bounds = self.get_trust_region(bounds)\n\n        # if specified modify bounds to limit maximum travel distances\n        if self.options.optim.max_travel_distances is not None:\n            bounds = self.get_max_travel_distances_region(bounds)\n\n        return bounds\n\n    def get_trust_region(self, bounds):\n\"\"\"get trust region based on turbo_state and last observation\"\"\"\n        if self.options.optim.use_turbo:\n            objective_data = self.vocs.objective_data(self.data, \"\")\n\n            # if this is the first time we are updating the state use the best f\n            # instead of the last f\n            if self.first_call:\n                y_last = torch.tensor(objective_data.min().to_numpy(), **self._tkwargs)\n                self.first_call = False\n            else:\n                y_last = torch.tensor(\n                    objective_data.iloc[-1].to_numpy(), **self._tkwargs\n                )\n            self.turbo_state = update_state(self.turbo_state, y_last)\n\n            # calculate trust region and apply to base bounds\n            trust_region = get_trust_region(\n                self.vocs,\n                self.model,\n                bounds,\n                self.data,\n                self.turbo_state,\n                self._tkwargs,\n            )\n\n            return rectilinear_domain_union(bounds, trust_region)\n        else:\n            raise RuntimeError(\n                \"cannot get trust region when `use_turbo` option is False\"\n            )\n\n    def get_max_travel_distances_region(self, bounds):\n\"\"\"get region based on max travel distances and last observation\"\"\"\n        if len(self.options.optim.max_travel_distances) != bounds.shape[-1]:\n            raise ValueError(\n                f\"length of max_travel_distances must match the number of \"\n                f\"variables {bounds.shape[-1]}\"\n            )\n\n        # get last point\n        if self.data.empty:\n            raise ValueError(\n                \"No data exists to specify max_travel_distances \"\n                \"from, add data first to use during BO\"\n            )\n        last_point = torch.tensor(\n            self.data[self.vocs.variable_names].iloc[-1].to_numpy(), **self._tkwargs\n        )\n\n        # bound lengths for normalization\n        lengths = bounds[1, :] - bounds[0, :]\n\n        # get maximum travel distances\n        max_travel_distances = (\n            torch.tensor(self.options.optim.max_travel_distances, **self._tkwargs)\n            * lengths\n        )\n        max_travel_bounds = torch.stack(\n            (last_point - max_travel_distances, last_point + max_travel_distances)\n        )\n\n        return rectilinear_domain_union(bounds, max_travel_bounds)\n\n    def _check_options(self, options: BayesianOptions):\n        if options.acq.proximal_lengthscales is not None:\n            n_lengthscales = len(options.acq.proximal_lengthscales)\n\n            if n_lengthscales != self.vocs.n_variables:\n                raise ValueError(\n                    f\"Number of proximal lengthscales ({n_lengthscales}) must match \"\n                    f\"number of variables {self.vocs.n_variables}\"\n                )\n            if options.optim.num_restarts != 1:\n                raise ValueError(\n                    \"`options.optim.num_restarts` must be 1 when proximal biasing is \"\n                    \"specified\"\n                )\n\n    def _validate_model(self, model):\n        if not isinstance(model, ModelListGP):\n            raise ValueError(\"model must be ModelListGP object\")\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_acquisition","title":"<code>get_acquisition(model)</code>","text":"<p>Returns a function that can be used to evaluate the acquisition function</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def get_acquisition(self, model):\n\"\"\"\n    Returns a function that can be used to evaluate the acquisition function\n    \"\"\"\n\n    # need a sampler for botorch &gt; 0.8\n    # get input data\n    input_data = self.get_input_data(self.data)\n    self.sampler = get_sampler(\n        model.posterior(input_data),\n        sample_shape=torch.Size([self.options.acq.monte_carlo_samples]),\n    )\n\n    # get base acquisition function\n    acq = self._get_acquisition(model)\n\n    # add proximal biasing if requested\n    if self.options.acq.proximal_lengthscales is not None:\n        acq = ProximalAcquisitionFunction(\n            acq,\n            torch.tensor(self.options.acq.proximal_lengthscales, **self._tkwargs),\n            transformed_weighting=self.options.acq.use_transformed_proximal_weights,\n            beta=10.0,\n        )\n\n    return acq\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_max_travel_distances_region","title":"<code>get_max_travel_distances_region(bounds)</code>","text":"<p>get region based on max travel distances and last observation</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def get_max_travel_distances_region(self, bounds):\n\"\"\"get region based on max travel distances and last observation\"\"\"\n    if len(self.options.optim.max_travel_distances) != bounds.shape[-1]:\n        raise ValueError(\n            f\"length of max_travel_distances must match the number of \"\n            f\"variables {bounds.shape[-1]}\"\n        )\n\n    # get last point\n    if self.data.empty:\n        raise ValueError(\n            \"No data exists to specify max_travel_distances \"\n            \"from, add data first to use during BO\"\n        )\n    last_point = torch.tensor(\n        self.data[self.vocs.variable_names].iloc[-1].to_numpy(), **self._tkwargs\n    )\n\n    # bound lengths for normalization\n    lengths = bounds[1, :] - bounds[0, :]\n\n    # get maximum travel distances\n    max_travel_distances = (\n        torch.tensor(self.options.optim.max_travel_distances, **self._tkwargs)\n        * lengths\n    )\n    max_travel_bounds = torch.stack(\n        (last_point - max_travel_distances, last_point + max_travel_distances)\n    )\n\n    return rectilinear_domain_union(bounds, max_travel_bounds)\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_optimum","title":"<code>get_optimum()</code>","text":"<p>select the best point(s) (for multi-objective generators, given by the model using the Posterior mean</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def get_optimum(self):\n\"\"\"select the best point(s) (for multi-objective generators, given by the\n    model using the Posterior mean\"\"\"\n    c_posterior_mean = ConstrainedMCAcquisitionFunction(\n        self.model,\n        qUpperConfidenceBound(\n            model=self.model, beta=0.0, objective=self._get_objective()\n        ),\n        self._get_constraint_callables(),\n    )\n\n    result, out = optimize_acqf(\n        acq_function=c_posterior_mean,\n        bounds=self._get_bounds(),\n        q=1,\n        raw_samples=self.options.optim.raw_samples * 5,\n        num_restarts=self.options.optim.num_restarts * 5,\n    )\n\n    return self._process_candidates(result)\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.get_trust_region","title":"<code>get_trust_region(bounds)</code>","text":"<p>get trust region based on turbo_state and last observation</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def get_trust_region(self, bounds):\n\"\"\"get trust region based on turbo_state and last observation\"\"\"\n    if self.options.optim.use_turbo:\n        objective_data = self.vocs.objective_data(self.data, \"\")\n\n        # if this is the first time we are updating the state use the best f\n        # instead of the last f\n        if self.first_call:\n            y_last = torch.tensor(objective_data.min().to_numpy(), **self._tkwargs)\n            self.first_call = False\n        else:\n            y_last = torch.tensor(\n                objective_data.iloc[-1].to_numpy(), **self._tkwargs\n            )\n        self.turbo_state = update_state(self.turbo_state, y_last)\n\n        # calculate trust region and apply to base bounds\n        trust_region = get_trust_region(\n            self.vocs,\n            self.model,\n            bounds,\n            self.data,\n            self.turbo_state,\n            self._tkwargs,\n        )\n\n        return rectilinear_domain_union(bounds, trust_region)\n    else:\n        raise RuntimeError(\n            \"cannot get trust region when `use_turbo` option is False\"\n        )\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_generator.BayesianGenerator.train_model","title":"<code>train_model(data=None, update_internal=True)</code>","text":"<p>Returns a ModelListGP containing independent models for the objectives and constraints</p> Source code in <code>xopt/generators/bayesian/bayesian_generator.py</code> <pre><code>def train_model(self, data: pd.DataFrame = None, update_internal=True) -&gt; Module:\n\"\"\"\n    Returns a ModelListGP containing independent models for the objectives and\n    constraints\n\n    \"\"\"\n    if data is None:\n        data = self.data\n    if data.empty:\n        raise ValueError(\"no data available to build model\")\n\n    _model = self.model_constructor.build_model(data, self._tkwargs)\n\n    # validate returned model\n    self._validate_model(_model)\n\n    if update_internal:\n        self._model = _model\n    return _model\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_exploration.BayesianExplorationGenerator","title":"<code>xopt.generators.bayesian.bayesian_exploration.BayesianExplorationGenerator</code>","text":"<p>         Bases: <code>BayesianGenerator</code></p> Source code in <code>xopt/generators/bayesian/bayesian_exploration.py</code> <pre><code>class BayesianExplorationGenerator(BayesianGenerator):\n    alias = \"bayesian_exploration\"\n    __doc__ = (\n\"\"\"Implements Bayeisan Exploration acquisition function\"\"\"\n        + f\"{format_option_descriptions(BayesianOptions())}\"\n    )\n\n    def __init__(self, vocs: VOCS, options: BayesianOptions = None):\n\"\"\"\n        Generator using UpperConfidenceBound acquisition function\n\n        Parameters\n        ----------\n        vocs: dict\n            Standard vocs for xopt\n\n        options: BayesianOptions\n            Options for the generator\n        \"\"\"\n        options = options or BayesianOptions()\n        if not isinstance(options, BayesianOptions):\n            raise ValueError(\"options must be a BayesianOptions object\")\n\n        if vocs.n_objectives != 1:\n            raise ValueError(\"vocs must have one objective for exploration\")\n        super().__init__(vocs, options)\n\n    @staticmethod\n    def default_options() -&gt; BayesianOptions:\n        return BayesianOptions()\n\n    def _get_acquisition(self, model):\n        qPV = qPosteriorVariance(\n            model,\n            sampler=self.sampler,\n            objective=self._get_objective(),\n        )\n\n        cqPV = ConstrainedMCAcquisitionFunction(\n            model,\n            qPV,\n            self._get_constraint_callables(),\n        )\n\n        return cqPV\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.bayesian_exploration.BayesianExplorationGenerator.__init__","title":"<code>__init__(vocs, options=None)</code>","text":"<p>Generator using UpperConfidenceBound acquisition function</p> <p>Parameters:</p> Name Type Description Default <code>vocs</code> <code>VOCS</code> <p>Standard vocs for xopt</p> required <p>options: BayesianOptions     Options for the generator</p> Source code in <code>xopt/generators/bayesian/bayesian_exploration.py</code> <pre><code>def __init__(self, vocs: VOCS, options: BayesianOptions = None):\n\"\"\"\n    Generator using UpperConfidenceBound acquisition function\n\n    Parameters\n    ----------\n    vocs: dict\n        Standard vocs for xopt\n\n    options: BayesianOptions\n        Options for the generator\n    \"\"\"\n    options = options or BayesianOptions()\n    if not isinstance(options, BayesianOptions):\n        raise ValueError(\"options must be a BayesianOptions object\")\n\n    if vocs.n_objectives != 1:\n        raise ValueError(\"vocs must have one objective for exploration\")\n    super().__init__(vocs, options)\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.mobo.MOBOGenerator","title":"<code>xopt.generators.bayesian.mobo.MOBOGenerator</code>","text":"<p>         Bases: <code>BayesianGenerator</code></p> Source code in <code>xopt/generators/bayesian/mobo.py</code> <pre><code>class MOBOGenerator(BayesianGenerator):\n    alias = \"mobo\"\n    __doc__ = (\n\"\"\"Implements Multi-Objective Bayesian Optimization using the Expected\n            Hypervolume Improvement acquisition function\"\"\"\n        + f\"{format_option_descriptions(MOBOOptions())}\"\n    )\n\n    def __init__(self, vocs: VOCS, options: MOBOOptions = None):\n        options = options or MOBOOptions()\n        if not isinstance(options, MOBOOptions):\n            raise ValueError(\"options must be a MOBOOptions object\")\n\n        super().__init__(vocs, options, supports_batch_generation=True)\n\n    @staticmethod\n    def default_options() -&gt; MOBOOptions:\n        return MOBOOptions()\n\n    @property\n    def reference_point(self):\n        if self.options.acq.reference_point is None:\n            raise XoptError(\n                \"referenece point must be specified for multi-objective \" \"algorithm\"\n            )\n\n        pt = []\n        for name in self.vocs.objective_names:\n            ref_val = self.options.acq.reference_point[name]\n            if self.vocs.objectives[name] == \"MINIMIZE\":\n                pt += [-ref_val]\n            elif self.vocs.objectives[name] == \"MAXIMIZE\":\n                pt += [ref_val]\n            else:\n                raise ValueError(\n                    f\"objective type {self.vocs.objectives[name]} not\\\n                        supported\"\n                )\n\n        return torch.tensor(pt, **self._tkwargs)\n\n    def _get_objective(self):\n        return create_mobo_objective(self.vocs, self._tkwargs)\n\n    def _get_acquisition(self, model):\n        inputs = self.get_input_data(self.data)\n\n        # fix problem with qNEHVI interpretation with constraints\n        acq = qNoisyExpectedHypervolumeImprovement(\n            model,\n            X_baseline=inputs,\n            constraints=self._get_constraint_callables(),\n            ref_point=self.reference_point,\n            sampler=self.sampler,\n            objective=self._get_objective(),\n            cache_root=False,\n            prune_baseline=True,\n        )\n\n        return acq\n\n    def calculate_hypervolume(self):\n\"\"\"compute hypervolume given data\"\"\"\n        objective_data = torch.tensor(\n            self.vocs.objective_data(self.data, return_raw=True).to_numpy()\n        )\n\n        # hypervolume must only take into account feasible data points\n        if self.vocs.n_constraints &gt; 0:\n            objective_data = objective_data[\n                self.vocs.feasibility_data(self.data)[\"feasible\"].to_list()\n            ]\n\n        n_objectives = self.vocs.n_objectives\n        weights = torch.zeros(n_objectives)\n        weights = set_botorch_weights(weights, self.vocs)\n        objective_data = objective_data * weights\n\n        # compute hypervolume\n        bd = DominatedPartitioning(ref_point=self.reference_point, Y=objective_data)\n        volume = bd.compute_hypervolume().item()\n\n        return volume\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.mobo.MOBOGenerator.calculate_hypervolume","title":"<code>calculate_hypervolume()</code>","text":"<p>compute hypervolume given data</p> Source code in <code>xopt/generators/bayesian/mobo.py</code> <pre><code>def calculate_hypervolume(self):\n\"\"\"compute hypervolume given data\"\"\"\n    objective_data = torch.tensor(\n        self.vocs.objective_data(self.data, return_raw=True).to_numpy()\n    )\n\n    # hypervolume must only take into account feasible data points\n    if self.vocs.n_constraints &gt; 0:\n        objective_data = objective_data[\n            self.vocs.feasibility_data(self.data)[\"feasible\"].to_list()\n        ]\n\n    n_objectives = self.vocs.n_objectives\n    weights = torch.zeros(n_objectives)\n    weights = set_botorch_weights(weights, self.vocs)\n    objective_data = objective_data * weights\n\n    # compute hypervolume\n    bd = DominatedPartitioning(ref_point=self.reference_point, Y=objective_data)\n    volume = bd.compute_hypervolume().item()\n\n    return volume\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.upper_confidence_bound.UpperConfidenceBoundGenerator","title":"<code>xopt.generators.bayesian.upper_confidence_bound.UpperConfidenceBoundGenerator</code>","text":"<p>         Bases: <code>BayesianGenerator</code></p> Source code in <code>xopt/generators/bayesian/upper_confidence_bound.py</code> <pre><code>class UpperConfidenceBoundGenerator(BayesianGenerator):\n    alias = \"upper_confidence_bound\"\n    __doc__ = (\n\"\"\"Implements Bayeisan Optimization using the Upper Confidence Bound\n    acquisition function\"\"\"\n        + f\"{format_option_descriptions(UCBOptions())}\"\n    )\n\n    def __init__(self, vocs: VOCS, options: UCBOptions = None):\n\"\"\"\n        Generator using UpperConfidenceBound acquisition function\n\n        Parameters\n        ----------\n        vocs: dict\n            Standard vocs dictionary for xopt\n\n        options: UpperConfidenceBoundOptions\n            Specific options for this generator\n        \"\"\"\n        options = options or UCBOptions()\n        if not isinstance(options, UCBOptions):\n            raise ValueError(\"options must be a UCBOptions object\")\n\n        if vocs.n_objectives != 1:\n            raise ValueError(\"vocs must have one objective for optimization\")\n\n        super().__init__(vocs, options)\n\n    @staticmethod\n    def default_options() -&gt; BayesianOptions:\n        return UCBOptions()\n\n    def _get_acquisition(self, model):\n        qUCB = qUpperConfidenceBound(\n            model,\n            sampler=self.sampler,\n            objective=self._get_objective(),\n            beta=self.options.acq.beta,\n        )\n\n        cqUCB = ConstrainedMCAcquisitionFunction(\n            model,\n            qUCB,\n            self._get_constraint_callables(),\n        )\n\n        return cqUCB\n</code></pre>"},{"location":"api/generators/bayesian/#xopt.generators.bayesian.upper_confidence_bound.UpperConfidenceBoundGenerator.__init__","title":"<code>__init__(vocs, options=None)</code>","text":"<p>Generator using UpperConfidenceBound acquisition function</p> <p>Parameters:</p> Name Type Description Default <code>vocs</code> <code>VOCS</code> <p>Standard vocs dictionary for xopt</p> required <p>options: UpperConfidenceBoundOptions     Specific options for this generator</p> Source code in <code>xopt/generators/bayesian/upper_confidence_bound.py</code> <pre><code>def __init__(self, vocs: VOCS, options: UCBOptions = None):\n\"\"\"\n    Generator using UpperConfidenceBound acquisition function\n\n    Parameters\n    ----------\n    vocs: dict\n        Standard vocs dictionary for xopt\n\n    options: UpperConfidenceBoundOptions\n        Specific options for this generator\n    \"\"\"\n    options = options or UCBOptions()\n    if not isinstance(options, UCBOptions):\n        raise ValueError(\"options must be a UCBOptions object\")\n\n    if vocs.n_objectives != 1:\n        raise ValueError(\"vocs must have one objective for optimization\")\n\n    super().__init__(vocs, options)\n</code></pre>"},{"location":"api/generators/genetic/","title":"Genetic generators","text":""},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator","title":"<code>xopt.generators.ga.cnsga.CNSGAGenerator</code>","text":"<p>         Bases: <code>Generator</code></p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>class CNSGAGenerator(Generator):\n    alias = \"cnsga\"\n\n    @staticmethod\n    def default_options() -&gt; CNSGAOptions:\n        return CNSGAOptions()\n\n    def __init__(self, vocs, options: CNSGAOptions = None):\n        options = options or CNSGAOptions()\n        if not isinstance(options, CNSGAOptions):\n            raise TypeError(\"options must be of type CNSGAOptions\")\n        super().__init__(vocs, options)\n\n        # Internal data structures\n        self.children = (\n            []\n        )  # list of unevaluated inputs. This should be a list of dicts.\n        self.population = None  # The latest population data (fully evaluated)\n        self.offspring = None  # Newly evaluated data, but not yet added to population\n\n        self._loaded_population = (\n            None  # use these to generate children until the first pop is made\n        )\n\n        # DEAP toolbox (internal)\n        self.toolbox = cnsga_toolbox(vocs, selection=\"auto\")\n\n        if options.population_file is not None:\n            self.load_population_csv(options.population_file)\n\n        if options.output_path is not None:\n            assert os.path.isdir(options.output_path), \"Output directory does not exist\"\n\n        # if data is not None:\n        #    self.population = cnsga_select(data, n_pop, vocs, self.toolbox)\n\n    def old__init__(\n        self,\n        vocs,\n        *,\n        n_pop,\n        data=None,\n        crossover_probability=0.9,\n        mutation_probability=1.0,\n    ):\n        self._vocs = vocs  # TODO: use proper options\n        self.n_pop = n_pop\n        self.crossover_probability = crossover_probability\n        self.mutation_probability = mutation_probability\n\n        # Internal data structures\n        self.children = []  # unevaluated inputs. This should be a list of dicts.\n        self.population = None  # The latest population (fully evaluated)\n        self.offspring = None  # Newly evaluated data, but not yet added to population\n\n        # DEAP toolbox (internal)\n        self.toolbox = cnsga_toolbox(vocs, selection=\"auto\")\n\n        if data is not None:\n            self.population = cnsga_select(data, n_pop, vocs, self.toolbox)\n\n    def create_children(self):\n        # No population, so create random children\n        if self.population is None:\n            # Special case when pop is loaded from file\n            if self._loaded_population is None:\n                return [self.vocs.random_inputs() for _ in range(self.n_pop)]\n            else:\n                pop = self._loaded_population\n        else:\n            pop = self.population\n\n        # Use population to create children\n        inputs = cnsga_variation(\n            pop,\n            self.vocs,\n            self.toolbox,\n            crossover_probability=self.options.crossover_probability,\n            mutation_probability=self.options.mutation_probability,\n        )\n        return inputs.to_dict(orient=\"records\")\n\n    def add_data(self, new_data: pd.DataFrame):\n        self.offspring = pd.concat([self.offspring, new_data])\n\n        # Next generation\n        if len(self.offspring) &gt;= self.n_pop:\n            candidates = pd.concat([self.population, self.offspring])\n            self.population = cnsga_select(\n                candidates, self.n_pop, self.vocs, self.toolbox\n            )\n\n            if self.options.output_path is not None:\n                self.write_offspring()\n                self.write_population()\n\n            self.children = []  # reset children\n            self.offspring = None  # reset offspring\n\n    def generate(self, n_candidates) -&gt; List[Dict]:\n\"\"\"\n        generate `n_candidates` candidates\n\n        \"\"\"\n\n        # Make sure we have enough children to fulfill the request\n        while len(self.children) &lt; n_candidates:\n            self.children.extend(self.create_children())\n\n        return [self.children.pop() for _ in range(n_candidates)]\n\n    def write_offspring(self, filename=None):\n\"\"\"\n        Write the current offspring to a CSV file.\n\n        Similar to write_population\n        \"\"\"\n        if self.offspring is None:\n            logger.warning(\"No offspring to write\")\n            return\n\n        if filename is None:\n            filename = f\"{self.alias}_offspring_{xopt.utils.isotime(include_microseconds=True)}.csv\"\n            filename = os.path.join(self.options.output_path, filename)\n\n        self.offspring.to_csv(filename, index_label=\"xopt_index\")\n\n    def write_population(self, filename=None):\n\"\"\"\n        Write the current population to a CSV file.\n\n        Similar to write_offspring\n        \"\"\"\n        if self.population is None:\n            logger.warning(\"No population to write\")\n            return\n\n        if filename is None:\n            filename = f\"{self.alias}_population_{xopt.utils.isotime(include_microseconds=True)}.csv\"\n            filename = os.path.join(self.options.output_path, filename)\n\n        self.population.to_csv(filename, index_label=\"xopt_index\")\n\n    def load_population_csv(self, filename):\n\"\"\"\n        Read a population from a CSV file.\n        These will be reverted back to children for re-evaluation.\n        \"\"\"\n        pop = pd.read_csv(filename, index_col=\"xopt_index\")\n        self._loaded_population = pop\n        # This is a list of dicts\n        self.children = self.vocs.convert_dataframe_to_inputs(pop).to_dict(\n            orient=\"records\"\n        )\n        logger.info(f\"Loaded population of len {len(pop)} from file: {filename}\")\n\n    @property\n    def n_pop(self):\n\"\"\"\n        Convenience alias for `options.population_size`\n        \"\"\"\n        return self.options.population_size\n</code></pre>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.n_pop","title":"<code>n_pop</code>  <code>property</code>","text":"<p>Convenience alias for <code>options.population_size</code></p>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.generate","title":"<code>generate(n_candidates)</code>","text":"<p>generate <code>n_candidates</code> candidates</p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>def generate(self, n_candidates) -&gt; List[Dict]:\n\"\"\"\n    generate `n_candidates` candidates\n\n    \"\"\"\n\n    # Make sure we have enough children to fulfill the request\n    while len(self.children) &lt; n_candidates:\n        self.children.extend(self.create_children())\n\n    return [self.children.pop() for _ in range(n_candidates)]\n</code></pre>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.load_population_csv","title":"<code>load_population_csv(filename)</code>","text":"<p>Read a population from a CSV file. These will be reverted back to children for re-evaluation.</p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>def load_population_csv(self, filename):\n\"\"\"\n    Read a population from a CSV file.\n    These will be reverted back to children for re-evaluation.\n    \"\"\"\n    pop = pd.read_csv(filename, index_col=\"xopt_index\")\n    self._loaded_population = pop\n    # This is a list of dicts\n    self.children = self.vocs.convert_dataframe_to_inputs(pop).to_dict(\n        orient=\"records\"\n    )\n    logger.info(f\"Loaded population of len {len(pop)} from file: {filename}\")\n</code></pre>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.write_offspring","title":"<code>write_offspring(filename=None)</code>","text":"<p>Write the current offspring to a CSV file.</p> <p>Similar to write_population</p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>def write_offspring(self, filename=None):\n\"\"\"\n    Write the current offspring to a CSV file.\n\n    Similar to write_population\n    \"\"\"\n    if self.offspring is None:\n        logger.warning(\"No offspring to write\")\n        return\n\n    if filename is None:\n        filename = f\"{self.alias}_offspring_{xopt.utils.isotime(include_microseconds=True)}.csv\"\n        filename = os.path.join(self.options.output_path, filename)\n\n    self.offspring.to_csv(filename, index_label=\"xopt_index\")\n</code></pre>"},{"location":"api/generators/genetic/#xopt.generators.ga.cnsga.CNSGAGenerator.write_population","title":"<code>write_population(filename=None)</code>","text":"<p>Write the current population to a CSV file.</p> <p>Similar to write_offspring</p> Source code in <code>xopt/generators/ga/cnsga.py</code> <pre><code>def write_population(self, filename=None):\n\"\"\"\n    Write the current population to a CSV file.\n\n    Similar to write_offspring\n    \"\"\"\n    if self.population is None:\n        logger.warning(\"No population to write\")\n        return\n\n    if filename is None:\n        filename = f\"{self.alias}_population_{xopt.utils.isotime(include_microseconds=True)}.csv\"\n        filename = os.path.join(self.options.output_path, filename)\n\n    self.population.to_csv(filename, index_label=\"xopt_index\")\n</code></pre>"},{"location":"api/generators/scipy/","title":"SciPy generators","text":""},{"location":"api/generators/scipy/#xopt.generators.scipy.neldermead.NelderMeadGenerator","title":"<code>xopt.generators.scipy.neldermead.NelderMeadGenerator</code>","text":"<p>         Bases: <code>ScipyOptimizeGenerator</code></p> <p>Nelder-Mead algorithm from SciPy in Xopt's Generator form.</p> Source code in <code>xopt/generators/scipy/neldermead.py</code> <pre><code>class NelderMeadGenerator(ScipyOptimizeGenerator):\n\"\"\"\n    Nelder-Mead algorithm from SciPy in Xopt's Generator form.\n    \"\"\"\n\n    alias = \"neldermead\"\n\n    @staticmethod\n    def default_options() -&gt; NelderMeadOptions:\n        return NelderMeadOptions()\n\n    def __init__(self, vocs, options: NelderMeadOptions = None):\n        options = options or NelderMeadOptions()\n        if not isinstance(options, NelderMeadOptions):\n            raise TypeError(\"options must be of type NedlerMeadOptions\")\n        super().__init__(vocs, options)\n\n    def _init_algorithm(self):\n\"\"\"\n        sets self._algorithm to the generator function (initializing it).\n        \"\"\"\n\n        options = self.options  # convenience\n\n        if options.initial_simplex:\n            sim = np.array(\n                [self.options.initial_simplex[k] for k in self.vocs.variable_names]\n            ).T\n        else:\n            sim = None\n\n        self._algorithm = _neldermead_generator(  # adapted from scipy.optimize\n            self.func,  # Handled by base class\n            self.x0,  # Handled by base class\n            adaptive=self.options.adaptive,\n            xatol=self.options.xatol,\n            fatol=self.options.fatol,\n            initial_simplex=sim,\n            bounds=self.vocs.bounds,\n        )\n\n    @property\n    def simplex(self):\n\"\"\"\n        Returns the simplex in the current state.\n        \"\"\"\n        sim = self.state\n        return dict(zip(self.vocs.variable_names, sim.T))\n</code></pre>"},{"location":"api/generators/scipy/#xopt.generators.scipy.neldermead.NelderMeadGenerator.simplex","title":"<code>simplex</code>  <code>property</code>","text":"<p>Returns the simplex in the current state.</p>"},{"location":"examples/basic/checkpointing_and_restarts/","title":"Checkpointing and Restarts","text":"In\u00a0[1]: Copied! <pre># Import the class\nfrom xopt import Xopt\n\n# Make a proper input file.\nYAML = \"\"\"\nxopt:\n    dump_file: dump.yaml\n\ngenerator:\n    name: random\n\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n    function_kwargs:\n        a: 999\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    linked_variables: {x9: x1}\n    constants: {a: dummy_constant}\n\n\"\"\"\n</pre> # Import the class from xopt import Xopt  # Make a proper input file. YAML = \"\"\" xopt:     dump_file: dump.yaml  generator:     name: random  evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK     function_kwargs:         a: 999  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     linked_variables: {x9: x1}     constants: {a: dummy_constant}  \"\"\" In\u00a0[2]: Copied! <pre>import yaml\n\n# create Xopt object.\nX = Xopt(YAML)\n\n# take 10 steps and view data\nfor _ in range(10):\n    X.step()\n\nX.data\n</pre> import yaml  # create Xopt object. X = Xopt(YAML)  # take 10 steps and view data for _ in range(10):     X.step()  X.data Out[2]: x1 x2 a x9 y1 y2 c1 c2 some_array xopt_runtime xopt_error 1 3.067480 2.290848 dummy_constant 3.067480 3.067480 2.290848 13.724266 9.799088 [1, 2, 3] 0.000058 False 2 2.754901 0.293855 dummy_constant 2.754901 2.754901 0.293855 6.688739 5.127075 [1, 2, 3] 0.000039 False 3 1.044519 0.761659 dummy_constant 1.044519 1.044519 0.761659 0.750384 0.364966 [1, 2, 3] 0.000039 False 4 1.893179 0.440562 dummy_constant 1.893179 1.893179 0.440562 2.865169 1.944481 [1, 2, 3] 0.000050 False 5 2.602787 2.670267 dummy_constant 2.602787 2.602787 2.670267 12.806914 9.131772 [1, 2, 3] 0.000042 False 6 1.799957 0.876853 dummy_constant 1.799957 1.799957 0.876853 2.952177 1.831907 [1, 2, 3] 0.000047 False 7 1.811522 0.982474 dummy_constant 1.811522 1.811522 0.982474 3.256574 1.952872 [1, 2, 3] 0.000038 False 8 1.925994 2.750891 dummy_constant 1.925994 1.925994 2.750891 10.370852 7.099970 [1, 2, 3] 0.000039 False 9 2.871807 1.691219 dummy_constant 2.871807 2.871807 1.691219 10.168933 7.044473 [1, 2, 3] 0.000040 False 10 3.108475 3.097799 dummy_constant 3.108475 3.108475 3.097799 18.159009 13.552698 [1, 2, 3] 0.000043 False In\u00a0[3]: Copied! <pre>config = yaml.safe_load(open(\"dump.yaml\"))\nX2 = Xopt(config)\nprint(X2.options)\nprint(X2.generator)\nprint(X2.evaluator)\n\nX2.data\n</pre> config = yaml.safe_load(open(\"dump.yaml\")) X2 = Xopt(config) print(X2.options) print(X2.generator) print(X2.evaluator)  X2.data <pre>asynch=False strict=False dump_file='dump.yaml' max_evaluations=None\n&lt;xopt.generators.random.RandomGenerator object at 0x7f93f1887040&gt;\nfunction=&lt;function evaluate_TNK at 0x7f93f191b5e0&gt; max_workers=1 executor=NormalExecutor[DummyExecutor](loader=ObjLoader[DummyExecutor](object=None, loader=CallableModel(callable=&lt;class 'xopt.evaluator.DummyExecutor'&gt;, signature=Kwargs_DummyExecutor(args=[], kwarg_order=[])), object_type=&lt;class 'xopt.evaluator.DummyExecutor'&gt;), executor_type=&lt;class 'xopt.evaluator.DummyExecutor'&gt;, submit_callable='submit', map_callable='map', shutdown_callable='shutdown', executor=&lt;xopt.evaluator.DummyExecutor object at 0x7f93f1887880&gt;) function_kwargs={'sleep': 0, 'random_sleep': 0, 'raise_probability': 0, 'a': 999} vectorized=False\n</pre> Out[3]: a c1 c2 some_array x1 x2 x9 xopt_error xopt_runtime y1 y2 1 dummy_constant 13.724266 9.799088 [1, 2, 3] 3.067480 2.290848 3.067480 False 0.000058 3.067480 2.290848 10 dummy_constant 18.159009 13.552698 [1, 2, 3] 3.108475 3.097799 3.108475 False 0.000043 3.108475 3.097799 2 dummy_constant 6.688739 5.127075 [1, 2, 3] 2.754901 0.293855 2.754901 False 0.000039 2.754901 0.293855 3 dummy_constant 0.750384 0.364966 [1, 2, 3] 1.044519 0.761659 1.044519 False 0.000039 1.044519 0.761659 4 dummy_constant 2.865169 1.944481 [1, 2, 3] 1.893179 0.440562 1.893179 False 0.000050 1.893179 0.440562 5 dummy_constant 12.806914 9.131772 [1, 2, 3] 2.602787 2.670267 2.602787 False 0.000042 2.602787 2.670267 6 dummy_constant 2.952177 1.831907 [1, 2, 3] 1.799957 0.876853 1.799957 False 0.000047 1.799957 0.876853 7 dummy_constant 3.256574 1.952872 [1, 2, 3] 1.811522 0.982474 1.811522 False 0.000038 1.811522 0.982474 8 dummy_constant 10.370852 7.099970 [1, 2, 3] 1.925994 2.750891 1.925994 False 0.000039 1.925994 2.750891 9 dummy_constant 10.168933 7.044473 [1, 2, 3] 2.871807 1.691219 2.871807 False 0.000040 2.871807 1.691219 In\u00a0[4]: Copied! <pre>for _ in range(10):\n    X2.step()\n\nX2.data\n</pre> for _ in range(10):     X2.step()  X2.data Out[4]: a c1 c2 some_array x1 x2 x9 xopt_error xopt_runtime y1 y2 1 dummy_constant 13.724266 9.799088 [1, 2, 3] 3.067480 2.290848 3.067480 False 0.000058 3.067480 2.290848 10 dummy_constant 18.159009 13.552698 [1, 2, 3] 3.108475 3.097799 3.108475 False 0.000043 3.108475 3.097799 2 dummy_constant 6.688739 5.127075 [1, 2, 3] 2.754901 0.293855 2.754901 False 0.000039 2.754901 0.293855 3 dummy_constant 0.750384 0.364966 [1, 2, 3] 1.044519 0.761659 1.044519 False 0.000039 1.044519 0.761659 4 dummy_constant 2.865169 1.944481 [1, 2, 3] 1.893179 0.440562 1.893179 False 0.000050 1.893179 0.440562 5 dummy_constant 12.806914 9.131772 [1, 2, 3] 2.602787 2.670267 2.602787 False 0.000042 2.602787 2.670267 6 dummy_constant 2.952177 1.831907 [1, 2, 3] 1.799957 0.876853 1.799957 False 0.000047 1.799957 0.876853 7 dummy_constant 3.256574 1.952872 [1, 2, 3] 1.811522 0.982474 1.811522 False 0.000038 1.811522 0.982474 8 dummy_constant 10.370852 7.099970 [1, 2, 3] 1.925994 2.750891 1.925994 False 0.000039 1.925994 2.750891 9 dummy_constant 10.168933 7.044473 [1, 2, 3] 2.871807 1.691219 2.871807 False 0.000040 2.871807 1.691219 11 dummy_constant 1.321077 0.885239 [1, 2, 3] 1.435523 0.600175 1.435523 False 0.000066 1.435523 0.600175 12 dummy_constant 8.641243 5.859802 [1, 2, 3] 2.681351 1.549528 2.681351 False 0.000043 2.681351 1.549528 13 dummy_constant 0.994795 0.553530 [1, 2, 3] 0.686868 1.220146 0.686868 False 0.000045 0.686868 1.220146 14 dummy_constant 6.252062 4.007657 [1, 2, 3] 1.984122 1.843517 1.984122 False 0.000044 1.984122 1.843517 15 dummy_constant 6.295595 4.098737 [1, 2, 3] 1.314265 2.353567 1.314265 False 0.000046 1.314265 2.353567 16 dummy_constant 5.548062 3.697740 [1, 2, 3] 2.328605 1.094932 2.328605 False 0.000047 2.328605 1.094932 17 dummy_constant 10.004572 6.871771 [1, 2, 3] 2.474330 2.224468 2.474330 False 0.000045 2.474330 2.224468 18 dummy_constant 8.616464 6.077125 [1, 2, 3] 1.289448 2.835358 1.289448 False 0.000044 1.289448 2.835358 19 dummy_constant 6.294643 4.048930 [1, 2, 3] 1.919091 1.926573 1.919091 False 0.000045 1.919091 1.926573 20 dummy_constant 7.898821 5.624504 [1, 2, 3] 2.805790 1.054829 2.805790 False 0.000049 2.805790 1.054829 In\u00a0[4]: Copied! <pre>\n</pre>"},{"location":"examples/basic/checkpointing_and_restarts/#checkpointing-and-restarts","title":"Checkpointing and Restarts\u00b6","text":"<p>If <code>dump_file</code> is provided Xopt will save the data and the Xopt configuration in a yaml file. This can be used directly to create a new Xopt object.</p>"},{"location":"examples/basic/checkpointing_and_restarts/#checkpoints","title":"Checkpoints\u00b6","text":"<p>Since we specified a dump file Xopt will dump the data and all of the options required to create a new Xopt object that continues the run.</p>"},{"location":"examples/basic/checkpointing_and_restarts/#create-xopt-object-from-dump-file","title":"Create Xopt object from dump file\u00b6","text":""},{"location":"examples/basic/xopt_basic/","title":"Xopt basic example","text":"In\u00a0[1]: Copied! <pre># Import the class\nfrom xopt import Xopt\n</pre> # Import the class from xopt import Xopt In\u00a0[2]: Copied! <pre># Nicer plotting\n%config InlineBackend.figure_format = 'retina'\n</pre> # Nicer plotting %config InlineBackend.figure_format = 'retina' In\u00a0[3]: Copied! <pre># Make a proper input file.\nYAML = \"\"\"\nxopt: {}\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n    function_kwargs:\n        a: 999\n\ngenerator:\n    name: random\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    linked_variables: {x9: x1}\n    constants: {a: dummy_constant}\n\n\"\"\"\n</pre> # Make a proper input file. YAML = \"\"\" xopt: {} evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK     function_kwargs:         a: 999  generator:     name: random  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     linked_variables: {x9: x1}     constants: {a: dummy_constant}  \"\"\" In\u00a0[4]: Copied! <pre># create Xopt object.\nX = Xopt(YAML)\n</pre> # create Xopt object. X = Xopt(YAML) In\u00a0[5]: Copied! <pre># Convenient representation of the state.\nX\n</pre> # Convenient representation of the state. X Out[5]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g0676a82\nData size: 0\nConfig as YAML:\nxopt: {asynch: false, strict: false, dump_file: null, max_evaluations: null}\ngenerator: {name: random}\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  max_workers: 1\n  function_kwargs: {sleep: 0, random_sleep: 0, raise_probability: 0, a: 999}\n  vectorized: false\nvocs:\n  variables:\n    x1: [0.0, 3.14159]\n    x2: [0.0, 3.14159]\n  constraints:\n    c1: [GREATER_THAN, 0.0]\n    c2: [LESS_THAN, 0.5]\n  objectives: {y1: MINIMIZE, y2: MINIMIZE}\n  constants: {a: dummy_constant}\n  linked_variables: {x9: x1}\n</pre> In\u00a0[6]: Copied! <pre>X.random_evaluate(10)\n</pre> X.random_evaluate(10) Out[6]: <pre>{'y1': array([2.81863406, 1.80933485, 1.11249386, 0.78804525, 2.69370684,\n        0.81625261, 2.70308876, 1.17711646, 2.96119474, 0.71287878]),\n 'y2': array([2.00019006, 3.03696234, 2.21231784, 2.47605414, 0.3531604 ,\n        1.11801641, 0.19463116, 2.01139436, 1.36517456, 0.34172747]),\n 'c1': array([11.03551207, 11.56447459,  5.09314228,  5.73026226,  6.4300322 ,\n         0.99489567,  6.30372751,  4.48923173,  9.551486  , -0.43959712]),\n 'c2': array([7.62663412, 8.15053565, 3.30718113, 3.98776003, 4.83391157,\n        0.48196   , 4.94685021, 2.74279963, 6.80600658, 0.07036757]),\n 'some_array': array([1, 2, 3]),\n 'xopt_runtime': 7.770000001983135e-05,\n 'xopt_error': False}</pre> In\u00a0[7]: Copied! <pre>import numpy as np\nnp.random.seed(10)\n</pre> import numpy as np np.random.seed(10) In\u00a0[8]: Copied! <pre># Take one step (generate a single point)\nX.step()\n</pre> # Take one step (generate a single point) X.step() In\u00a0[9]: Copied! <pre># examine the results\nX.data\n</pre> # examine the results X.data Out[9]: x1 x2 a x9 y1 y2 c1 c2 some_array xopt_runtime xopt_error 1 0.718417 3.076396 dummy_constant 0.718417 0.718417 3.076396 9.066664 6.685522 [1, 2, 3] 0.000044 False In\u00a0[10]: Copied! <pre># take a couple of steps and examine the results\nfor _ in range(10):\n    X.step()\nX.data\n</pre> # take a couple of steps and examine the results for _ in range(10):     X.step() X.data Out[10]: x1 x2 a x9 y1 y2 c1 c2 some_array xopt_runtime xopt_error 1 0.718417 3.076396 dummy_constant 0.718417 0.718417 3.076396 9.066664 6.685522 [1, 2, 3] 0.000044 False 2 1.150927 0.789155 dummy_constant 1.150927 1.150927 0.789155 1.045563 0.507317 [1, 2, 3] 0.000323 False 3 1.575485 2.435371 dummy_constant 1.575485 1.575485 2.435371 7.510380 4.902330 [1, 2, 3] 0.000045 False 4 2.519358 0.752314 dummy_constant 2.519358 2.519358 0.752314 5.920078 4.141468 [1, 2, 3] 0.000044 False 5 2.610313 2.864063 dummy_constant 2.610313 2.610313 2.864063 13.942816 10.042213 [1, 2, 3] 0.000036 False 6 0.988470 0.146419 dummy_constant 0.988470 0.988470 0.146419 0.068991 0.363623 [1, 2, 3] 0.000033 False 7 3.129186 1.532492 dummy_constant 3.129186 3.129186 1.532492 11.086588 7.978659 [1, 2, 3] 0.000032 False 8 0.588668 1.217284 dummy_constant 0.588668 0.588668 1.217284 0.768029 0.522359 [1, 2, 3] 0.000032 False 9 0.874131 2.224635 dummy_constant 0.874131 0.874131 2.224635 4.617362 3.114340 [1, 2, 3] 0.000034 False 10 0.258320 0.896686 dummy_constant 0.258320 0.258320 0.896686 -0.106956 0.215769 [1, 2, 3] 0.000034 False 11 1.437138 2.694950 dummy_constant 1.437138 1.437138 2.694950 8.326577 5.696033 [1, 2, 3] 0.000031 False In\u00a0[11]: Copied! <pre>import matplotlib.pyplot as plt\n\nX.data.plot(*X.vocs.objective_names, kind=\"scatter\")\n</pre> import matplotlib.pyplot as plt  X.data.plot(*X.vocs.objective_names, kind=\"scatter\") Out[11]: <pre>&lt;Axes: xlabel='y1', ylabel='y2'&gt;</pre> In\u00a0[12]: Copied! <pre>import yaml\nimport json\n\nconfig  = yaml.safe_load(YAML) \n# All these methods work\n\nX = Xopt(YAML)\nX = Xopt(config)\nX = Xopt(json.dumps(config, indent=4))\n</pre> import yaml import json  config  = yaml.safe_load(YAML)  # All these methods work  X = Xopt(YAML) X = Xopt(config) X = Xopt(json.dumps(config, indent=4))  In\u00a0[13]: Copied! <pre># Call the evaluator's function directly.\nX.evaluate({'x1': .5, 'x2': 0.5})\n</pre> # Call the evaluator's function directly. X.evaluate({'x1': .5, 'x2': 0.5}) Out[13]: <pre>{'y1': 0.5,\n 'y2': 0.5,\n 'c1': -0.6,\n 'c2': 0.0,\n 'some_array': array([1, 2, 3]),\n 'xopt_runtime': 5.7600999980422785e-05,\n 'xopt_error': False}</pre> In\u00a0[14]: Copied! <pre># Create random inputs according to the vocs\nX.random_inputs()\n</pre> # Create random inputs according to the vocs X.random_inputs() Out[14]: <pre>{'x1': 1.9687064016299047,\n 'x2': 1.0237385762437203,\n 'a': 'dummy_constant',\n 'x9': 1.9687064016299047}</pre> In\u00a0[15]: Copied! <pre># These can be combined:\nnp.random.seed(10)\nX.evaluate(X.random_inputs())\n</pre> # These can be combined: np.random.seed(10) X.evaluate(X.random_inputs()) Out[15]: <pre>{'y1': 0.7184167803196234,\n 'y2': 3.0763958834119975,\n 'c1': 9.066663826202975,\n 'c2': 6.6855216379874784,\n 'some_array': array([1, 2, 3]),\n 'xopt_runtime': 6.47020000315024e-05,\n 'xopt_error': False}</pre> In\u00a0[16]: Copied! <pre># And this is a convenience method for above\nnp.random.seed(10)\nX.random_evaluate()\n</pre> # And this is a convenience method for above np.random.seed(10) X.random_evaluate() Out[16]: <pre>{'y1': 0.7184167803196234,\n 'y2': 3.0763958834119975,\n 'c1': 9.066663826202975,\n 'c2': 6.6855216379874784,\n 'some_array': array([1, 2, 3]),\n 'xopt_runtime': 5.1501000029929855e-05,\n 'xopt_error': False}</pre> In\u00a0[17]: Copied! <pre># Notebook printing output\nfrom xopt import output_notebook\noutput_notebook()\n</pre> # Notebook printing output from xopt import output_notebook output_notebook() In\u00a0[18]: Copied! <pre>X.step()\n</pre> X.step() <pre>Running Xopt step\n</pre>"},{"location":"examples/basic/xopt_basic/#xopt-basic-example","title":"Xopt basic example\u00b6","text":"<p>An Xopt problem can be described by a simple YAML file. Here we will demonstrate how this is used to optimize a well-known constrained multi-objective test function, TNK. The TNK function is defined with:</p> <p>$n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/basic/xopt_basic/#xopt-components","title":"Xopt Components\u00b6","text":"<p>The definition of the Xopt object requires 4 parts, listed below:</p> <ul> <li>The <code>Xopt</code> object itself, which controlls data flow, stores data and governs data</li> </ul> <p>loading and saving.</p> <ul> <li>The <code>Evaluator</code> object, which evaluates input points using the arbitrary function</li> </ul> <p>specified by the <code>function</code> property.</p> <ul> <li>The <code>Generator</code> object, which, when given data that has been evaluated, generates</li> </ul> <p>future points to evaluate using the evaluator.</p> <ul> <li>The <code>VOCS</code> (variables, objectives, constraints, statics) object, which specifies the</li> </ul> <p>input domain, the objectives, constraints and constants passed to the evaluator function.</p> <p>Through the YAML interface these objects and their options are specified by name.  Here we will make one:</p>"},{"location":"examples/basic/xopt_basic/#run-random-data-generation","title":"Run Random data generation\u00b6","text":""},{"location":"examples/basic/xopt_basic/#plotting","title":"Plotting\u00b6","text":"<p>Plot the objective results</p>"},{"location":"examples/basic/xopt_basic/#alternative-initialization-methods","title":"Alternative initialization methods\u00b6","text":"<p>For convenience, if the first (and only) positional argument is present, then Xopt will automatically try to load from JSON, YAML str or files, or as a dict.</p>"},{"location":"examples/basic/xopt_basic/#convenience-methods","title":"Convenience Methods\u00b6","text":""},{"location":"examples/basic/xopt_basic/#logging","title":"Logging\u00b6","text":"<p>Normally Xopt will not issue print statments, and instead issue logging messages. Below will enable these to be seen in the notebook</p>"},{"location":"examples/basic/xopt_evaluator/","title":"Xopt Evaluator Basic Usage","text":"In\u00a0[1]: Copied! <pre># needed for macos\nimport platform\nif platform.system() == \"Darwin\": import multiprocessing;multiprocessing.set_start_method(\"fork\")\n</pre> # needed for macos import platform if platform.system() == \"Darwin\": import multiprocessing;multiprocessing.set_start_method(\"fork\")  In\u00a0[2]: Copied! <pre>from xopt import Xopt, Evaluator, Generator, VOCS\nfrom xopt.generators.random import RandomGenerator\n\nimport pandas as pd\n\nfrom time import sleep\nfrom numpy.random import randint\n\nfrom typing import Dict\n\nimport numpy as np\nnp.random.seed(666) # for reproducibility\n\n# Nicer plotting\n%config InlineBackend.figure_format = 'retina'\n</pre> from xopt import Xopt, Evaluator, Generator, VOCS from xopt.generators.random import RandomGenerator  import pandas as pd  from time import sleep from numpy.random import randint  from typing import Dict  import numpy as np np.random.seed(666) # for reproducibility  # Nicer plotting %config InlineBackend.figure_format = 'retina' <p>Define a custom function <code>f(inputs: Dict) -&gt; outputs: Dict</code>.</p> In\u00a0[3]: Copied! <pre>def f(inputs: Dict, a=2) -&gt; Dict:\n\n    sleep(randint(1, 5)*.1)  # simulate computation time\n    # Make some occasional errors\n    if np.any(inputs[\"x\"] &gt; 0.8):\n        raise ValueError(\"x &gt; 0.8\")\n\n    return {\"f1\": inputs[\"x\"] ** 2 + inputs[\"y\"] ** 2}\n</pre> def f(inputs: Dict, a=2) -&gt; Dict:      sleep(randint(1, 5)*.1)  # simulate computation time     # Make some occasional errors     if np.any(inputs[\"x\"] &gt; 0.8):         raise ValueError(\"x &gt; 0.8\")      return {\"f1\": inputs[\"x\"] ** 2 + inputs[\"y\"] ** 2} <p>Define variables, objectives, constraints, and other settings (VOCS)</p> In\u00a0[4]: Copied! <pre>vocs = VOCS(variables={\"x\": [0, 1], \"y\": [0, 1]}, objectives={\"f1\": \"MINIMIZE\"})\nvocs\n</pre> vocs = VOCS(variables={\"x\": [0, 1], \"y\": [0, 1]}, objectives={\"f1\": \"MINIMIZE\"}) vocs   Out[4]: <pre>VOCS(variables={'x': [0.0, 1.0], 'y': [0.0, 1.0]}, constraints={}, objectives={'f1': 'MINIMIZE'}, constants={}, linked_variables={})</pre> <p>This can be used to make some random inputs for evaluating the function.</p> In\u00a0[5]: Copied! <pre>in1 = vocs.random_inputs()\n\nf(in1)\n</pre> in1 = vocs.random_inputs()  f(in1) Out[5]: <pre>{'f1': 0.11401572022703582}</pre> In\u00a0[6]: Copied! <pre># Add in occasional errors. \ntry:\n    f({\"x\": 1, \"y\": 0})\nexcept Exception as ex:\n    print(f\"Caught error in f: {ex}\")\n</pre> # Add in occasional errors.  try:     f({\"x\": 1, \"y\": 0}) except Exception as ex:     print(f\"Caught error in f: {ex}\") <pre>Caught error in f: x &gt; 0.8\n</pre> In\u00a0[7]: Copied! <pre># Create Evaluator\nev = Evaluator(function=f)\n</pre> # Create Evaluator ev = Evaluator(function=f) In\u00a0[8]: Copied! <pre># Single input evaluation\nev.evaluate(in1)\n</pre> # Single input evaluation ev.evaluate(in1) Out[8]: <pre>{'f1': 0.11401572022703582,\n 'xopt_runtime': 0.2004687979999744,\n 'xopt_error': False}</pre> In\u00a0[9]: Copied! <pre># Dataframe evaluation\nin10 = vocs.random_inputs(10)\nev.evaluate_data(in10)\n</pre> # Dataframe evaluation in10 = vocs.random_inputs(10) ev.evaluate_data(in10)  Out[9]: f1 xopt_runtime xopt_error 0 0.529588 0.300557 False 1 1.154098 0.400637 False 2 0.641450 0.200469 False 3 0.619825 0.200429 False 4 0.536923 0.100325 False 5 0.342636 0.300605 False 6 0.589861 0.100329 False 7 0.556977 0.400626 False 8 0.055586 0.300566 False 9 0.785844 0.300571 False In\u00a0[10]: Copied! <pre># Dataframe submission (returns futures dict)\nfutures = ev.submit_data(in10)\nfor future in futures:\n    print(future.result())\n</pre> # Dataframe submission (returns futures dict) futures = ev.submit_data(in10) for future in futures:     print(future.result()) <pre>{'f1': 0.5295876188736232, 'xopt_runtime': 0.30059769599995434, 'xopt_error': False}\n{'f1': 1.154097903998413, 'xopt_runtime': 0.30060519699998167, 'xopt_error': False}\n{'f1': 0.641449645681512, 'xopt_runtime': 0.10036646799994742, 'xopt_error': False}\n{'f1': 0.6198251611679085, 'xopt_runtime': 0.10030876700000135, 'xopt_error': False}\n{'f1': 0.5369228584400957, 'xopt_runtime': 0.40063885900008245, 'xopt_error': False}\n{'f1': 0.3426360721182605, 'xopt_runtime': 0.3005668959999639, 'xopt_error': False}\n{'f1': 0.5898614653804399, 'xopt_runtime': 0.20053483300000607, 'xopt_error': False}\n{'f1': 0.5569771979382582, 'xopt_runtime': 0.40071366100005434, 'xopt_error': False}\n{'f1': 0.055585968775201305, 'xopt_runtime': 0.30054549499993755, 'xopt_error': False}\n{'f1': 0.7858444543806786, 'xopt_runtime': 0.30054689500002496, 'xopt_error': False}\n</pre> In\u00a0[11]: Copied! <pre># Dataframe evaluation, vectorized\nev.vectorized = True\nev.evaluate_data(in10)\n</pre> # Dataframe evaluation, vectorized ev.vectorized = True ev.evaluate_data(in10)  Out[11]: f1 xopt_runtime xopt_error 0 0.529588 0.402027 False 1 1.154098 0.402027 False 2 0.641450 0.402027 False 3 0.619825 0.402027 False 4 0.536923 0.402027 False 5 0.342636 0.402027 False 6 0.589861 0.402027 False 7 0.556977 0.402027 False 8 0.055586 0.402027 False 9 0.785844 0.402027 False In\u00a0[12]: Copied! <pre># Vectorized submission. This returns a single future.\nev.vectorized = True\nfutures = ev.submit_data(in10)\nlen(futures)\n</pre> # Vectorized submission. This returns a single future. ev.vectorized = True futures = ev.submit_data(in10) len(futures) Out[12]: <pre>1</pre> In\u00a0[13]: Copied! <pre>futures[0].result()\n</pre> futures[0].result() Out[13]: <pre>{'f1': array([0.52958762, 1.1540979 , 0.64144965, 0.61982516, 0.53692286,\n        0.34263607, 0.58986147, 0.5569772 , 0.05558597, 0.78584445]),\n 'xopt_runtime': 0.4006957599999623,\n 'xopt_error': False}</pre> In\u00a0[14]: Copied! <pre># Collect in a dataframe\nres = futures[0].result()\n# If there is an error, all outputs are spoiled.\nif res['xopt_error']:\n    res = [res]\npd.DataFrame(res)\n</pre> # Collect in a dataframe res = futures[0].result() # If there is an error, all outputs are spoiled. if res['xopt_error']:     res = [res] pd.DataFrame(res) Out[14]: f1 xopt_runtime xopt_error 0 0.529588 0.400696 False 1 1.154098 0.400696 False 2 0.641450 0.400696 False 3 0.619825 0.400696 False 4 0.536923 0.400696 False 5 0.342636 0.400696 False 6 0.589861 0.400696 False 7 0.556977 0.400696 False 8 0.055586 0.400696 False 9 0.785844 0.400696 False In\u00a0[15]: Copied! <pre>from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nMAX_WORKERS = 10\n</pre> from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor MAX_WORKERS = 10 In\u00a0[16]: Copied! <pre># Create Executor insance\nexecutor = ProcessPoolExecutor(max_workers=MAX_WORKERS)\nexecutor\n</pre> # Create Executor insance executor = ProcessPoolExecutor(max_workers=MAX_WORKERS) executor Out[16]: <pre>&lt;concurrent.futures.process.ProcessPoolExecutor at 0x7fb108d02430&gt;</pre> In\u00a0[17]: Copied! <pre># Dask (Optional)\n# from dask.distributed import Client\n# import logging\n# client = Client( silence_logs=logging.ERROR)\n# executor = client.get_executor()\n# client\n</pre> # Dask (Optional) # from dask.distributed import Client # import logging # client = Client( silence_logs=logging.ERROR) # executor = client.get_executor() # client In\u00a0[18]: Copied! <pre># This calls `executor.map`\nev = Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS)\n</pre> # This calls `executor.map` ev = Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS) In\u00a0[19]: Copied! <pre># This will run in parallel\nev.evaluate_data(in10)\n</pre> # This will run in parallel ev.evaluate_data(in10) Out[19]: f1 xopt_runtime xopt_error 0 0.529588 0.100995 False 1 1.154098 0.101071 False 2 0.641450 0.101394 False 3 0.619825 0.101595 False 4 0.536923 0.101749 False 5 0.342636 0.102077 False 6 0.589861 0.104533 False 7 0.556977 0.101355 False 8 0.055586 0.101554 False 9 0.785844 0.102891 False In\u00a0[20]: Copied! <pre>X = Xopt(generator=RandomGenerator(vocs), evaluator=Evaluator(function=f), vocs=vocs)\n\n# Submit to the evaluator some new inputs\nX.submit_data(vocs.random_inputs(4))\n\n# Unevaluated inputs are collected in a dataframe\nX._input_data\n</pre> X = Xopt(generator=RandomGenerator(vocs), evaluator=Evaluator(function=f), vocs=vocs)  # Submit to the evaluator some new inputs X.submit_data(vocs.random_inputs(4))  # Unevaluated inputs are collected in a dataframe X._input_data Out[20]: x y 1 0.333263 0.670598 2 0.127120 0.708698 3 0.478903 0.998964 4 0.249796 0.363820 In\u00a0[21]: Copied! <pre># Internal futures dictionary\nX._futures\n</pre> # Internal futures dictionary X._futures Out[21]: <pre>{1: &lt;Future at 0x7fb108c65b80 state=finished returned dict&gt;,\n 2: &lt;Future at 0x7fb108c65dc0 state=finished returned dict&gt;,\n 3: &lt;Future at 0x7fb108c65e20 state=finished returned dict&gt;,\n 4: &lt;Future at 0x7fb108c65f40 state=finished returned dict&gt;}</pre> In\u00a0[22]: Copied! <pre># Collect all finished futures and updata dataframe\nX.process_futures()\nX.data\n</pre> # Collect all finished futures and updata dataframe X.process_futures() X.data Out[22]: x y f1 xopt_runtime xopt_error 1 0.333263 0.670598 0.560765 0.400582 False 2 0.127120 0.708698 0.518413 0.400626 False 3 0.478903 0.998964 1.227277 0.400634 False 4 0.249796 0.363820 0.194763 0.400668 False In\u00a0[23]: Copied! <pre># Futures are now cleared out\nX._futures\n</pre> # Futures are now cleared out X._futures Out[23]: <pre>{}</pre> In\u00a0[24]: Copied! <pre># This is the internal counter\nX._ix_last\n</pre> # This is the internal counter X._ix_last Out[24]: <pre>4</pre> In\u00a0[25]: Copied! <pre># This causes immediate evaluation\nX.evaluate_data(vocs.random_inputs(4))\n</pre> # This causes immediate evaluation X.evaluate_data(vocs.random_inputs(4)) Out[25]: x y f1 xopt_runtime xopt_error 5 0.600748 0.031670 0.361902 0.100328 False 6 0.596771 0.132335 0.373648 0.100428 False 7 0.665451 0.208765 0.486407 0.101280 False 8 0.276934 0.555539 0.385316 0.300517 False In\u00a0[26]: Copied! <pre># Singe generation step\nX.step()\nX.data\n</pre> # Singe generation step X.step() X.data Out[26]: x y f1 xopt_runtime xopt_error 1 0.333263 0.670598 0.560765 0.400582 False 2 0.127120 0.708698 0.518413 0.400626 False 3 0.478903 0.998964 1.227277 0.400634 False 4 0.249796 0.363820 0.194763 0.400668 False 5 0.600748 0.031670 0.361902 0.100328 False 6 0.596771 0.132335 0.373648 0.100428 False 7 0.665451 0.208765 0.486407 0.101280 False 8 0.276934 0.555539 0.385316 0.300517 False 9 0.287898 0.757285 0.656366 0.100336 False In\u00a0[27]: Copied! <pre># Usage with a parallel executor. \nX2 = Xopt(\n    generator=RandomGenerator(vocs),\n    evaluator=Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS),\n    vocs=vocs,\n)\nX2.options.asynch = True\n</pre> # Usage with a parallel executor.  X2 = Xopt(     generator=RandomGenerator(vocs),     evaluator=Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS),     vocs=vocs, ) X2.options.asynch = True In\u00a0[28]: Copied! <pre>X2.step()\n</pre> X2.step() In\u00a0[29]: Copied! <pre>for _ in range(20):\n    X2.step()\n\nlen(X2.data)\n</pre> for _ in range(20):     X2.step()  len(X2.data) Out[29]: <pre>50</pre> In\u00a0[30]: Copied! <pre>X2.data.plot.scatter(\"x\", \"y\")\n</pre> X2.data.plot.scatter(\"x\", \"y\") Out[30]: <pre>&lt;Axes: xlabel='x', ylabel='y'&gt;</pre> In\u00a0[31]: Copied! <pre># Asynchronous, Vectorized\nX2 = Xopt(\n    generator=RandomGenerator(vocs),\n    evaluator=Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS),\n    vocs=vocs,\n)\nX2.options.asynch = True\nX2.evaluator.vectorized = True\n\n# This takes fewer steps to achieve a similar numver of evaluations\nfor _ in range(3):\n    X2.step()\n\nlen(X2.data)\n</pre> # Asynchronous, Vectorized X2 = Xopt(     generator=RandomGenerator(vocs),     evaluator=Evaluator(function=f, executor=executor, max_workers=MAX_WORKERS),     vocs=vocs, ) X2.options.asynch = True X2.evaluator.vectorized = True  # This takes fewer steps to achieve a similar numver of evaluations for _ in range(3):     X2.step()  len(X2.data) Out[31]: <pre>30</pre>"},{"location":"examples/basic/xopt_evaluator/#xopt-evaluator-basic-usage","title":"Xopt Evaluator Basic Usage\u00b6","text":"<p>The <code>Evaluator</code> handles the execution of the user-provided <code>function</code> with optional <code>function_kwags</code>, asyncrhonously and parallel, with exception handling.</p>"},{"location":"examples/basic/xopt_evaluator/#executors","title":"Executors\u00b6","text":""},{"location":"examples/basic/xopt_evaluator/#evaluator-in-the-xopt-object","title":"Evaluator in the Xopt object\u00b6","text":""},{"location":"examples/basic/xopt_generator/","title":"Working with Xopt generators","text":"In\u00a0[1]: Copied! <pre># Import the class\nfrom xopt.generators import generators, generator_default_options\n</pre> # Import the class from xopt.generators import generators, generator_default_options In\u00a0[2]: Copied! <pre># named generators\ngenerators.keys()\n</pre> # named generators generators.keys() Out[2]: <pre>dict_keys(['random', 'extremum_seeking', 'mggpo', 'rcds', 'cnsga', 'upper_confidence_bound', 'mobo', 'bayesian_exploration', 'time_dependent_upper_confidence_bound', 'expected_improvement', 'multi_fidelity', 'neldermead'])</pre> In\u00a0[3]: Copied! <pre># get default options for the upper confidence bound generator\noptions = generator_default_options[\"upper_confidence_bound\"]\n</pre> # get default options for the upper confidence bound generator options = generator_default_options[\"upper_confidence_bound\"] In\u00a0[4]: Copied! <pre># modify the UCB beta parameter and the number of initial samples\noptions.n_initial = 1\noptions.acq.beta = 1.0\n</pre> # modify the UCB beta parameter and the number of initial samples options.n_initial = 1 options.acq.beta = 1.0 In\u00a0[5]: Copied! <pre># define vocs for the problem\nfrom xopt.vocs import VOCS\nimport math\n\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> # define vocs for the problem from xopt.vocs import VOCS import math  vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[6]: Copied! <pre># create a new generator\ngenerator = generators[\"upper_confidence_bound\"](vocs=vocs, options=options)\ngenerator.options\n</pre> # create a new generator generator = generators[\"upper_confidence_bound\"](vocs=vocs, options=options) generator.options Out[6]: <pre>UCBOptions(optim=OptimOptions(num_restarts=20, raw_samples=20, sequential=True, max_travel_distances=None, use_turbo=False), acq=UpperConfidenceBoundOptions(proximal_lengthscales=None, use_transformed_proximal_weights=True, monte_carlo_samples=128, beta=1.0), model=ModelOptions(name='standard', custom_constructor=None, use_low_noise_prior=True, covar_modules={}, mean_modules={}), n_initial=1, use_cuda=False)</pre> In\u00a0[7]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\n\ndef test_function(input_dict):\n    return {\"f\": np.sin(input_dict[\"x\"])}\n</pre> # define a test function to optimize import numpy as np   def test_function(input_dict):     return {\"f\": np.sin(input_dict[\"x\"])} In\u00a0[8]: Copied! <pre># create xopt evaluator and run the optimization\nfrom xopt import Evaluator, Xopt\n\nevaluator = Evaluator(function=test_function)\n\nX = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX.options\n</pre> # create xopt evaluator and run the optimization from xopt import Evaluator, Xopt  evaluator = Evaluator(function=test_function)  X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X.options Out[8]: <pre>XoptOptions(asynch=False, strict=False, dump_file=None, max_evaluations=None)</pre> In\u00a0[9]: Copied! <pre># run the optimization for a couple of iterations (see bayes_opt folder for\n# more examples of ucb)\nfor i in range(4):\n    X.step()\n</pre> # run the optimization for a couple of iterations (see bayes_opt folder for # more examples of ucb) for i in range(4):     X.step() In\u00a0[10]: Copied! <pre>X.data\n</pre> X.data Out[10]: x f xopt_runtime xopt_error 1 3.458550 -3.116767e-01 0.000023 False 2 0.000000 0.000000e+00 0.000012 False 3 5.626197 -6.107348e-01 0.000013 False 4 6.283185 -2.449294e-16 0.000014 False In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/basic/xopt_generator/#working-with-xopt-generators","title":"Working with Xopt generators\u00b6","text":""},{"location":"examples/basic/xopt_parallel/","title":"Xopt Parallel Examples","text":"In\u00a0[1]: Copied! <pre>from xopt import Xopt\n</pre> from xopt import Xopt In\u00a0[2]: Copied! <pre># Helpers for this notebook\nimport multiprocessing\nN_CPUS=multiprocessing.cpu_count()\nN_CPUS\n\nimport os\n\n# directory for data. \nos.makedirs(\"temp\", exist_ok=True)\n\n# Notebook printing output\n#from xopt import output_notebook\n#output_notebook()\n\n# Nicer plotting\n%config InlineBackend.figure_format = 'retina'\n</pre> # Helpers for this notebook import multiprocessing N_CPUS=multiprocessing.cpu_count() N_CPUS  import os  # directory for data.  os.makedirs(\"temp\", exist_ok=True)  # Notebook printing output #from xopt import output_notebook #output_notebook()  # Nicer plotting %config InlineBackend.figure_format = 'retina'  <p>The <code>Xopt</code> object can be instantiated from a JSON or YAML file, or a dict, with the proper structure.</p> <p>Here we will make one</p> In\u00a0[3]: Copied! <pre># Make a proper input file.\nYAML = \"\"\"\nxopt:\n  asynch: True\n  max_evaluations: 1000\n\ngenerator:\n  name: cnsga\n  output_path: temp\n  population_size:  64\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  function_kwargs:\n    sleep: 0\n    random_sleep: 0.1\nvocs:\n  variables:\n    x1: [0, 3.14159]\n    x2: [0, 3.14159]\n  objectives: {y1: MINIMIZE, y2: MINIMIZE}\n  constraints:\n    c1: [GREATER_THAN, 0]\n    c2: [LESS_THAN, 0.5]\n  linked_variables: {x9: x1}\n  constants: {a: dummy_constant}\n\n\"\"\"\nX = Xopt(YAML)\nX\n</pre> # Make a proper input file. YAML = \"\"\" xopt:   asynch: True   max_evaluations: 1000  generator:   name: cnsga   output_path: temp   population_size:  64    evaluator:   function: xopt.resources.test_functions.tnk.evaluate_TNK   function_kwargs:     sleep: 0     random_sleep: 0.1    vocs:   variables:     x1: [0, 3.14159]     x2: [0, 3.14159]   objectives: {y1: MINIMIZE, y2: MINIMIZE}   constraints:     c1: [GREATER_THAN, 0]     c2: [LESS_THAN, 0.5]   linked_variables: {x9: x1}   constants: {a: dummy_constant}  \"\"\" X = Xopt(YAML) X Out[3]: <pre>\n            Xopt\n________________________________\nVersion: 1.1.2+31.g422c5a9.dirty\nData size: 0\nConfig as YAML:\nxopt: {asynch: true, strict: false, dump_file: null, max_evaluations: 1000}\ngenerator: {name: cnsga, population_size: 64, crossover_probability: 0.9, mutation_probability: 1.0,\n  population_file: null, output_path: temp}\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  max_workers: 1\n  function_kwargs: {sleep: 0, random_sleep: 0.1, raise_probability: 0}\n  vectorized: false\nvocs:\n  variables:\n    x1: [0.0, 3.14159]\n    x2: [0.0, 3.14159]\n  constraints:\n    c1: [GREATER_THAN, 0.0]\n    c2: [LESS_THAN, 0.5]\n  objectives: {y1: MINIMIZE, y2: MINIMIZE}\n  constants: {a: dummy_constant}\n  linked_variables: {x9: x1}\n</pre> In\u00a0[4]: Copied! <pre>%%timeit\n# Check that the average time is close to random_sleep\nX.evaluator.function({\"x1\": 0.5, \"x2\": 0.5}, random_sleep = .1)\n</pre> %%timeit # Check that the average time is close to random_sleep X.evaluator.function({\"x1\": 0.5, \"x2\": 0.5}, random_sleep = .1) <pre>96.5 ms \u00b1 16.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</pre> In\u00a0[5]: Copied! <pre>%%time\nX.run()\n</pre> %%time X.run() <pre>CPU times: user 6.94 s, sys: 225 ms, total: 7.16 s\nWall time: 1min 50s\n</pre> In\u00a0[6]: Copied! <pre>from concurrent.futures import ProcessPoolExecutor\n</pre> from concurrent.futures import ProcessPoolExecutor In\u00a0[7]: Copied! <pre>%%time\nX = Xopt(YAML)\n\nwith ProcessPoolExecutor() as executor:\n    X.evaluator.executor = executor\n    X.evaluator.max_workers = N_CPUS\n    X.run()\nlen(X.data)\n</pre> %%time X = Xopt(YAML)  with ProcessPoolExecutor() as executor:     X.evaluator.executor = executor     X.evaluator.max_workers = N_CPUS     X.run() len(X.data) <pre>CPU times: user 3.86 s, sys: 294 ms, total: 4.15 s\nWall time: 12.9 s\n</pre> Out[7]: <pre>1000</pre> In\u00a0[8]: Copied! <pre>from concurrent.futures import ThreadPoolExecutor\n</pre> from concurrent.futures import ThreadPoolExecutor In\u00a0[9]: Copied! <pre>%%time\n\nX = Xopt(YAML)\n\nwith ThreadPoolExecutor() as executor:\n    X.evaluator.executor = executor\n    X.evaluator.max_workers = N_CPUS\n    X.run()\nlen(X.data)\n</pre> %%time  X = Xopt(YAML)  with ThreadPoolExecutor() as executor:     X.evaluator.executor = executor     X.evaluator.max_workers = N_CPUS     X.run() len(X.data) <pre>CPU times: user 4.3 s, sys: 192 ms, total: 4.49 s\nWall time: 11.3 s\n</pre> Out[9]: <pre>1000</pre> In\u00a0[10]: Copied! <pre>X = Xopt(YAML)\nX.yaml('test.yaml') # Write this input to file\n!cat test.yaml\n</pre> X = Xopt(YAML) X.yaml('test.yaml') # Write this input to file !cat test.yaml <pre>xopt: {asynch: true, strict: false, dump_file: null, max_evaluations: 1000}\ngenerator: {name: cnsga, population_size: 64, crossover_probability: 0.9, mutation_probability: 1.0,\n  population_file: null, output_path: temp}\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  max_workers: 1\n  function_kwargs: {sleep: 0, random_sleep: 0.1, raise_probability: 0}\n  vectorized: false\nvocs:\n  variables:\n    x1: [0.0, 3.14159]\n    x2: [0.0, 3.14159]\n  constraints:\n    c1: [GREATER_THAN, 0.0]\n    c2: [LESS_THAN, 0.5]\n  objectives: {y1: MINIMIZE, y2: MINIMIZE}\n  constants: {a: dummy_constant}\n  linked_variables: {x9: x1}\n</pre> In\u00a0[11]: Copied! <pre>%%time\n!mpirun -n {N_CPUS} python -m mpi4py.futures -m xopt.mpi.run -vv --logfile xopt.log test.yaml\n</pre> %%time !mpirun -n {N_CPUS} python -m mpi4py.futures -m xopt.mpi.run -vv --logfile xopt.log test.yaml <pre>Namespace(input_file='test.yaml', logfile='xopt.log', verbose=2)\nParallel execution with 10 workers\nInitializing Xopt object\nInitializing generator cnsga,\nCreated toolbox with 2 variables, 2 constraints, and 2 objectives.\n    Using selection algorithm: nsga2\nInitializing Xopt object\nXopt object initialized\nEnabling async mode\n\n            Xopt\n________________________________\nVersion: 1.1.2+31.g422c5a9.dirty\nData size: 0\nConfig as YAML:\nxopt: {asynch: true, strict: false, dump_file: null, max_evaluations: 1000}\ngenerator: {name: cnsga, population_size: 64, crossover_probability: 0.9, mutation_probability: 1.0,\n  population_file: null, output_path: temp}\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  max_workers: 1\n  function_kwargs: {sleep: 0, random_sleep: 0.1, raise_probability: 0}\n  vectorized: false\nvocs:\n  variables:\n    x1: [0.0, 3.14159]\n    x2: [0.0, 3.14159]\n  constraints:\n    c1: [GREATER_THAN, 0.0]\n    c2: [LESS_THAN, 0.5]\n  objectives: {y1: MINIMIZE, y2: MINIMIZE}\n  constants: {a: dummy_constant}\n  linked_variables: {x9: x1}\n\n\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nRunning Xopt step\nXopt is done. Max evaluations 1000 reached.\n--------------------------------------------------------------------------\nA system call failed during shared memory initialization that should\nnot have.  It is likely that your MPI job will now either abort or\nexperience performance degradation.\n\n  Local host:  ChristophersMBP\n  System call: unlink(2) /var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T//ompi.ChristophersMBP.501/pid.15991/1/vader_segment.ChristophersMBP.501.89fb0001.4\n  Error:       No such file or directory (errno 2)\n--------------------------------------------------------------------------\nCPU times: user 180 ms, sys: 53.8 ms, total: 233 ms\nWall time: 14.8 s\n</pre> In\u00a0[12]: Copied! <pre>!tail xopt.log\n</pre> !tail xopt.log <pre>2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Running Xopt step\n2022-08-18T11:45:27-0700 - xopt.base - INFO - Xopt is done. Max evaluations 1000 reached.\n</pre> In\u00a0[13]: Copied! <pre>from dask.distributed import Client\nclient = Client()\nexecutor = client.get_executor()\nclient\n</pre> from dask.distributed import Client client = Client() executor = client.get_executor() client <pre>2022-08-18 11:45:28,982 - distributed.diskutils - INFO - Found stale lock file and directory '/var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-o2x19bac', purging\n2022-08-18 11:45:28,982 - distributed.diskutils - INFO - Found stale lock file and directory '/var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-pkfm2l0b', purging\n2022-08-18 11:45:28,982 - distributed.diskutils - INFO - Found stale lock file and directory '/var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-2pe9ydgc', purging\n2022-08-18 11:45:28,982 - distributed.diskutils - INFO - Found stale lock file and directory '/var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-3ol1dgla', purging\n2022-08-18 11:45:28,982 - distributed.diskutils - INFO - Found stale lock file and directory '/var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-37kim0g2', purging\n</pre> Out[13]: Client <p>Client-ee055da8-1f25-11ed-bd56-060412c509ec</p> Connection method: Cluster object Cluster type: distributed.LocalCluster Dashboard:  http://127.0.0.1:8787/status Cluster Info LocalCluster <p>e3a69ad2</p> Dashboard: http://127.0.0.1:8787/status Workers: 5                  Total threads: 10                  Total memory: 64.00 GiB                  Status: running Using processes: True Scheduler Info Scheduler <p>Scheduler-8735408f-e5e0-4929-bc89-e8e4f21e0082</p> Comm: tcp://127.0.0.1:54856                      Workers: 5                      Dashboard: http://127.0.0.1:8787/status Total threads: 10                      Started: Just now                      Total memory: 64.00 GiB                      Workers Worker: 0 Comm:  tcp://127.0.0.1:54896                          Total threads:  2                          Dashboard:  http://127.0.0.1:54897/status Memory:  12.80 GiB                          Nanny:  tcp://127.0.0.1:54861                          Local directory:  /var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-nlturonv                          Worker: 1 Comm:  tcp://127.0.0.1:54899                          Total threads:  2                          Dashboard:  http://127.0.0.1:54900/status Memory:  12.80 GiB                          Nanny:  tcp://127.0.0.1:54863                          Local directory:  /var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-wu2vuab_                          Worker: 2 Comm:  tcp://127.0.0.1:54902                          Total threads:  2                          Dashboard:  http://127.0.0.1:54903/status Memory:  12.80 GiB                          Nanny:  tcp://127.0.0.1:54862                          Local directory:  /var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-c67xq0c9                          Worker: 3 Comm:  tcp://127.0.0.1:54891                          Total threads:  2                          Dashboard:  http://127.0.0.1:54892/status Memory:  12.80 GiB                          Nanny:  tcp://127.0.0.1:54860                          Local directory:  /var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-l9dnpx62                          Worker: 4 Comm:  tcp://127.0.0.1:54890                          Total threads:  2                          Dashboard:  http://127.0.0.1:54893/status Memory:  12.80 GiB                          Nanny:  tcp://127.0.0.1:54859                          Local directory:  /var/folders/2f/l5_mybzs30j4qqvyj98w1_nw0000gn/T/dask-worker-space/worker-eerlov4r                          In\u00a0[14]: Copied! <pre>%%time\nX = Xopt(YAML)\nX.evaluator.executor = executor\nX.evaluator.max_workers = N_CPUS\nX.run()\nlen(X.data)\n</pre> %%time X = Xopt(YAML) X.evaluator.executor = executor X.evaluator.max_workers = N_CPUS X.run() len(X.data) <pre>CPU times: user 4.25 s, sys: 559 ms, total: 4.81 s\nWall time: 12.5 s\n</pre> Out[14]: <pre>1000</pre> In\u00a0[15]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[16]: Copied! <pre>X.data\n</pre> X.data Out[16]: x1 x2 a x9 y1 y2 c1 c2 some_array xopt_runtime xopt_error 10 2.471422 0.488736 dummy_constant 2.471422 2.471422 0.488736 5.446776 3.886633 [1, 2, 3] 0.011140 False 6 0.976978 2.405552 dummy_constant 0.976978 0.976978 2.405552 5.641776 3.858635 [1, 2, 3] 0.038039 False 7 0.455740 2.939118 dummy_constant 0.455740 0.455740 2.939118 7.923855 5.951254 [1, 2, 3] 0.074599 False 11 0.232046 0.367798 dummy_constant 0.232046 0.232046 0.367798 -0.719546 0.089277 [1, 2, 3] 0.036718 False 8 1.494465 2.718696 dummy_constant 1.494465 1.494465 2.718696 8.643407 5.911574 [1, 2, 3] 0.058163 False ... ... ... ... ... ... ... ... ... ... ... ... 998 0.801200 0.637918 dummy_constant NaN 0.801200 0.637918 0.072322 0.109743 [1, 2, 3] 0.037957 False 1006 0.480176 0.997499 dummy_constant NaN 0.480176 0.997499 0.163026 0.247899 [1, 2, 3] 0.008176 False 989 1.272588 0.147963 dummy_constant NaN 1.272588 0.147963 0.669124 0.720823 [1, 2, 3] 0.191139 False 1002 0.583817 0.787997 dummy_constant NaN 0.583817 0.787997 0.033047 0.089968 [1, 2, 3] 0.071444 False 997 0.875323 0.596263 dummy_constant NaN 0.875323 0.596263 0.220698 0.150134 [1, 2, 3] 0.105140 False <p>1000 rows \u00d7 11 columns</p> In\u00a0[17]: Copied! <pre>df = pd.concat([X.data, X.vocs.feasibility_data(X.data)], axis=1)\ndf[df['feasible']]\n</pre> df = pd.concat([X.data, X.vocs.feasibility_data(X.data)], axis=1) df[df['feasible']] Out[17]: x1 x2 a x9 y1 y2 c1 c2 some_array xopt_runtime xopt_error feasible_c1 feasible_c2 feasible 9 0.547623 1.169586 dummy_constant 0.547623 0.547623 1.169586 0.592854 0.450613 [1, 2, 3] 0.110823 False True True True 26 0.980003 0.426040 dummy_constant 0.980003 0.980003 0.426040 0.045761 0.235873 [1, 2, 3] 0.081441 False True True True 18 0.922711 0.491246 dummy_constant 0.922711 0.922711 0.491246 0.090082 0.178761 [1, 2, 3] 0.171685 False True True True 66 0.432459 1.093330 dummy_constant 0.432459 0.432459 1.093330 0.285668 0.356603 [1, 2, 3] 0.087319 False True True True 72 0.504830 1.092646 dummy_constant 0.504830 0.504830 1.092646 0.368625 0.351253 [1, 2, 3] 0.009447 False True True True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1000 1.048656 0.099549 dummy_constant NaN 1.048656 0.099549 0.103946 0.461385 [1, 2, 3] 0.014905 False True True True 998 0.801200 0.637918 dummy_constant NaN 0.801200 0.637918 0.072322 0.109743 [1, 2, 3] 0.037957 False True True True 1006 0.480176 0.997499 dummy_constant NaN 0.480176 0.997499 0.163026 0.247899 [1, 2, 3] 0.008176 False True True True 1002 0.583817 0.787997 dummy_constant NaN 0.583817 0.787997 0.033047 0.089968 [1, 2, 3] 0.071444 False True True True 997 0.875323 0.596263 dummy_constant NaN 0.875323 0.596263 0.220698 0.150134 [1, 2, 3] 0.105140 False True True True <p>439 rows \u00d7 14 columns</p> In\u00a0[18]: Copied! <pre># Plot the feasible ones\nfeasible_df = df[df[\"feasible\"]]\nfeasible_df.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\")\n</pre> # Plot the feasible ones feasible_df = df[df[\"feasible\"]] feasible_df.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\") In\u00a0[19]: Copied! <pre># Plot the infeasible ones\ninfeasible_df = df[~df[\"feasible\"]]\ninfeasible_df.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\")\n</pre> # Plot the infeasible ones infeasible_df = df[~df[\"feasible\"]] infeasible_df.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\") In\u00a0[20]: Copied! <pre># This is the final population\ndf1 = X.generator.population\ndf1.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\")\n</pre> # This is the final population df1 = X.generator.population df1.plot(\"y1\", \"y2\", kind=\"scatter\").set_aspect(\"equal\") In\u00a0[21]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[22]: Copied! <pre># Extract objectives from output\nk1, k2 = \"y1\", \"y2\"\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nax.scatter(\n    infeasible_df[k1],\n    infeasible_df[k2],\n    color=\"blue\",\n    marker=\".\",\n    alpha=0.5,\n    label=\"infeasible\",\n)\nax.scatter(\n    feasible_df[k1], feasible_df[k2], color=\"orange\", marker=\".\", label=\"feasible\"\n)\nax.scatter(df1[k1], df1[k2], color=\"red\", marker=\".\", label=\"final population\")\nax.set_xlabel(k1)\nax.set_ylabel(k2)\nax.set_aspect(\"auto\")\nax.set_title(f\"Xopt's CNSGA algorithm\")\nplt.legend()\n</pre> # Extract objectives from output k1, k2 = \"y1\", \"y2\"  fig, ax = plt.subplots(figsize=(6, 6))  ax.scatter(     infeasible_df[k1],     infeasible_df[k2],     color=\"blue\",     marker=\".\",     alpha=0.5,     label=\"infeasible\", ) ax.scatter(     feasible_df[k1], feasible_df[k2], color=\"orange\", marker=\".\", label=\"feasible\" ) ax.scatter(df1[k1], df1[k2], color=\"red\", marker=\".\", label=\"final population\") ax.set_xlabel(k1) ax.set_ylabel(k2) ax.set_aspect(\"auto\") ax.set_title(f\"Xopt's CNSGA algorithm\") plt.legend() Out[22]: <pre>&lt;matplotlib.legend.Legend at 0x1429c0b80&gt;</pre> In\u00a0[23]: Copied! <pre># Cleanup\n!rm -r dask-worker-space\n!rm -r temp\n!rm xopt.log*\n!rm test.yaml\n</pre> # Cleanup !rm -r dask-worker-space !rm -r temp !rm xopt.log* !rm test.yaml <pre>rm: dask-worker-space: No such file or directory\n</pre>"},{"location":"examples/basic/xopt_parallel/#xopt-parallel-examples","title":"Xopt Parallel Examples\u00b6","text":"<p>Xopt provides methods to parallelize optimizations using Processes, Threads, MPI, and Dask using the <code>concurrent.futures</code> interface as defined in  https://www.python.org/dev/peps/pep-3148/ .</p>"},{"location":"examples/basic/xopt_parallel/#processes","title":"Processes\u00b6","text":""},{"location":"examples/basic/xopt_parallel/#threads","title":"Threads\u00b6","text":"<p>Continue running, this time with threads.</p>"},{"location":"examples/basic/xopt_parallel/#mpi","title":"MPI\u00b6","text":"<p>The <code>test.yaml</code> file completely defines the problem. We will also direct the logging to an <code>xopt.log</code> file. The following invocation recruits 4 MPI workers to solve this problem.</p> <p>We can also continue by calling <code>.save</code> with a JSON filename. This will write all of previous results into the file.</p>"},{"location":"examples/basic/xopt_parallel/#dask","title":"Dask\u00b6","text":""},{"location":"examples/basic/xopt_parallel/#load-output-into-pandas","title":"Load output into Pandas\u00b6","text":"<p>This algorithm writes two types of files: <code>gen_{i}.json</code> with all of the new individuals evaluated in a generation, and <code>pop_{i}.json</code> with the latest best population. Xopt provides some functions to load these easily into a Pandas dataframe for further analysis.</p>"},{"location":"examples/basic/xopt_parallel/#matplotlib-plotting","title":"matplotlib plotting\u00b6","text":"<p>You can always use matplotlib for customizable plotting</p>"},{"location":"examples/basic/xopt_vocs/","title":"VOCS data structure","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\n</pre> from xopt.vocs import VOCS In\u00a0[2]: Copied! <pre>help(VOCS)\n</pre> help(VOCS) <pre>Help on class VOCS in module xopt.vocs:\n\nclass VOCS(xopt.pydantic.XoptBaseModel)\n |  VOCS(*, variables: Dict[str, types.ConstrainedListValue] = {}, constraints: Dict[str, types.ConstrainedListValue] = {}, objectives: Dict[str, xopt.vocs.ObjectiveEnum] = {}, constants: Dict[str, Any] = {}, linked_variables: Dict[str, str] = {}) -&gt; None\n |  \n |  Variables, Objectives, Constraints, and other Settings (VOCS) data structure\n |  to describe optimization problems.\n |  \n |  Method resolution order:\n |      VOCS\n |      xopt.pydantic.XoptBaseModel\n |      pydantic.main.BaseModel\n |      pydantic.utils.Representation\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  as_yaml(self)\n |  \n |  constraint_data(self, data: Union[pandas.core.frame.DataFrame, List[Dict]], prefix: str = 'constraint_') -&gt; pandas.core.frame.DataFrame\n |      Returns a dataframe containing constraint data transformed according to\n |      `vocs.constraints` such that values that satisfy each constraint are negative.\n |      \n |      Args:\n |          data: data to be processed.\n |          prefix: prefix added to column names.\n |      \n |      Returns:\n |          result: processed Dataframe\n |  \n |  convert_dataframe_to_inputs(self, data: pandas.core.frame.DataFrame) -&gt; pandas.core.frame.DataFrame\n |      Extracts only inputs from a dataframe.\n |      This will add constants.\n |  \n |  convert_numpy_to_inputs(self, inputs: numpy.ndarray) -&gt; pandas.core.frame.DataFrame\n |      convert 2D numpy array to list of dicts (inputs) for evaluation\n |      Assumes that the columns of the array match correspond to\n |      `sorted(self.vocs.variables.keys())\n |  \n |  extract_data(self, data: pandas.core.frame.DataFrame, return_raw=False)\n |      split dataframe into seperate dataframes for variables, objectives and\n |      constraints based on vocs - objective data is transformed based on\n |      `vocs.objectives` properties\n |      \n |      Args:\n |          data: dataframe to be split\n |          return_raw: if True, return untransformed objective data\n |      \n |      Returns:\n |          variable_data: dataframe containing variable data\n |          objective_data: dataframe containing objective data\n |          constraint_data: dataframe containing constraint data\n |  \n |  feasibility_data(self, data: Union[pandas.core.frame.DataFrame, List[Dict]], prefix: str = 'feasible_') -&gt; pandas.core.frame.DataFrame\n |      Returns a dataframe containing booleans denoting if a constraint is satisfied or\n |      not. Returned dataframe also contains a column `feasibility` which denotes if\n |      all constraints are satisfied.\n |      \n |      Args:\n |          data: data to be processed.\n |          prefix: prefix added to column names.\n |      \n |      Returns:\n |          result: processed Dataframe\n |  \n |  objective_data(self, data: Union[pandas.core.frame.DataFrame, List[Dict]], prefix: str = 'objective_', return_raw=False) -&gt; pandas.core.frame.DataFrame\n |      Returns a dataframe containing objective data transformed according to\n |      `vocs.objectives` such that we always assume minimization.\n |      \n |      Args:\n |          data: data to be processed.\n |          prefix: prefix added to column names.\n |      \n |      Returns:\n |          result: processed Dataframe\n |  \n |  random_inputs(self, n=None, include_constants=True, include_linked_variables=True)\n |      Uniform sampling of the variables.\n |      \n |      Returns a dict of inputs.\n |      \n |      If include_constants, the vocs.constants are added to the dict.\n |      \n |      Optional:\n |          n (integer) to make arrays of inputs, of size n.\n |  \n |  validate_input_data(self, input_points: pandas.core.frame.DataFrame) -&gt; None\n |      Validates input data. Raises an error if the input data does not satisfy\n |      requirements given by vocs.\n |      \n |      Args:\n |          input_points: input data to be validated.\n |      \n |      Returns:\n |          None\n |      \n |      Raises:\n |          ValueError: if input data does not satisfy requirements.\n |  \n |  variable_data(self, data: Union[pandas.core.frame.DataFrame, List[Dict]], prefix: str = 'variable_') -&gt; pandas.core.frame.DataFrame\n |      Returns a dataframe containing variables according to `vocs.variables` in sorted\n |      order\n |      \n |      Args:\n |          data: Data to be processed.\n |          prefix: Prefix added to column names.\n |      \n |      Returns:\n |          result: processed Dataframe\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  from_yaml(yaml_text) from pydantic.main.ModelMetaclass\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __json_encoder__ = pydantic_encoder(obj: Any) -&gt; Any\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  all_names\n |      Returns all vocs names (variables, constants, objectives, constraints\n |  \n |  bounds\n |      Returns a bounds array (mins, maxs) of shape (2, n_variables)\n |      Arrays of lower and upper bounds can be extracted by:\n |          mins, maxs = vocs.bounds\n |  \n |  constant_names\n |      Returns a sorted list of constraint names\n |  \n |  constraint_names\n |      Returns a sorted list of constraint names\n |  \n |  n_constants\n |      Returns the number of constants\n |  \n |  n_constraints\n |      Returns the number of constraints\n |  \n |  n_inputs\n |      Returns the number of inputs (variables and constants)\n |  \n |  n_objectives\n |      Returns the number of objectives\n |  \n |  n_outputs\n |      Returns the number of outputs (objectives and constraints)\n |  \n |  n_variables\n |      Returns the number of variables\n |  \n |  objective_names\n |      Returns a sorted list of objective names\n |  \n |  output_names\n |      Returns a sorted list of objective and constraint names (objectives first\n |      then constraints)\n |  \n |  variable_names\n |      Returns a sorted list of variable names\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  Config = &lt;class 'xopt.vocs.VOCS.Config'&gt;\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  __annotations__ = {'constants': typing.Dict[str, typing.Any], 'constra...\n |  \n |  __class_vars__ = set()\n |  \n |  __config__ = &lt;class 'xopt.vocs.Config'&gt;\n |  \n |  __custom_root_type__ = False\n |  \n |  __exclude_fields__ = None\n |  \n |  __fields__ = {'constants': ModelField(name='constants', type=Mapping[s...\n |  \n |  __hash__ = None\n |  \n |  __include_fields__ = None\n |  \n |  __post_root_validators__ = []\n |  \n |  __pre_root_validators__ = []\n |  \n |  __private_attributes__ = {}\n |  \n |  __schema_cache__ = {}\n |  \n |  __signature__ = &lt;Signature (*, variables: Dict[str, types.Constr..., l...\n |  \n |  __validators__ = {}\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pydantic.main.BaseModel:\n |  \n |  __eq__(self, other: Any) -&gt; bool\n |  \n |  __getstate__(self) -&gt; 'DictAny'\n |  \n |  __init__(__pydantic_self__, **data: Any) -&gt; None\n |      Create a new model by parsing and validating input data from keyword arguments.\n |      \n |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n |  \n |  __iter__(self) -&gt; 'TupleGenerator'\n |      so `dict(model)` works\n |  \n |  __repr_args__(self) -&gt; 'ReprArgs'\n |  \n |  __setattr__(self, name, value)\n |  \n |  __setstate__(self, state: 'DictAny') -&gt; None\n |  \n |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -&gt; 'Model'\n |      Duplicate a model, optionally choose which fields to include, exclude and change.\n |      \n |      :param include: fields to include in new model\n |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n |          the new model: you should trust this data\n |      :param deep: set to `True` to make a deep copy of the model\n |      :return: new model instance\n |  \n |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -&gt; 'DictStrAny'\n |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n |  \n |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -&gt; 'unicode'\n |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n |      \n |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pydantic.main.BaseModel:\n |  \n |  __get_validators__() -&gt; 'CallableGenerator' from pydantic.main.ModelMetaclass\n |  \n |  __try_update_forward_refs__(**localns: Any) -&gt; None from pydantic.main.ModelMetaclass\n |      Same as update_forward_refs but will not raise exception\n |      when forward references are not defined.\n |  \n |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -&gt; 'Model' from pydantic.main.ModelMetaclass\n |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n |      Default values are respected, but no other validation is performed.\n |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n |  \n |  from_orm(obj: Any) -&gt; 'Model' from pydantic.main.ModelMetaclass\n |  \n |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model' from pydantic.main.ModelMetaclass\n |  \n |  parse_obj(obj: Any) -&gt; 'Model' from pydantic.main.ModelMetaclass\n |  \n |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model' from pydantic.main.ModelMetaclass\n |  \n |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -&gt; 'DictStrAny' from pydantic.main.ModelMetaclass\n |  \n |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -&gt; 'unicode' from pydantic.main.ModelMetaclass\n |  \n |  update_forward_refs(**localns: Any) -&gt; None from pydantic.main.ModelMetaclass\n |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n |  \n |  validate(value: Any) -&gt; 'Model' from pydantic.main.ModelMetaclass\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pydantic.main.BaseModel:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __fields_set__\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pydantic.utils.Representation:\n |  \n |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -&gt; Generator[Any, NoneType, NoneType]\n |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n |  \n |  __repr__(self) -&gt; 'unicode'\n |  \n |  __repr_name__(self) -&gt; 'unicode'\n |      Name of the instance's class, used in __repr__.\n |  \n |  __repr_str__(self, join_str: 'unicode') -&gt; 'unicode'\n |  \n |  __rich_repr__(self) -&gt; 'RichReprResult'\n |      Get fields for Rich library\n |  \n |  __str__(self) -&gt; 'unicode'\n\n</pre> In\u00a0[3]: Copied! <pre>Y = \"\"\"\nvariables:\n  a: [0, 1e3] # Note that 1e3 usually parses as a str with YAML. \n  b: [-1, 1]\nobjectives:\n  c: maximize\n  d: minimize \nconstraints:\n  e: ['Less_than', 2]\n  f: ['greater_than', 0]\nconstants:\n  g: 1234\n\n\"\"\"\n\nvocs = VOCS.from_yaml(Y)\nvocs\n</pre> Y = \"\"\" variables:   a: [0, 1e3] # Note that 1e3 usually parses as a str with YAML.    b: [-1, 1] objectives:   c: maximize   d: minimize  constraints:   e: ['Less_than', 2]   f: ['greater_than', 0] constants:   g: 1234  \"\"\"  vocs = VOCS.from_yaml(Y) vocs Out[3]: <pre>VOCS(variables={'a': [0.0, 1000.0], 'b': [-1.0, 1.0]}, constraints={'e': ['LESS_THAN', 2.0], 'f': ['GREATER_THAN', 0.0]}, objectives={'c': 'MAXIMIZE', 'd': 'MINIMIZE'}, constants={'g': 1234}, linked_variables={})</pre> In\u00a0[4]: Copied! <pre># as dict\nvocs.dict()\n</pre> # as dict vocs.dict() Out[4]: <pre>{'variables': {'a': [0.0, 1000.0], 'b': [-1.0, 1.0]},\n 'constraints': {'e': ['LESS_THAN', 2.0], 'f': ['GREATER_THAN', 0.0]},\n 'objectives': {'c': 'MAXIMIZE', 'd': 'MINIMIZE'},\n 'constants': {'g': 1234},\n 'linked_variables': {}}</pre> In\u00a0[5]: Copied! <pre>#  re-parse dict\nvocs2 = VOCS.parse_obj(vocs.dict())\n</pre> #  re-parse dict vocs2 = VOCS.parse_obj(vocs.dict()) In\u00a0[6]: Copied! <pre># Check that these are the same\nvocs2 == vocs\n</pre> # Check that these are the same vocs2 == vocs Out[6]: <pre>True</pre> In\u00a0[7]: Copied! <pre># This replaces the old vocs[\"variables\"]\ngetattr(vocs, \"variables\")\n</pre> # This replaces the old vocs[\"variables\"] getattr(vocs, \"variables\") Out[7]: <pre>{'a': [0.0, 1000.0], 'b': [-1.0, 1.0]}</pre> In\u00a0[8]: Copied! <pre>vocs.objectives[\"c\"] == 'MAXIMIZE'\n</pre> vocs.objectives[\"c\"] == 'MAXIMIZE' Out[8]: <pre>True</pre> In\u00a0[9]: Copied! <pre># json\nvocs.json()\n</pre> # json vocs.json() Out[9]: <pre>'{\"variables\":{\"a\":[0.0,1000.0],\"b\":[-1.0,1.0]},\"constraints\":{\"e\":[\"LESS_THAN\",2.0],\"f\":[\"GREATER_THAN\",0.0]},\"objectives\":{\"c\":\"MAXIMIZE\",\"d\":\"MINIMIZE\"},\"constants\":{\"g\":1234},\"linked_variables\":{}}'</pre> In\u00a0[10]: Copied! <pre>from xopt.vocs import form_objective_data, form_constraint_data, form_feasibility_data\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame(vocs.random_inputs(10))\n# Add some outputs\ndata[\"c\"] = data[\"a\"] + data[\"b\"]\ndata[\"d\"] = data[\"a\"] - data[\"b\"]\ndata[\"e\"] = data[\"a\"] * 2 + data[\"b\"] * 2\ndata[\"f\"] = data[\"a\"] * 2 - data[\"b\"] * 2\ndata.index = np.arange(len(data)) + 5  # custom index\ndata\n</pre> from xopt.vocs import form_objective_data, form_constraint_data, form_feasibility_data import pandas as pd import numpy as np  data = pd.DataFrame(vocs.random_inputs(10)) # Add some outputs data[\"c\"] = data[\"a\"] + data[\"b\"] data[\"d\"] = data[\"a\"] - data[\"b\"] data[\"e\"] = data[\"a\"] * 2 + data[\"b\"] * 2 data[\"f\"] = data[\"a\"] * 2 - data[\"b\"] * 2 data.index = np.arange(len(data)) + 5  # custom index data Out[10]: a b g c d e f 5 375.574477 -0.595480 1234 374.978997 376.169956 749.957994 752.339913 6 86.000177 0.887992 1234 86.888169 85.112184 173.776339 170.224369 7 351.074996 -0.405325 1234 350.669671 351.480321 701.339342 702.960643 8 835.912761 -0.014551 1234 835.898209 835.927312 1671.796419 1671.854624 9 877.972502 -0.698420 1234 877.274082 878.670922 1754.548165 1757.341843 10 831.509928 0.892653 1234 832.402580 830.617275 1664.805161 1661.234550 11 365.130950 -0.087207 1234 365.043744 365.218157 730.087488 730.436314 12 464.291829 -0.408253 1234 463.883576 464.700083 927.767153 929.400165 13 471.213601 0.520539 1234 471.734140 470.693062 943.468281 941.386125 14 808.891195 -0.744540 1234 808.146654 809.635735 1616.293308 1619.271470 In\u00a0[11]: Copied! <pre>vocs.objectives\n</pre> vocs.objectives Out[11]: <pre>{'c': 'MAXIMIZE', 'd': 'MINIMIZE'}</pre> In\u00a0[12]: Copied! <pre># These are in standard form for minimization\nform_objective_data(vocs.objectives, data)\n</pre> # These are in standard form for minimization form_objective_data(vocs.objectives, data) Out[12]: objective_c objective_d 5 -374.978997 376.169956 6 -86.888169 85.112184 7 -350.669671 351.480321 8 -835.898209 835.927312 9 -877.274082 878.670922 10 -832.402580 830.617275 11 -365.043744 365.218157 12 -463.883576 464.700083 13 -471.734140 470.693062 14 -808.146654 809.635735 In\u00a0[13]: Copied! <pre># This is also available as a method\nvocs.objective_data(data)\n</pre> # This is also available as a method vocs.objective_data(data) Out[13]: objective_c objective_d 5 -374.978997 376.169956 6 -86.888169 85.112184 7 -350.669671 351.480321 8 -835.898209 835.927312 9 -877.274082 878.670922 10 -832.402580 830.617275 11 -365.043744 365.218157 12 -463.883576 464.700083 13 -471.734140 470.693062 14 -808.146654 809.635735 In\u00a0[14]: Copied! <pre># use the to_numpy() method to convert for low level use.\nvocs.objective_data(data).to_numpy()\n</pre> # use the to_numpy() method to convert for low level use. vocs.objective_data(data).to_numpy() Out[14]: <pre>array([[-374.97899719,  376.16995634],\n       [ -86.88816931,   85.11218445],\n       [-350.66967111,  351.48032145],\n       [-835.89820932,  835.92731185],\n       [-877.27408244,  878.67092171],\n       [-832.40258033,  830.61727512],\n       [-365.04374388,  365.21815705],\n       [-463.88357629,  464.70008262],\n       [-471.73414026,  470.69306227],\n       [-808.14665421,  809.63573503]])</pre> In\u00a0[15]: Copied! <pre>vocs.constraint_data(data)\n</pre> vocs.constraint_data(data) Out[15]: constraint_e constraint_f 5 747.957994 -752.339913 6 171.776339 -170.224369 7 699.339342 -702.960643 8 1669.796419 -1671.854624 9 1752.548165 -1757.341843 10 1662.805161 -1661.234550 11 728.087488 -730.436314 12 925.767153 -929.400165 13 941.468281 -941.386125 14 1614.293308 -1619.271470 In\u00a0[16]: Copied! <pre>vocs.feasibility_data(data)\n</pre> vocs.feasibility_data(data) Out[16]: feasible_e feasible_f feasible 5 False True False 6 False True False 7 False True False 8 False True False 9 False True False 10 False True False 11 False True False 12 False True False 13 False True False 14 False True False In\u00a0[17]: Copied! <pre>Y = \"\"\"\nvariables:\n  a: [0, 1e3] # Note that 1e3 usually parses as a str with YAML. \n  b: [-1, 1]\nobjectives:\n  c: maximize\n  d: minimize \nconstraints:\n  e: ['Less_than', 2]\n  f: ['greater_than', 0]\nconstants:\n  g: 1234\n\n\"\"\"\n\nvocs = VOCS.from_yaml(Y)\n</pre> Y = \"\"\" variables:   a: [0, 1e3] # Note that 1e3 usually parses as a str with YAML.    b: [-1, 1] objectives:   c: maximize   d: minimize  constraints:   e: ['Less_than', 2]   f: ['greater_than', 0] constants:   g: 1234  \"\"\"  vocs = VOCS.from_yaml(Y) In\u00a0[18]: Copied! <pre>d = {'a': [1,2,3]}\n\ndf = pd.DataFrame(d)\ndf2 = pd.DataFrame(df).copy()\n\ndf2['b'] = np.nan\ndf2['b'] - 1\n</pre> d = {'a': [1,2,3]}  df = pd.DataFrame(d) df2 = pd.DataFrame(df).copy()  df2['b'] = np.nan df2['b'] - 1 Out[18]: <pre>0   NaN\n1   NaN\n2   NaN\nName: b, dtype: float64</pre> In\u00a0[19]: Copied! <pre>data['a']  = np.nan\n</pre> data['a']  = np.nan In\u00a0[20]: Copied! <pre>a = 2\ndef f(x=a):\n    return x\na=99\nf()\n</pre> a = 2 def f(x=a):     return x a=99 f() Out[20]: <pre>2</pre> In\u00a0[21]: Copied! <pre>pd.DataFrame(6e66, index=[1,2,3], columns=['A'])\n</pre> pd.DataFrame(6e66, index=[1,2,3], columns=['A']) Out[21]: A 1 6.000000e+66 2 6.000000e+66 3 6.000000e+66 In\u00a0[22]: Copied! <pre># These are in standard form for minimization\n\ndata = pd.DataFrame({'c':[1,2,3,4]}, index=[9,3,4,5])\n\nform_objective_data(vocs.objectives, data)\n</pre> # These are in standard form for minimization  data = pd.DataFrame({'c':[1,2,3,4]}, index=[9,3,4,5])  form_objective_data(vocs.objectives, data) Out[22]: objective_c objective_d 9 -1.0 inf 3 -2.0 inf 4 -3.0 inf 5 -4.0 inf"},{"location":"examples/basic/xopt_vocs/#vocs-data-structure","title":"VOCS data structure\u00b6","text":"<p>Variables, Objectives, Constraints, and other Settings (VOCS) helps define our optimization problems.</p>"},{"location":"examples/basic/xopt_vocs/#objective-evaluation","title":"Objective Evaluation\u00b6","text":""},{"location":"examples/basic/xopt_vocs/#error-handling","title":"Error handling\u00b6","text":""},{"location":"examples/bayes_exp/bayesian_exploration/","title":"Bayesian Exploration","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport torch\nimport yaml\nfrom copy import deepcopy\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import BayesianExplorationGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\nvocs = deepcopy(tnk_vocs)\n\n# can only explore one objective\ndel vocs.objectives[\"y2\"]\n\ngenerator_options = BayesianExplorationGenerator.default_options()\ngenerator_options.optim.num_restarts = 20\ngenerator_options.optim.raw_samples = 20\ngenerator_options.optim.max_travel_distances = [0.25, 0.25]\n\nevaluator = Evaluator(function=evaluate_TNK)\ngenerator = BayesianExplorationGenerator(vocs, generator_options)\n\nprint(yaml.dump(generator.options.dict()))\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import pandas as pd import torch import yaml from copy import deepcopy from xopt import Xopt, Evaluator from xopt.generators.bayesian import BayesianExplorationGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs  vocs = deepcopy(tnk_vocs)  # can only explore one objective del vocs.objectives[\"y2\"]  generator_options = BayesianExplorationGenerator.default_options() generator_options.optim.num_restarts = 20 generator_options.optim.raw_samples = 20 generator_options.optim.max_travel_distances = [0.25, 0.25]  evaluator = Evaluator(function=evaluate_TNK) generator = BayesianExplorationGenerator(vocs, generator_options)  print(yaml.dump(generator.options.dict())) <pre>acq:\n  monte_carlo_samples: 128\n  proximal_lengthscales: null\n  use_transformed_proximal_weights: true\nmodel:\n  covar_modules: {}\n  custom_constructor: null\n  mean_modules: {}\n  name: standard\n  use_low_noise_prior: true\nn_initial: 3\noptim:\n  max_travel_distances:\n  - 0.25\n  - 0.25\n  num_restarts: 20\n  raw_samples: 20\n  sequential: true\n  use_turbo: false\nuse_cuda: false\n\n</pre> In\u00a0[2]: Copied! <pre>X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.7, 0.95]}))\n</pre> X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.7, 0.95]})) Out[2]: x1 x2 y1 y2 c1 c2 some_array xopt_runtime xopt_error 1 1.00 0.70 1.00 0.70 0.584045 0.290 [1, 2, 3] 0.000050 False 2 0.75 0.95 0.75 0.95 0.494833 0.265 [1, 2, 3] 0.000018 False In\u00a0[3]: Copied! <pre>for i in range(10):\n    print(f\"step {i}\")\n    X.step()\n</pre> for i in range(10):     print(f\"step {i}\")     X.step() <pre>step 0\nstep 1\nstep 2\nstep 3\nstep 4\nstep 5\nstep 6\nstep 7\nstep 8\nstep 9\n</pre> In\u00a0[4]: Copied! <pre># view the data\nX.data\n</pre> # view the data X.data Out[4]: x1 x2 y1 y2 c1 c2 some_array xopt_runtime xopt_error a 1 1.000000 0.700000 1.000000 0.700000 0.584045 0.290000 [1, 2, 3] 0.000050 False NaN 2 0.750000 0.950000 0.750000 0.950000 0.494833 0.265000 [1, 2, 3] 0.000018 False NaN 3 1.535397 1.735397 1.535397 1.735397 4.313110 2.598255 [1, 2, 3] 0.000037 False dummy_constant 4 0.909918 2.277666 0.909918 2.277666 4.917744 3.328127 [1, 2, 3] 0.000039 False dummy_constant 5 1.645107 1.623038 1.645107 1.623038 4.241211 2.572483 [1, 2, 3] 0.000035 False dummy_constant 6 0.884853 0.893068 0.884853 0.893068 0.480808 0.302614 [1, 2, 3] 0.000040 False dummy_constant 7 0.099455 0.534514 0.099455 0.534514 -0.606361 0.161627 [1, 2, 3] 0.000044 False dummy_constant 8 0.463592 0.971354 0.463592 0.971354 0.091812 0.223500 [1, 2, 3] 0.000043 False dummy_constant 9 0.745696 0.374562 0.745696 0.374562 -0.343155 0.076101 [1, 2, 3] 0.000058 False dummy_constant 10 1.040713 0.449614 1.040713 0.449614 0.188145 0.294910 [1, 2, 3] 0.000074 False dummy_constant 11 1.126217 0.445726 1.126217 0.445726 0.370232 0.395094 [1, 2, 3] 0.000044 False dummy_constant 12 0.868667 0.656299 0.868667 0.656299 0.245281 0.160345 [1, 2, 3] 0.000048 False dummy_constant In\u00a0[5]: Copied! <pre># plot results\nax = X.data.plot(\"x1\", \"x2\")\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(\"x1\", \"x2\") ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre>from matplotlib import pyplot as plt  # plot model predictions\n\ndata = X.data\n\nbounds = generator.vocs.bounds\nmodel = generator.train_model(generator.data)\n\n# create mesh\nn = 50\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\noutputs = generator.vocs.output_names\nwith torch.no_grad():\n    post = model.posterior(pts)\n\n    for i in range(len(vocs.output_names)):\n        mean = post.mean[...,i]\n        fig, ax = plt.subplots()\n        ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C1\")\n        c = ax.pcolor(\n            xx, yy, mean.squeeze().reshape(n, n),\n            cmap=\"seismic\",\n            vmin=-10.0,\n            vmax=10.0)\n        fig.colorbar(c)\n        ax.set_title(f\"Posterior mean: {outputs[i]}\")\n</pre> from matplotlib import pyplot as plt  # plot model predictions  data = X.data  bounds = generator.vocs.bounds model = generator.train_model(generator.data)  # create mesh n = 50 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  outputs = generator.vocs.output_names with torch.no_grad():     post = model.posterior(pts)      for i in range(len(vocs.output_names)):         mean = post.mean[...,i]         fig, ax = plt.subplots()         ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C1\")         c = ax.pcolor(             xx, yy, mean.squeeze().reshape(n, n),             cmap=\"seismic\",             vmin=-10.0,             vmax=10.0)         fig.colorbar(c)         ax.set_title(f\"Posterior mean: {outputs[i]}\") In\u00a0[7]: Copied! <pre>from xopt.generators.bayesian.objectives import feasibility\n\nacq_func = generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(xx, yy, acq.reshape(n, n))\n    ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C3\")\n\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    feas = feasibility(pts.unsqueeze(1), model, generator.sampler, vocs).flatten()\n\n    fig2, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    ax2.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C3\")\n\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n</pre> from xopt.generators.bayesian.objectives import feasibility  acq_func = generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(xx, yy, acq.reshape(n, n))     ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C3\")      fig.colorbar(c)     ax.set_title(\"Acquisition function\")      feas = feasibility(pts.unsqueeze(1), model, generator.sampler, vocs).flatten()      fig2, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     ax2.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C3\")      fig2.colorbar(c)     ax2.set_title(\"Feasible Region\") In\u00a0[8]: Copied! <pre># print generator model hyperparameters\nfor name, val in X.generator.model.named_parameters():\n    print(f\"{name}:{val}\")\n\nX.generator.model.models[2].covar_module.base_kernel.lengthscale\n</pre> # print generator model hyperparameters for name, val in X.generator.model.named_parameters():     print(f\"{name}:{val}\")  X.generator.model.models[2].covar_module.base_kernel.lengthscale <pre>models.0.likelihood.noise_covar.raw_noise:Parameter containing:\ntensor([-24.6629], dtype=torch.float64, requires_grad=True)\nmodels.0.mean_module.raw_constant:Parameter containing:\ntensor(-0.0872, dtype=torch.float64, requires_grad=True)\nmodels.0.covar_module.raw_outputscale:Parameter containing:\ntensor(3.4111, dtype=torch.float64, requires_grad=True)\nmodels.0.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[0.0620, 0.9200]], dtype=torch.float64, requires_grad=True)\nmodels.1.mean_module.raw_constant:Parameter containing:\ntensor(-0.6278, dtype=torch.float64, requires_grad=True)\nmodels.1.covar_module.raw_outputscale:Parameter containing:\ntensor(0.9223, dtype=torch.float64, requires_grad=True)\nmodels.1.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[-0.2746, -1.3737]], dtype=torch.float64, requires_grad=True)\nmodels.2.mean_module.raw_constant:Parameter containing:\ntensor(1.9551, dtype=torch.float64, requires_grad=True)\nmodels.2.covar_module.raw_outputscale:Parameter containing:\ntensor(2.3625, dtype=torch.float64, requires_grad=True)\nmodels.2.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[ 0.1078, -0.0452]], dtype=torch.float64, requires_grad=True)\n</pre> Out[8]: <pre>tensor([[0.7485, 0.6708]], dtype=torch.float64, grad_fn=&lt;SoftplusBackward0&gt;)</pre> In\u00a0[9]: Copied! <pre>X.vocs.feasibility_data(X.data)\n</pre> X.vocs.feasibility_data(X.data) Out[9]: feasible_c1 feasible_c2 feasible 1 True True True 2 True True True 3 True False False 4 True False False 5 True False False 6 True True True 7 False True False 8 True True True 9 False True False 10 True True True 11 True True True 12 True True True In\u00a0[10]: Copied! <pre># generate next point\nX.generator.generate(1)\n</pre> # generate next point X.generator.generate(1) Out[10]: x1 x2 a 0 0.545051 1.10483 dummy_constant In\u00a0[10]: Copied! <pre>\n</pre>"},{"location":"examples/bayes_exp/bayesian_exploration/#bayesian-exploration","title":"Bayesian Exploration\u00b6","text":"<p>Here we demonstrate the use of Bayesian Exploration to characterize an unknown function in the presence of constraints (see here). The function we wish to explore is the first objective of the TNK test problem.</p>"},{"location":"examples/bayes_exp/bayesian_exploration/#specifiying-generator-options","title":"Specifiying generator options\u00b6","text":"<p>We start with the generator defaults and modify as needed for conservative exploration, which should prevent any constraint violations.</p>"},{"location":"examples/bayes_exp/bayesian_exploration/#run-exploration","title":"Run exploration\u00b6","text":"<p>We start with evaluating 2 points that we know satisfy the constraints. We then run 30 exploration steps.</p>"},{"location":"examples/bayes_exp/bayesian_exploration/#introspect-models","title":"Introspect models\u00b6","text":"<p>During exploration we generate Gaussian Process models of each objective and constraint. We demonstrate how they are viewed below.</p>"},{"location":"examples/bayes_exp/bayesian_exploration/#view-acquisition-function-and-feasibility-prediction","title":"View acquisition function and feasibility prediction\u00b6","text":""},{"location":"examples/bayes_exp/bayesian_exploration/#generator-model-hyperparameters","title":"Generator model hyperparameters\u00b6","text":""},{"location":"examples/bayes_exp/bayesian_exploration/#examine-the-number-of-constraint-violations","title":"Examine the number of constraint violations\u00b6","text":"<p>Using the convience function provided by the vocs object we can evaluate which samples violate either or both of our constraints.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_from_yaml/","title":"Bayesian Exploration from yaml","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom xopt import Xopt\nimport yaml\n\nYAML = \"\"\"\nxopt:\n    dump_file: dump.yaml\ngenerator:\n    name: bayesian_exploration\n    n_initial: 5\n    optim:\n        num_restarts: 1\n    acq:\n        proximal_lengthscales: [1.5, 1.5]\n\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    constants: {a: dummy_constant}\n\n\"\"\"\nyaml_output = yaml.safe_load(YAML)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import torch from xopt import Xopt import yaml  YAML = \"\"\" xopt:     dump_file: dump.yaml generator:     name: bayesian_exploration     n_initial: 5     optim:         num_restarts: 1     acq:         proximal_lengthscales: [1.5, 1.5]  evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     constants: {a: dummy_constant}  \"\"\" yaml_output = yaml.safe_load(YAML) In\u00a0[2]: Copied! <pre>X = Xopt(config=yaml_output)\nX\n</pre> X = Xopt(config=yaml_output) X Out[2]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g0676a82\nData size: 0\nConfig as YAML:\nxopt: {asynch: false, strict: false, dump_file: dump.yaml, max_evaluations: null}\ngenerator:\n  name: bayesian_exploration\n  optim: {num_restarts: 1, raw_samples: 20, sequential: true, max_travel_distances: null,\n    use_turbo: false}\n  acq:\n    proximal_lengthscales: [1.5, 1.5]\n    use_transformed_proximal_weights: true\n    monte_carlo_samples: 128\n  model:\n    name: standard\n    custom_constructor: null\n    use_low_noise_prior: true\n    covar_modules: {}\n    mean_modules: {}\n  n_initial: 5\n  use_cuda: false\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  max_workers: 1\n  function_kwargs: {sleep: 0, random_sleep: 0, raise_probability: 0}\n  vectorized: false\nvocs:\n  variables:\n    x1: [0.0, 3.14159]\n    x2: [0.0, 3.14159]\n  constraints:\n    c1: [GREATER_THAN, 0.0]\n    c2: [LESS_THAN, 0.5]\n  objectives: {y1: MINIMIZE}\n  constants: {a: dummy_constant}\n  linked_variables: {}\n</pre> In\u00a0[3]: Copied! <pre>X.step()\n\nfor i in range(5):\n    print(f\"step {i}\")\n    X.step()\n</pre> X.step()  for i in range(5):     print(f\"step {i}\")     X.step() <pre>step 0\nstep 1\nstep 2\nstep 3\nstep 4\n</pre> In\u00a0[4]: Copied! <pre>X.data\n</pre> X.data Out[4]: x1 x2 a y1 y2 c1 c2 some_array xopt_runtime xopt_error 1 3.069520 0.534574 dummy_constant 3.069520 0.534574 8.800486 6.603629 [1, 2, 3] 0.000056 False 2 2.849233 2.108858 dummy_constant 2.849233 2.108858 11.637207 8.107318 [1, 2, 3] 0.000015 False 3 2.742616 0.831974 dummy_constant 2.742616 0.831974 7.214120 5.139535 [1, 2, 3] 0.000010 False 4 3.011064 1.279510 dummy_constant 3.011064 1.279510 9.604717 6.913080 [1, 2, 3] 0.000014 False 5 0.016193 2.944380 dummy_constant 0.016193 2.944380 7.570021 6.209061 [1, 2, 3] 0.000015 False 6 0.016212 2.944373 dummy_constant 0.016212 2.944373 7.569983 6.209010 [1, 2, 3] 0.000043 False 7 0.014781 2.944657 dummy_constant 0.014781 2.944657 7.571548 6.211787 [1, 2, 3] 0.000079 False 8 0.014797 2.944664 dummy_constant 0.014797 2.944664 7.571588 6.211803 [1, 2, 3] 0.000046 False 9 0.013182 2.946347 dummy_constant 0.013182 2.946347 7.581389 6.221604 [1, 2, 3] 0.000046 False 10 0.011530 2.945289 dummy_constant 0.011530 2.945289 7.575054 6.218039 [1, 2, 3] 0.000044 False In\u00a0[5]: Copied! <pre># plot results\nax = X.data.plot(\"x1\", \"x2\")\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(\"x1\", \"x2\") ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre>from matplotlib import pyplot as plt  # plot model predictions\n\ndata = X.data\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.model\n\n# create mesh\nn = 50\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\noutputs = X.generator.vocs.output_names\nwith torch.no_grad():\n    post = model.posterior(pts)\n\n    for i in range(len(X.vocs.output_names)):\n        mean = post.mean[...,i]\n        fig, ax = plt.subplots()\n        ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C1\")\n        c = ax.pcolor(\n            xx, yy, mean.squeeze().reshape(n, n),\n            cmap=\"seismic\",\n            vmin=-10.0,\n            vmax=10.0)\n        fig.colorbar(c)\n        ax.set_title(f\"Posterior mean: {outputs[i]}\")\n</pre> from matplotlib import pyplot as plt  # plot model predictions  data = X.data  bounds = X.generator.vocs.bounds model = X.generator.model  # create mesh n = 50 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  outputs = X.generator.vocs.output_names with torch.no_grad():     post = model.posterior(pts)      for i in range(len(X.vocs.output_names)):         mean = post.mean[...,i]         fig, ax = plt.subplots()         ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C1\")         c = ax.pcolor(             xx, yy, mean.squeeze().reshape(n, n),             cmap=\"seismic\",             vmin=-10.0,             vmax=10.0)         fig.colorbar(c)         ax.set_title(f\"Posterior mean: {outputs[i]}\") In\u00a0[7]: Copied! <pre>from xopt.generators.bayesian.objectives import feasibility\n\nacq_func = X.generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(xx, yy, acq.reshape(n, n))\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    feas = feasibility(pts.unsqueeze(1), model, X.generator.sampler, X.vocs).flatten()\n\n    fig2, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n</pre> from xopt.generators.bayesian.objectives import feasibility  acq_func = X.generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(xx, yy, acq.reshape(n, n))     fig.colorbar(c)     ax.set_title(\"Acquisition function\")      feas = feasibility(pts.unsqueeze(1), model, X.generator.sampler, X.vocs).flatten()      fig2, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(\"Feasible Region\")"},{"location":"examples/bayes_exp/bayesian_exploration_rosenbrock/","title":"Bayesian Exploration","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport torch\nimport yaml\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import BayesianExplorationGenerator\nfrom xopt.resources.test_functions.rosenbrock import make_rosenbrock_vocs, evaluate_rosenbrock\n\nvocs = make_rosenbrock_vocs(2)\n\ngenerator_options = BayesianExplorationGenerator.default_options()\ngenerator_options.optim.num_restarts = 1\ngenerator_options.acq.proximal_lengthscales = [0.1, 0.1]\n\nevaluator = Evaluator(function=evaluate_rosenbrock)\ngenerator = BayesianExplorationGenerator(vocs, generator_options)\n\nprint(yaml.dump(generator.options.dict()))\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import pandas as pd import torch import yaml from xopt import Xopt, Evaluator from xopt.generators.bayesian import BayesianExplorationGenerator from xopt.resources.test_functions.rosenbrock import make_rosenbrock_vocs, evaluate_rosenbrock  vocs = make_rosenbrock_vocs(2)  generator_options = BayesianExplorationGenerator.default_options() generator_options.optim.num_restarts = 1 generator_options.acq.proximal_lengthscales = [0.1, 0.1]  evaluator = Evaluator(function=evaluate_rosenbrock) generator = BayesianExplorationGenerator(vocs, generator_options)  print(yaml.dump(generator.options.dict())) <pre>acq:\n  monte_carlo_samples: 128\n  proximal_lengthscales:\n  - 0.1\n  - 0.1\n  use_transformed_proximal_weights: true\nmodel:\n  covar_modules: {}\n  custom_constructor: null\n  mean_modules: {}\n  name: standard\n  use_low_noise_prior: true\nn_initial: 3\noptim:\n  max_travel_distances: null\n  num_restarts: 1\n  raw_samples: 20\n  sequential: true\n  use_turbo: false\nuse_cuda: false\n\n</pre> In\u00a0[2]: Copied! <pre>X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\nX.evaluate_data(pd.DataFrame({\"x0\":[1.0, 0.75],\"x1\":[0.75, 1.0]}))\n\nfor i in range(30):\n    print(f\"step {i}\")\n    X.step()\n</pre> X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs) X.evaluate_data(pd.DataFrame({\"x0\":[1.0, 0.75],\"x1\":[0.75, 1.0]}))  for i in range(30):     print(f\"step {i}\")     X.step() <pre>step 0\nstep 1\nstep 2\nstep 3\nstep 4\nstep 5\nstep 6\nstep 7\nstep 8\nstep 9\nstep 10\nstep 11\nstep 12\nstep 13\nstep 14\nstep 15\nstep 16\nstep 17\nstep 18\nstep 19\nstep 20\nstep 21\nstep 22\nstep 23\nstep 24\nstep 25\nstep 26\nstep 27\nstep 28\nstep 29\n</pre> In\u00a0[3]: Copied! <pre># view the data\nX.data\n</pre> # view the data X.data Out[3]: x0 x1 y xopt_runtime xopt_error 1 1.000000 0.750000 6.250000 0.000017 False 2 0.750000 1.000000 19.203125 0.000005 False 3 1.000744 1.274077 7.430480 0.000016 False 4 1.361725 1.283884 32.667702 0.000017 False 5 0.874770 1.446021 46.364159 0.000017 False 6 0.607807 1.448827 116.663738 0.000016 False 7 0.202329 1.441060 196.670690 0.000015 False 8 -0.215179 1.465161 202.792850 0.000017 False 9 -0.384921 1.117078 95.797446 0.000017 False 10 -0.781730 1.166146 33.981961 0.000017 False 11 -1.028284 0.843117 8.704273 0.000017 False 12 -1.397572 0.968162 102.779743 0.000016 False 13 -1.785356 0.851548 553.424137 0.000017 False 14 -2.000000 1.117990 839.597895 0.000018 False 15 -2.000000 0.714678 1088.334345 0.000017 False 16 -1.824933 0.295149 929.242775 0.000033 False 17 -1.480815 0.091700 447.621749 0.000016 False 18 -1.118822 -0.162785 204.583659 0.000016 False 19 -0.717193 -0.309335 70.797126 0.000017 False 20 -0.311313 -0.446152 31.211848 0.000018 False 21 0.088516 -0.585855 36.077545 0.000017 False 22 0.477860 -0.774570 100.857599 0.000027 False 23 0.904302 -0.929262 305.218654 0.000017 False 24 1.355548 -1.004040 807.567880 0.000016 False 25 1.814587 -1.034915 1873.509818 0.000017 False 26 2.000000 -1.384317 2900.087148 0.000019 False 27 1.741583 -1.679554 2221.471431 0.000017 False 28 1.374623 -1.915717 1448.176261 0.000016 False 29 0.958768 -2.000000 852.195188 0.000016 False 30 1.084542 -1.761480 863.022231 0.000017 False 31 0.589390 -1.880469 496.500216 0.000036 False 32 0.177354 -2.000000 413.357457 0.000017 False In\u00a0[4]: Copied! <pre># plot results\nax = X.data.plot(*vocs.variable_names)\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(*vocs.variable_names) ax.set_aspect(\"equal\") In\u00a0[5]: Copied! <pre>from matplotlib import pyplot as plt  # plot model predictions\n\ndata = X.data\n\nbounds = generator.vocs.bounds\nmodel = generator.train_model(generator.data)\n\n# create mesh\nn = 200\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\noutputs = generator.vocs.output_names\nwith torch.no_grad():\n    post = model.posterior(pts)\n\n    mean = post.mean\n    std = torch.sqrt(post.variance)\n\n    fig, ax = plt.subplots()\n    ax.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")\n    c = ax.pcolor(xx, yy, mean.reshape(n, n))\n    fig.colorbar(c)\n    ax.set_title(f\"Posterior mean: {outputs[0]}\")\n\n    fig2, ax2 = plt.subplots()\n    ax2.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")\n    c = ax2.pcolor(xx, yy, std.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(f\"Posterior std: {outputs[0]}\")\n</pre> from matplotlib import pyplot as plt  # plot model predictions  data = X.data  bounds = generator.vocs.bounds model = generator.train_model(generator.data)  # create mesh n = 200 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  outputs = generator.vocs.output_names with torch.no_grad():     post = model.posterior(pts)      mean = post.mean     std = torch.sqrt(post.variance)      fig, ax = plt.subplots()     ax.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")     c = ax.pcolor(xx, yy, mean.reshape(n, n))     fig.colorbar(c)     ax.set_title(f\"Posterior mean: {outputs[0]}\")      fig2, ax2 = plt.subplots()     ax2.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")     c = ax2.pcolor(xx, yy, std.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(f\"Posterior std: {outputs[0]}\") In\u00a0[6]: Copied! <pre>from xopt.generators.bayesian.objectives import feasibility\n\nacq_func = generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(xx, yy, acq.reshape(n, n))\n    ax.plot(*data[vocs.variable_names].to_numpy().T, \"+C3\")\n\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    feas = feasibility(pts.unsqueeze(1), model, generator.sampler, vocs).flatten()\n\n    fig2, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    ax2.plot(*data[vocs.variable_names].to_numpy().T, \"+C3\")\n\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n</pre> from xopt.generators.bayesian.objectives import feasibility  acq_func = generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(xx, yy, acq.reshape(n, n))     ax.plot(*data[vocs.variable_names].to_numpy().T, \"+C3\")      fig.colorbar(c)     ax.set_title(\"Acquisition function\")      feas = feasibility(pts.unsqueeze(1), model, generator.sampler, vocs).flatten()      fig2, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     ax2.plot(*data[vocs.variable_names].to_numpy().T, \"+C3\")      fig2.colorbar(c)     ax2.set_title(\"Feasible Region\") In\u00a0[7]: Copied! <pre># print generator model hyperparameters\nfor name, val in X.generator.model.named_parameters():\n    print(f\"{name}:{val}\")\n</pre> # print generator model hyperparameters for name, val in X.generator.model.named_parameters():     print(f\"{name}:{val}\")  <pre>models.0.likelihood.noise_covar.raw_noise:Parameter containing:\ntensor([-17.0248], dtype=torch.float64, requires_grad=True)\nmodels.0.mean_module.raw_constant:Parameter containing:\ntensor(5.1674, dtype=torch.float64, requires_grad=True)\nmodels.0.covar_module.raw_outputscale:Parameter containing:\ntensor(11.9431, dtype=torch.float64, requires_grad=True)\nmodels.0.covar_module.base_kernel.raw_lengthscale:Parameter containing:\ntensor([[-0.0884,  1.4797]], dtype=torch.float64, requires_grad=True)\n</pre> In\u00a0[8]: Copied! <pre>X.vocs.feasibility_data(X.data)\n</pre> X.vocs.feasibility_data(X.data) Out[8]: feasible 1 True 2 True 3 True 4 True 5 True 6 True 7 True 8 True 9 True 10 True 11 True 12 True 13 True 14 True 15 True 16 True 17 True 18 True 19 True 20 True 21 True 22 True 23 True 24 True 25 True 26 True 27 True 28 True 29 True 30 True 31 True 32 True"},{"location":"examples/bayes_exp/bayesian_exploration_rosenbrock/#bayesian-exploration","title":"Bayesian Exploration\u00b6","text":"<p>Here we demonstrate the use of Bayesian Exploration to characterize an unknown function in the presence of constraints (see here). The function we wish to explore is the first objective of the TNK test problem.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_rosenbrock/#specifiying-generator-options","title":"Specifiying generator options\u00b6","text":"<p>We start with the generator defaults and modify as needed for conservative exploration, which should prevent any constraint violations.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_rosenbrock/#run-exploration","title":"Run exploration\u00b6","text":"<p>We start with evaluating 2 points that we know satisfy the constraints. We then run 30 exploration steps.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_rosenbrock/#introspect-models","title":"Introspect models\u00b6","text":"<p>During exploration we generate Gaussian Process models of each objective and constraint. We demonstrate how they are viewed below.</p>"},{"location":"examples/bayes_exp/bayesian_exploration_rosenbrock/#view-acquisition-function-and-feasibility-prediction","title":"View acquisition function and feasibility prediction\u00b6","text":""},{"location":"examples/bayes_exp/bayesian_exploration_rosenbrock/#generator-model-hyperparameters","title":"Generator model hyperparameters\u00b6","text":""},{"location":"examples/bayes_exp/bayesian_exploration_rosenbrock/#examine-the-number-of-constraint-violations","title":"Examine the number of constraint violations\u00b6","text":"<p>Using the convience function provided by the vocs object we can evaluate which samples violate either or both of our constraints.</p>"},{"location":"examples/bayes_exp/benchmarking/","title":"Normal Model with Standard transforms and no constraints","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom copy import deepcopy\nfrom xopt.generators.bayesian import BayesianExplorationGenerator\nfrom xopt.vocs import VOCS\n\nvocs = VOCS(\n    variables = {\"x\":[0,1]},\n    objectives = {\"y\":\"MAXIMIZE\"},\n    constraints = {\"c\": [\"LESS_THAN\", 0]}\n)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import matplotlib.pyplot as plt import pandas as pd import torch from copy import deepcopy from xopt.generators.bayesian import BayesianExplorationGenerator from xopt.vocs import VOCS  vocs = VOCS(     variables = {\"x\":[0,1]},     objectives = {\"y\":\"MAXIMIZE\"},     constraints = {\"c\": [\"LESS_THAN\", 0]} ) In\u00a0[2]: Copied! <pre># define test functions\ndef y(x):\n    return torch.sin(2*3.14*x)\n\ndef c(x):\n    return torch.cos(2*3.14*x + 0.25)\n\ntest_x = torch.linspace(*torch.tensor(vocs.bounds.flatten()), 100)\n\n# define training data to pass to the generator\ntrain_x = torch.tensor((0.2,0.5, 0.6))\ntrain_y = y(train_x)\ntrain_c = c(train_x)\n\ndata = pd.DataFrame(\n    {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c}\n)\n\ndef plot_ground_truth():\n    fig,ax = plt.subplots()\n    ax.plot(test_x, y(test_x),'--C0')\n    ax.plot(test_x, c(test_x),'--C1')\n    ax.plot(train_x, train_y,'oC0')\n    ax.plot(train_x, train_c,'oC1')\n\n    return ax\nplot_ground_truth()\n</pre> # define test functions def y(x):     return torch.sin(2*3.14*x)  def c(x):     return torch.cos(2*3.14*x + 0.25)  test_x = torch.linspace(*torch.tensor(vocs.bounds.flatten()), 100)  # define training data to pass to the generator train_x = torch.tensor((0.2,0.5, 0.6)) train_y = y(train_x) train_c = c(train_x)  data = pd.DataFrame(     {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c} )  def plot_ground_truth():     fig,ax = plt.subplots()     ax.plot(test_x, y(test_x),'--C0')     ax.plot(test_x, c(test_x),'--C1')     ax.plot(train_x, train_y,'oC0')     ax.plot(train_x, train_c,'oC1')      return ax plot_ground_truth() Out[2]: <pre>&lt;Axes: &gt;</pre> In\u00a0[3]: Copied! <pre># plot the generator model and acquisition function\nfrom xopt.utils import visualize_model\ngenerator = BayesianExplorationGenerator(deepcopy(vocs), BayesianExplorationGenerator.default_options())\ngenerator.vocs.constraints = {}\n\nvisualize_model(generator, data)\n</pre> # plot the generator model and acquisition function from xopt.utils import visualize_model generator = BayesianExplorationGenerator(deepcopy(vocs), BayesianExplorationGenerator.default_options()) generator.vocs.constraints = {}  visualize_model(generator, data)  In\u00a0[4]: Copied! <pre># plot the generator model and acquisition function\ngenerator = BayesianExplorationGenerator(deepcopy(vocs), BayesianExplorationGenerator.default_options())\ngenerator.vocs.constraints = {\"c\": [\"LESS_THAN\", 0]}\n\nvisualize_model(generator, data)\n</pre> # plot the generator model and acquisition function generator = BayesianExplorationGenerator(deepcopy(vocs), BayesianExplorationGenerator.default_options()) generator.vocs.constraints = {\"c\": [\"LESS_THAN\", 0]}  visualize_model(generator, data)"},{"location":"examples/bayes_exp/benchmarking/#normal-model-with-standard-transforms-and-no-constraints","title":"Normal Model with Standard transforms and no constraints\u00b6","text":"<ul> <li>acquisition function is UCB with beta = 2</li> </ul>"},{"location":"examples/bayes_exp/benchmarking/#normal-model-with-standard-transforms-and-constraints","title":"Normal Model with Standard transforms and constraints\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/","title":"Xopt CNSGA algorithm","text":"In\u00a0[1]: Copied! <pre>from xopt.generators.ga.cnsga import CNSGAGenerator\n\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\nfrom xopt import Xopt, Evaluator\n\nimport pandas as pd\n</pre> from xopt.generators.ga.cnsga import CNSGAGenerator  from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs  from xopt import Xopt, Evaluator  import pandas as pd In\u00a0[2]: Copied! <pre># Useful for debugging\n#%load_ext autoreload\n#%autoreload 2\n</pre> # Useful for debugging #%load_ext autoreload #%autoreload 2 In\u00a0[3]: Copied! <pre>ev = Evaluator(function=evaluate_TNK)\nev.function_kwargs = {'raise_probability':0.1} # optional random crashing, to mimic real-world use.\n</pre> ev = Evaluator(function=evaluate_TNK) ev.function_kwargs = {'raise_probability':0.1} # optional random crashing, to mimic real-world use.  In\u00a0[4]: Copied! <pre>X = Xopt(\n    generator=CNSGAGenerator(tnk_vocs),\n    evaluator=ev,\n    vocs=tnk_vocs,\n)\n</pre> X = Xopt(     generator=CNSGAGenerator(tnk_vocs),     evaluator=ev,     vocs=tnk_vocs, ) <p>Run 100 generations</p> In\u00a0[5]: Copied! <pre>%%time\nfor _ in range(64 * 20):\n    X.step()\n</pre> %%time for _ in range(64 * 20):     X.step() <pre>CPU times: user 6.29 s, sys: 3.59 ms, total: 6.29 s\nWall time: 6.29 s\n</pre> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n</pre> import matplotlib.pyplot as plt  %matplotlib inline %config InlineBackend.figure_format = 'retina' In\u00a0[7]: Copied! <pre>def plot_population(X):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    \n    fdata = tnk_vocs.feasibility_data(X.data)\n    \n    k1 = \"x1\"\n    k2 = \"x2\"\n    \n    X.data.plot.scatter(k1, k2, marker=\".\", alpha=0.1, color=\"black\", ax=ax)\n    X.data[fdata[\"feasible\"]].plot.scatter(\n        k1, k2, marker=\"x\", alpha=0.3, color=\"orange\", ax=ax\n    )\n    X.generator.population.plot.scatter(k1, k2, marker=\"o\", color=\"red\", alpha=1, ax=ax)\n    ax.set_xlabel(k1)\n    ax.set_ylabel(k2)\n    ax.set_xlim(0, 1.5)\n    ax.set_ylim(0, 1.5)\n    ax.set_title(\"TNK with Xopt's CNSGA\")\n</pre> def plot_population(X):     fig, ax = plt.subplots(figsize=(8, 8))          fdata = tnk_vocs.feasibility_data(X.data)          k1 = \"x1\"     k2 = \"x2\"          X.data.plot.scatter(k1, k2, marker=\".\", alpha=0.1, color=\"black\", ax=ax)     X.data[fdata[\"feasible\"]].plot.scatter(         k1, k2, marker=\"x\", alpha=0.3, color=\"orange\", ax=ax     )     X.generator.population.plot.scatter(k1, k2, marker=\"o\", color=\"red\", alpha=1, ax=ax)     ax.set_xlabel(k1)     ax.set_ylabel(k2)     ax.set_xlim(0, 1.5)     ax.set_ylim(0, 1.5)     ax.set_title(\"TNK with Xopt's CNSGA\") In\u00a0[8]: Copied! <pre>plot_population(X)\n</pre> plot_population(X) <p>Write the current population</p> In\u00a0[9]: Copied! <pre>X.generator.write_population('test.csv')\n</pre> X.generator.write_population('test.csv') In\u00a0[10]: Copied! <pre>from xopt import Xopt\n</pre> from xopt import Xopt In\u00a0[11]: Copied! <pre>YAML = \"\"\"\nxopt:\n    max_evaluations: 6400\ngenerator:\n    name: cnsga\n    population_size: 64\n    population_file: test.csv\n    output_path: .\n\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n    function_kwargs:\n      raise_probability: 0.1\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    linked_variables: {x9: x1}\n    constants: {a: dummy_constant}\n\n\"\"\"\n\nX = Xopt(YAML)\nX\n</pre> YAML = \"\"\" xopt:     max_evaluations: 6400 generator:     name: cnsga     population_size: 64     population_file: test.csv     output_path: .  evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK     function_kwargs:       raise_probability: 0.1  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     linked_variables: {x9: x1}     constants: {a: dummy_constant}  \"\"\"  X = Xopt(YAML) X Out[11]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g0676a82\nData size: 0\nConfig as YAML:\nxopt: {asynch: false, strict: false, dump_file: null, max_evaluations: 6400}\ngenerator: {name: cnsga, population_size: 64, crossover_probability: 0.9, mutation_probability: 1.0,\n  population_file: test.csv, output_path: .}\nevaluator:\n  function: xopt.resources.test_functions.tnk.evaluate_TNK\n  max_workers: 1\n  function_kwargs: {sleep: 0, random_sleep: 0, raise_probability: 0.1}\n  vectorized: false\nvocs:\n  variables:\n    x1: [0.0, 3.14159]\n    x2: [0.0, 3.14159]\n  constraints:\n    c1: [GREATER_THAN, 0.0]\n    c2: [LESS_THAN, 0.5]\n  objectives: {y1: MINIMIZE, y2: MINIMIZE}\n  constants: {a: dummy_constant}\n  linked_variables: {x9: x1}\n</pre> <p>This will have loaded children from the population file. These will need to be re-evaluated.</p> In\u00a0[12]: Copied! <pre>len(X.generator.children)\n</pre> len(X.generator.children) Out[12]: <pre>64</pre> In\u00a0[13]: Copied! <pre>%%time\nX.run()\n</pre> %%time X.run() <pre>CPU times: user 32.8 s, sys: 95 ms, total: 32.9 s\nWall time: 32.9 s\n</pre> In\u00a0[14]: Copied! <pre>plot_population(X)\n</pre> plot_population(X) In\u00a0[15]: Copied! <pre>len(X.data)\n</pre> len(X.data) Out[15]: <pre>6400</pre> <p>Setting <code>output_path</code> will write .csv files for each population, as well as the offspring considered in each generation</p> In\u00a0[16]: Copied! <pre>from glob import glob\npop_files = sorted(glob(\"cnsga_population*\"))\npop_files[:10]\n</pre> from glob import glob pop_files = sorted(glob(\"cnsga_population*\")) pop_files[:10] Out[16]: <pre>['cnsga_population_2023-05-01T17:17:13.615770+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:13.944946+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:14.279694+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:14.601094+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:14.929897+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:15.246599+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:15.576623+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:15.892562+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:16.223398+00:00.csv',\n 'cnsga_population_2023-05-01T17:17:16.539200+00:00.csv']</pre> In\u00a0[17]: Copied! <pre>offspring_files = sorted(glob(\"cnsga_offspring*\"))\noffspring_files[0:10]\n</pre> offspring_files = sorted(glob(\"cnsga_offspring*\")) offspring_files[0:10] Out[17]: <pre>['cnsga_offspring_2023-05-01T17:17:13.611062+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:13.940125+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:14.274311+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:14.596283+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:14.921894+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:15.242154+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:15.571758+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:15.887168+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:16.218790+00:00.csv',\n 'cnsga_offspring_2023-05-01T17:17:16.534202+00:00.csv']</pre> In\u00a0[18]: Copied! <pre>from xopt.utils import read_xopt_csv\npop_df = read_xopt_csv(pop_files[-1])\npop_df.plot.scatter(\"x1\", \"x2\", marker=\"o\", color=\"red\", alpha=1)\n</pre> from xopt.utils import read_xopt_csv pop_df = read_xopt_csv(pop_files[-1]) pop_df.plot.scatter(\"x1\", \"x2\", marker=\"o\", color=\"red\", alpha=1) Out[18]: <pre>&lt;Axes: xlabel='x1', ylabel='x2'&gt;</pre> <p>Similarly, offsrping files can be loaded. This will load the last few:</p> In\u00a0[19]: Copied! <pre>offspring_df = read_xopt_csv(*offspring_files[-10:])\noffspring_df.plot.scatter(\"x1\", \"x2\", marker=\".\", color=\"black\", alpha=.1)\n</pre> offspring_df = read_xopt_csv(*offspring_files[-10:]) offspring_df.plot.scatter(\"x1\", \"x2\", marker=\".\", color=\"black\", alpha=.1) Out[19]: <pre>&lt;Axes: xlabel='x1', ylabel='x2'&gt;</pre> <p>Occationally there are duplicates in offspring</p> In\u00a0[20]: Copied! <pre>all_offspring = read_xopt_csv(*offspring_files) \nlen(all_offspring), len(all_offspring.drop_duplicates())\n</pre> all_offspring = read_xopt_csv(*offspring_files)  len(all_offspring), len(all_offspring.drop_duplicates()) Out[20]: <pre>(6400, 6396)</pre> In\u00a0[21]: Copied! <pre># Cleanup\n!rm cnsga_population*\n!rm cnsga_offspring*\n!rm test.csv\n</pre> # Cleanup !rm cnsga_population* !rm cnsga_offspring* !rm test.csv In\u00a0[22]: Copied! <pre>df = pd.DataFrame(X.generator.generate(1000))\n\nfig, ax = plt.subplots()\ndf.plot.scatter(\"x1\", \"x2\", marker=\".\", color=\"green\", alpha=0.5, ax=ax, label='candidates')\npop_df.plot.scatter(\"x1\", \"x2\", marker=\"o\", color=\"red\", alpha=1, ax=ax, label='population')\nplt.legend()\n</pre> df = pd.DataFrame(X.generator.generate(1000))  fig, ax = plt.subplots() df.plot.scatter(\"x1\", \"x2\", marker=\".\", color=\"green\", alpha=0.5, ax=ax, label='candidates') pop_df.plot.scatter(\"x1\", \"x2\", marker=\"o\", color=\"red\", alpha=1, ax=ax, label='population') plt.legend() Out[22]: <pre>&lt;matplotlib.legend.Legend at 0x7f1d4c81c700&gt;</pre> In\u00a0[23]: Copied! <pre># Notice that this returns `some_array`\nevaluate_TNK({'x1':1, 'x2':1})\n</pre> # Notice that this returns `some_array` evaluate_TNK({'x1':1, 'x2':1}) Out[23]: <pre>{'y1': 1, 'y2': 1, 'c1': 0.9, 'c2': 0.5, 'some_array': array([1, 2, 3])}</pre> In\u00a0[24]: Copied! <pre># Here we make a version that does not have this\ndef evaluate_TNK2(*args, **kwargs):\n    outputs = evaluate_TNK(*args, **kwargs)\n    outputs.pop('some_array')\n    return outputs\n</pre> # Here we make a version that does not have this def evaluate_TNK2(*args, **kwargs):     outputs = evaluate_TNK(*args, **kwargs)     outputs.pop('some_array')     return outputs In\u00a0[25]: Copied! <pre>from xopt import Xopt\n\nYAML = \"\"\"\nxopt:\n    max_evaluations: 6400\ngenerator:\n    name: cnsga\n    population_size: 64\n\nevaluator:\n    function: __main__.evaluate_TNK2\n    function_kwargs:\n      raise_probability: 0.1\n    vectorized: True\n    max_workers: 100 \n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    linked_variables: {x9: x1}\n    constants: {a: dummy_constant}\n\n\"\"\"\n\n\nX2 = Xopt(YAML)\nX2.evaluator.function = evaluate_TNK2\n\nX2.run()\n\nlen(X2.data)\n</pre> from xopt import Xopt  YAML = \"\"\" xopt:     max_evaluations: 6400 generator:     name: cnsga     population_size: 64  evaluator:     function: __main__.evaluate_TNK2     function_kwargs:       raise_probability: 0.1     vectorized: True     max_workers: 100   vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     linked_variables: {x9: x1}     constants: {a: dummy_constant}  \"\"\"   X2 = Xopt(YAML) X2.evaluator.function = evaluate_TNK2  X2.run()  len(X2.data) Out[25]: <pre>6400</pre> In\u00a0[26]: Copied! <pre>plot_population(X)\n</pre> plot_population(X) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/cnsga/cnsga_tnk/#xopt-cnsga-algorithm","title":"Xopt CNSGA algorithm\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/#plot","title":"Plot\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/#yaml-method","title":"YAML method\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/#examine-generator","title":"Examine generator\u00b6","text":""},{"location":"examples/cnsga/cnsga_tnk/#vectorized-evaluation","title":"Vectorized evaluation\u00b6","text":"<p>Some functions also allow vectorized inputs. This can often be very fast.</p> <p>However, vectorized evaluation has some restrictions. For example, the output dict cannot append additional arrays with odd lengths.</p>"},{"location":"examples/developer/algorithm_to_generator/","title":"Convering an algorithm to a generator","text":"In\u00a0[1]: Copied! <pre>def f(x):\n    print(f'evaluate f({x})')\n    return 2*x\n</pre> def f(x):     print(f'evaluate f({x})')     return 2*x In\u00a0[2]: Copied! <pre>def algorithm(func):\n    \n    x = 0\n    y = func(x)\n    \n    i = 0\n    while i &lt; 5:\n        i += 1\n        \n        x += i\n        y = func(x)\n        print(x, y)\n\nalgorithm(f)\n</pre> def algorithm(func):          x = 0     y = func(x)          i = 0     while i &lt; 5:         i += 1                  x += i         y = func(x)         print(x, y)  algorithm(f)         <pre>evaluate f(0)\nevaluate f(1)\n1 2\nevaluate f(3)\n3 6\nevaluate f(6)\n6 12\nevaluate f(10)\n10 20\nevaluate f(15)\n15 30\n</pre> In\u00a0[3]: Copied! <pre>def algorithm2(func):\n    x = 0\n    yield x\n    y = func(x)\n    \n    i = 0\n    while i &lt; 5:\n        i += 1\n        \n        x += i\n        yield x\n        y = func(x)\n        print(x, y)\n</pre> def algorithm2(func):     x = 0     yield x     y = func(x)          i = 0     while i &lt; 5:         i += 1                  x += i         yield x         y = func(x)         print(x, y)   In\u00a0[4]: Copied! <pre>class Generator:\n    def __init__(self):\n        self.alg = algorithm2(lambda x: self.data[x]) \n        self.data = {}\n        \n    def generate(self):\n        x =  next(self.alg)    \n        while x in self.data:\n            x = next(self.alg)    \n        return x\n    \n    def add_data(self, x, y):\n        self.data[x] = y\n</pre> class Generator:     def __init__(self):         self.alg = algorithm2(lambda x: self.data[x])          self.data = {}              def generate(self):         x =  next(self.alg)             while x in self.data:             x = next(self.alg)             return x          def add_data(self, x, y):         self.data[x] = y In\u00a0[5]: Copied! <pre>G = Generator()\n\nfor step in range(10):\n    print(f'--- step {step +1} ---')\n    try:\n        x = G.generate()\n        y = f(x) # actual call to f\n        G.add_data(x, y)\n    except StopIteration:\n        G.alg.close() # Clean up\n        break\n</pre> G = Generator()  for step in range(10):     print(f'--- step {step +1} ---')     try:         x = G.generate()         y = f(x) # actual call to f         G.add_data(x, y)     except StopIteration:         G.alg.close() # Clean up         break      <pre>--- step 1 ---\nevaluate f(0)\n--- step 2 ---\nevaluate f(1)\n--- step 3 ---\n1 2\nevaluate f(3)\n--- step 4 ---\n3 6\nevaluate f(6)\n--- step 5 ---\n6 12\nevaluate f(10)\n--- step 6 ---\n10 20\nevaluate f(15)\n--- step 7 ---\n15 30\n</pre>"},{"location":"examples/developer/algorithm_to_generator/#convering-an-algorithm-to-a-generator","title":"Convering an algorithm to a generator\u00b6","text":""},{"location":"examples/developer/algorithm_to_generator/#some-test-function","title":"Some test function\u00b6","text":""},{"location":"examples/developer/algorithm_to_generator/#algorithm-functional-form","title":"algorithm functional form\u00b6","text":""},{"location":"examples/developer/algorithm_to_generator/#generator-version","title":"Generator version\u00b6","text":"<p>This is a copy-paste of <code>algorithm</code>, but with <code>yield x</code> inserted before every call to <code>func(x)</code>.</p> <p>This allows for stepping through the algorithm, controlling the function evaluations separately.</p> <p>TODO: stopping criteria</p>"},{"location":"examples/developer/evaluator_devel/","title":"JSON encoding and decoding callable functions","text":"In\u00a0[1]: Copied! <pre>from pydantic import BaseModel, BaseSettings, root_validator, validator\n\nclass Config(BaseSettings):\n    option1: str\n    option2: int\n\n\nclass Evaluator(BaseModel):\n\n    config: Config\n\n    @root_validator(pre=True)\n    def validate_config(cls, values):\n\n        # check if config is instance of Config\n        if values.get(\"config\"):\n            return values\n\n        else: \n            return {\"config\": Config(**values)}\n</pre> from pydantic import BaseModel, BaseSettings, root_validator, validator  class Config(BaseSettings):     option1: str     option2: int   class Evaluator(BaseModel):      config: Config      @root_validator(pre=True)     def validate_config(cls, values):          # check if config is instance of Config         if values.get(\"config\"):             return values          else:              return {\"config\": Config(**values)}        In\u00a0[2]: Copied! <pre>evaluator = Evaluator(option1=\"hi\", option2=3)\nevaluator\n</pre> evaluator = Evaluator(option1=\"hi\", option2=3) evaluator Out[2]: <pre>Evaluator(config=Config(option1='hi', option2=3))</pre> In\u00a0[3]: Copied! <pre>config = Config(option1=\"hi\", option2=3)\nconfig\n</pre> config = Config(option1=\"hi\", option2=3) config Out[3]: <pre>Config(option1='hi', option2=3)</pre> In\u00a0[4]: Copied! <pre>evaluator_from_config = Evaluator(config=config)\nevaluator_from_config\n</pre> evaluator_from_config = Evaluator(config=config) evaluator_from_config Out[4]: <pre>Evaluator(config=Config(option1='hi', option2=3))</pre> In\u00a0[5]: Copied! <pre>import json\nfrom typing import Callable\nfrom types import FunctionType, MethodType\n\nfrom pydantic import Extra\n\nfrom xopt.utils import get_function\n</pre> import json from typing import Callable from types import FunctionType, MethodType  from pydantic import Extra  from xopt.utils import get_function  In\u00a0[6]: Copied! <pre>JSON_ENCODERS = {\n    FunctionType: lambda x: f\"{x.__module__}.{x.__qualname__}\",\n    Callable: lambda x: f\"{x.__module__}.{type(x).__qualname__}\",\n}\n\nfrom typing import Any, Callable, Dict, Generic, Iterable, Optional, TypeVar, Tuple\nObjType = TypeVar(\"ObjType\")\nJSON_ENCODERS = {\n    # function/method type distinguished for class members and not recognized as callables\n    FunctionType: lambda x: f\"{x.__module__}.{x.__qualname__}\",\n    MethodType: lambda x: f\"{x.__module__}.{x.__qualname__}\",\n    Callable: lambda x: f\"{x.__module__}.{type(x).__qualname__}\",\n    type: lambda x: f\"{x.__module__}.{x.__name__}\",\n    # for encoding instances of the ObjType}\n    ObjType: lambda x: f\"{x.__module__}.{x.__class__.__qualname__}\",\n}\n</pre> JSON_ENCODERS = {     FunctionType: lambda x: f\"{x.__module__}.{x.__qualname__}\",     Callable: lambda x: f\"{x.__module__}.{type(x).__qualname__}\", }  from typing import Any, Callable, Dict, Generic, Iterable, Optional, TypeVar, Tuple ObjType = TypeVar(\"ObjType\") JSON_ENCODERS = {     # function/method type distinguished for class members and not recognized as callables     FunctionType: lambda x: f\"{x.__module__}.{x.__qualname__}\",     MethodType: lambda x: f\"{x.__module__}.{x.__qualname__}\",     Callable: lambda x: f\"{x.__module__}.{type(x).__qualname__}\",     type: lambda x: f\"{x.__module__}.{x.__name__}\",     # for encoding instances of the ObjType}     ObjType: lambda x: f\"{x.__module__}.{x.__class__.__qualname__}\", }  In\u00a0[7]: Copied! <pre>class CallableModel(BaseModel):\n    callable: Callable\n\n    class Config:\n        arbitrary_types_allowed = True\n        json_encoders = JSON_ENCODERS\n        extra = 'forbid'\n\n    @root_validator(pre=True)\n    def validate_all(cls, values):\n\n        callable = values.pop(\"callable\")\n        if not isinstance(\n            callable,\n            (\n                str,\n                Callable,\n            ),\n        ):\n            raise ValueError(\n                \"Callable must be object or a string. Provided %s\", type(callable)\n            )\n\n        values[\"callable\"] = get_function(callable)\n\n\n\n        return values\n</pre>  class CallableModel(BaseModel):     callable: Callable      class Config:         arbitrary_types_allowed = True         json_encoders = JSON_ENCODERS         extra = 'forbid'      @root_validator(pre=True)     def validate_all(cls, values):          callable = values.pop(\"callable\")         if not isinstance(             callable,             (                 str,                 Callable,             ),         ):             raise ValueError(                 \"Callable must be object or a string. Provided %s\", type(callable)             )          values[\"callable\"] = get_function(callable)            return values In\u00a0[8]: Copied! <pre>def f(x):\n    return 2*x\n</pre> def f(x):     return 2*x In\u00a0[9]: Copied! <pre>m = CallableModel(callable=f)\nm.callable(3)\n</pre> m = CallableModel(callable=f) m.callable(3) Out[9]: <pre>6</pre> In\u00a0[10]: Copied! <pre>m.json()\n</pre> m.json() Out[10]: <pre>'{\"callable\": \"__main__.f\"}'</pre> In\u00a0[11]: Copied! <pre>m2 = CallableModel(**json.loads(m.json()))\nm2.callable(345)\n</pre> m2 = CallableModel(**json.loads(m.json())) m2.callable(345) Out[11]: <pre>690</pre> In\u00a0[12]: Copied! <pre>def f():\n    pass\n\n# or this\nf = lambda x: 2*x\n\ntype(f) is FunctionType\n</pre> def f():     pass  # or this f = lambda x: 2*x  type(f) is FunctionType Out[12]: <pre>True</pre> In\u00a0[13]: Copied! <pre>import json\nfrom typing import Callable\nfrom types import FunctionType, MethodType\n\nfrom pydantic import Extra, Field\n\nfrom xopt.pydantic import NormalExecutor\nfrom xopt.utils import get_function, get_function_defaults\n</pre> import json from typing import Callable from types import FunctionType, MethodType  from pydantic import Extra, Field  from xopt.pydantic import NormalExecutor from xopt.utils import get_function, get_function_defaults In\u00a0[14]: Copied! <pre>from concurrent.futures import ProcessPoolExecutor\nfrom xopt.evaluator import DummyExecutor\n\nJSON_ENCODERS = {\n    FunctionType: lambda x: f\"{x.__module__}.{x.__qualname__}\",\n    Callable: lambda x: f\"{x.__module__}.{type(x).__qualname__}\",\n}\n\nclass Evaluator(BaseModel):\n    function: Callable\n    max_workers: int = 1\n    executor: NormalExecutor = Field(exclude=True)\n    function_kwargs: dict = {}\n    \n\n    class Config:\n        arbitrary_types_allowed = True\n        # validate_assignment = True # Broken in 1.9.0. Trying to fix in https://github.com/samuelcolvin/pydantic/pull/4194\n        json_encoders = JSON_ENCODERS\n        extra = 'forbid'\n\n    @root_validator(pre=True)\n    def validate_all(cls, values):\n   \n        f = get_function(values[\"function\"])\n        kwargs = values.get(\"function_kwargs\", {})\n        kwargs = {**get_function_defaults(f), **kwargs}\n        values[\"function\"] = f\n        values[\"function_kwargs\"] = kwargs\n\n        max_workers = values.pop(\"max_workers\", 1)\n\n        executor = values.pop(\"executor\", None)\n        if not executor:\n            if max_workers &gt; 1:\n                executor = ProcessPoolExecutor(max_workers=max_workers)\n            else: \n                executor = DummyExecutor()\n\n        # Cast as a NormalExecutor\n        values[\"executor\"] =  NormalExecutor[type(executor)](executor=executor)\n        values[\"max_workers\"] = max_workers\n        \n        return values    \n\n\n\ndef g(a, b=2):\n    return a*b\n\nev = Evaluator(function=g, function_kwargs={'b':3}, max_workers=2, executor=None)\nev.executor = None\nev.json()\n</pre> from concurrent.futures import ProcessPoolExecutor from xopt.evaluator import DummyExecutor  JSON_ENCODERS = {     FunctionType: lambda x: f\"{x.__module__}.{x.__qualname__}\",     Callable: lambda x: f\"{x.__module__}.{type(x).__qualname__}\", }  class Evaluator(BaseModel):     function: Callable     max_workers: int = 1     executor: NormalExecutor = Field(exclude=True)     function_kwargs: dict = {}           class Config:         arbitrary_types_allowed = True         # validate_assignment = True # Broken in 1.9.0. Trying to fix in https://github.com/samuelcolvin/pydantic/pull/4194         json_encoders = JSON_ENCODERS         extra = 'forbid'      @root_validator(pre=True)     def validate_all(cls, values):             f = get_function(values[\"function\"])         kwargs = values.get(\"function_kwargs\", {})         kwargs = {**get_function_defaults(f), **kwargs}         values[\"function\"] = f         values[\"function_kwargs\"] = kwargs          max_workers = values.pop(\"max_workers\", 1)          executor = values.pop(\"executor\", None)         if not executor:             if max_workers &gt; 1:                 executor = ProcessPoolExecutor(max_workers=max_workers)             else:                  executor = DummyExecutor()          # Cast as a NormalExecutor         values[\"executor\"] =  NormalExecutor[type(executor)](executor=executor)         values[\"max_workers\"] = max_workers                  return values        def g(a, b=2):     return a*b  ev = Evaluator(function=g, function_kwargs={'b':3}, max_workers=2, executor=None) ev.executor = None ev.json() Out[14]: <pre>'{\"function\": \"__main__.g\", \"max_workers\": 2, \"function_kwargs\": {\"b\": 3}}'</pre> In\u00a0[15]: Copied! <pre>ev.executor\n</pre> ev.executor  In\u00a0[16]: Copied! <pre>ev.max_workers = 1\nev.executor=None\n</pre> ev.max_workers = 1 ev.executor=None In\u00a0[17]: Copied! <pre>ev.json()\n</pre> ev.json() Out[17]: <pre>'{\"function\": \"__main__.g\", \"max_workers\": 1, \"function_kwargs\": {\"b\": 3}}'</pre> In\u00a0[18]: Copied! <pre>from concurrent.futures import ThreadPoolExecutor\n</pre> from concurrent.futures import ThreadPoolExecutor In\u00a0[19]: Copied! <pre>with ThreadPoolExecutor() as executor:\n    #print(dir(executor))\n    print(type(executor))\n</pre> with ThreadPoolExecutor() as executor:     #print(dir(executor))     print(type(executor)) <pre>&lt;class 'concurrent.futures.thread.ThreadPoolExecutor'&gt;\n</pre> In\u00a0[20]: Copied! <pre>from xopt.pydantic import NormalExecutor\n</pre> from xopt.pydantic import NormalExecutor In\u00a0[21]: Copied! <pre>type(executor)\n</pre> type(executor) Out[21]: <pre>concurrent.futures.thread.ThreadPoolExecutor</pre> In\u00a0[22]: Copied! <pre>NormalExecutor[type(executor)](executor=executor)\n</pre> NormalExecutor[type(executor)](executor=executor) Out[22]: <pre>NormalExecutor[ThreadPoolExecutor](loader=ObjLoader[ThreadPoolExecutor](object=None, loader=CallableModel(callable=&lt;class 'concurrent.futures.thread.ThreadPoolExecutor'&gt;, signature=Kwargs_ThreadPoolExecutor(args=[], max_workers=None, initializer=None, initargs=None, kwarg_order=['max_workers', 'thread_name_prefix', 'initializer', 'initargs'], thread_name_prefix='')), object_type=&lt;class 'concurrent.futures.thread.ThreadPoolExecutor'&gt;), executor_type=&lt;class 'concurrent.futures.thread.ThreadPoolExecutor'&gt;, submit_callable='submit', map_callable='map', shutdown_callable='shutdown', executor=&lt;concurrent.futures.thread.ThreadPoolExecutor object at 0x7f29a11d3e80&gt;)</pre>"},{"location":"examples/developer/evaluator_devel/#init-methods","title":"Init methods\u00b6","text":""},{"location":"examples/developer/evaluator_devel/#json-encoding-and-decoding-callable-functions","title":"JSON encoding and decoding callable functions\u00b6","text":""},{"location":"examples/developer/evaluator_devel/#evaluator","title":"Evaluator\u00b6","text":""},{"location":"examples/developer/evaluator_devel/#executors","title":"Executors\u00b6","text":""},{"location":"examples/developer/executors/","title":"Pydantic-based executors","text":"In\u00a0[1]: Copied! <pre># imports\nimport contextlib\nimport copy\nimport json\nimport pickle\nimport inspect\nimport logging\nfrom concurrent.futures import Future, ThreadPoolExecutor\nfrom typing import Any, Callable, Dict, Generic, Iterable, Optional, TypeVar, Tuple\nfrom types import FunctionType, MethodType\nfrom pydantic import BaseModel, Field, root_validator, validate_arguments, validator, ValidationError, Extra\nfrom pydantic.generics import GenericModel\n\n\n\nfrom xopt.pydantic import validate_and_compose_signature\n\n\nlogger = logging.getLogger(\"__name__\")\n\n# Print code\nfrom IPython.display import display, Markdown\ndef sdisplay(obj):\n    spec = inspect.getsource(obj)\n    display(Markdown(f\"```python \\n {spec} \\n ```\"))\n</pre> # imports import contextlib import copy import json import pickle import inspect import logging from concurrent.futures import Future, ThreadPoolExecutor from typing import Any, Callable, Dict, Generic, Iterable, Optional, TypeVar, Tuple from types import FunctionType, MethodType from pydantic import BaseModel, Field, root_validator, validate_arguments, validator, ValidationError, Extra from pydantic.generics import GenericModel    from xopt.pydantic import validate_and_compose_signature   logger = logging.getLogger(\"__name__\")  # Print code from IPython.display import display, Markdown def sdisplay(obj):     spec = inspect.getsource(obj)     display(Markdown(f\"```python \\n {spec} \\n ```\"))  In\u00a0[2]: Copied! <pre>ObjType = TypeVar(\"ObjType\")\n</pre> ObjType = TypeVar(\"ObjType\") In\u00a0[3]: Copied! <pre>JSON_ENCODERS = {\n    # function/method type distinguished for class members and not recognized as callables\n    FunctionType: lambda x: f\"{x.__module__}.{x.__qualname__}\",\n    MethodType: lambda x: f\"{x.__module__}.{x.__qualname__}\",\n    Callable: lambda x: f\"{x.__module__}.{type(x).__qualname__}\",\n    type: lambda x: f\"{x.__module__}.{x.__name__}\",\n    # for encoding instances of the ObjType}\n    ObjType: lambda x: f\"{x.__module__}.{x.__class__.__qualname__}\",\n}\n</pre> JSON_ENCODERS = {     # function/method type distinguished for class members and not recognized as callables     FunctionType: lambda x: f\"{x.__module__}.{x.__qualname__}\",     MethodType: lambda x: f\"{x.__module__}.{x.__qualname__}\",     Callable: lambda x: f\"{x.__module__}.{type(x).__qualname__}\",     type: lambda x: f\"{x.__module__}.{x.__name__}\",     # for encoding instances of the ObjType}     ObjType: lambda x: f\"{x.__module__}.{x.__class__.__qualname__}\", } In\u00a0[4]: Copied! <pre>from xopt.pydantic import get_callable_from_string\n\n#sdisplay(get_callable_from_string)\n</pre> from xopt.pydantic import get_callable_from_string  #sdisplay(get_callable_from_string) In\u00a0[5]: Copied! <pre>def test_fn(x, y=4, *args, m, **kwargs):\n    return x\n\nvalidate_and_compose_signature(test_fn, y=5, x=2, hi=4)\n</pre> def test_fn(x, y=4, *args, m, **kwargs):     return x  validate_and_compose_signature(test_fn, y=5, x=2, hi=4) Out[5]: <pre>Kwargs_test_fn(args=[], m=&lt;class 'inspect._empty'&gt;, kwarg_order=['x', 'y', 'm'], x=2, y=5)</pre> In\u00a0[6]: Copied! <pre>from xopt.pydantic import CallableModel\n</pre> from xopt.pydantic import CallableModel <p>Let's test the callables on example function and class:</p> In\u00a0[7]: Copied! <pre>def test_function(x: int, y: int = 5):\n    return x + y\n\n\nclass TestClass:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n</pre> def test_function(x: int, y: int = 5):     return x + y   class TestClass:     def __init__(self, x, y):         self.x = x         self.y = y In\u00a0[8]: Copied! <pre>fn = CallableModel(callable=test_function, kwargs={\"x\":1, \"y\":3})\n\nfn.signature\n</pre> fn = CallableModel(callable=test_function, kwargs={\"x\":1, \"y\":3})  fn.signature Out[8]: <pre>Kwargs_test_function(args=[], kwarg_order=['x', 'y'], x=1, y=3)</pre> In\u00a0[9]: Copied! <pre>fn = CallableModel(callable=test_function, args=(1,3,))\n\nfn.signature\n</pre> fn = CallableModel(callable=test_function, args=(1,3,))  fn.signature Out[9]: <pre>Kwargs_test_function(args=[], kwarg_order=['x', 'y'], x=1, y=3)</pre> In\u00a0[10]: Copied! <pre>fn = CallableModel(callable=test_function, args=(1,), kwargs={\"y\":3})\n\nfn.signature\n</pre> fn = CallableModel(callable=test_function, args=(1,), kwargs={\"y\":3})  fn.signature Out[10]: <pre>Kwargs_test_function(args=[], kwarg_order=['x', 'y'], x=1, y=3)</pre> <p>Schema shows us the generated model:</p> In\u00a0[11]: Copied! <pre>fn.signature.schema()\n</pre> fn.signature.schema() Out[11]: <pre>{'title': 'Kwargs_test_function',\n 'type': 'object',\n 'properties': {'args': {'title': 'Args',\n   'default': [],\n   'type': 'array',\n   'items': {}},\n  'kwarg_order': {'title': 'Kwarg Order',\n   'default': ['x', 'y'],\n   'type': 'array',\n   'items': {}},\n  'x': {'title': 'X', 'default': 1, 'type': 'integer'},\n  'y': {'title': 'Y', 'default': 3, 'type': 'integer'}}}</pre> In\u00a0[12]: Copied! <pre># dict rep\nfn_dict = fn.dict()\nfn_dict\n</pre> # dict rep fn_dict = fn.dict() fn_dict Out[12]: <pre>{'callable': &lt;function __main__.test_function(x: int, y: int = 5)&gt;,\n 'signature': {'args': [], 'x': 1, 'y': 3}}</pre> In\u00a0[13]: Copied! <pre># load from dict\nfn_from_dict = CallableModel(**fn.dict()) \nfn_from_dict()\n</pre> # load from dict fn_from_dict = CallableModel(**fn.dict())  fn_from_dict() Out[13]: <pre>4</pre> In\u00a0[14]: Copied! <pre># json representation\nfn.json()\n</pre> # json representation fn.json()  Out[14]: <pre>'{\"callable\":\"__main__.test_function\",\"signature\":{\"args\":[],\"x\":1,\"y\":3}}'</pre> In\u00a0[15]: Copied! <pre># callable from json\nfn_from_json = CallableModel.parse_raw(fn.json())\nfn_from_json()\n</pre> # callable from json fn_from_json = CallableModel.parse_raw(fn.json()) fn_from_json() Out[15]: <pre>4</pre> In\u00a0[16]: Copied! <pre># Class kwargs passed after\nparameterized_class = CallableModel(callable=TestClass, kwargs={\"x\":1, \"y\":3})\ntest_class_obj = parameterized_class()\nassert isinstance(test_class_obj, (TestClass,))\n</pre> # Class kwargs passed after parameterized_class = CallableModel(callable=TestClass, kwargs={\"x\":1, \"y\":3}) test_class_obj = parameterized_class() assert isinstance(test_class_obj, (TestClass,)) In\u00a0[17]: Copied! <pre># dict rep\nparameterized_class_dict = parameterized_class.dict()\nparameterized_class_dict\n</pre> # dict rep parameterized_class_dict = parameterized_class.dict() parameterized_class_dict Out[17]: <pre>{'callable': __main__.TestClass, 'signature': {'args': [], 'x': 1, 'y': 3}}</pre> In\u00a0[18]: Copied! <pre># from dict\nparameterized_class_from_dict = CallableModel(**parameterized_class_dict)\nparameterized_class_from_dict\n</pre> # from dict parameterized_class_from_dict = CallableModel(**parameterized_class_dict) parameterized_class_from_dict Out[18]: <pre>CallableModel(callable=&lt;class '__main__.TestClass'&gt;, signature=Kwargs_TestClass(args=[], kwarg_order=['x', 'y'], x=1, y=3))</pre> In\u00a0[19]: Copied! <pre>parameterized_class_from_dict_obj = parameterized_class_from_dict()\nassert isinstance(parameterized_class_from_dict_obj, (TestClass,))\n</pre> parameterized_class_from_dict_obj = parameterized_class_from_dict() assert isinstance(parameterized_class_from_dict_obj, (TestClass,)) In\u00a0[20]: Copied! <pre>#json \nparameterized_class_json = parameterized_class.json()\nparameterized_class_json\n</pre> #json  parameterized_class_json = parameterized_class.json() parameterized_class_json Out[20]: <pre>'{\"callable\":\"__main__.TestClass\",\"signature\":{\"args\":[],\"x\":1,\"y\":3}}'</pre> In\u00a0[21]: Copied! <pre>parameterized_class_from_json = CallableModel.parse_raw(parameterized_class_json)\ntest_class_obj = parameterized_class_from_json()\nassert isinstance(test_class_obj, (TestClass,))\n</pre> parameterized_class_from_json = CallableModel.parse_raw(parameterized_class_json) test_class_obj = parameterized_class_from_json() assert isinstance(test_class_obj, (TestClass,)) <p>We can use the callables to construct a dynamic object loader. The generic type allows us to use this same method for any executor. The syntax: <code>ObjLoader[ThreadPoolExecutor]</code> composes a new class entirely, this one specific to the <code>ThreadPoolExecutor</code>.</p> In\u00a0[22]: Copied! <pre>from xopt.pydantic import ObjLoader\n</pre> from xopt.pydantic import ObjLoader <p>Let's test object loader on our <code>TestClass</code>:</p> In\u00a0[23]: Copied! <pre># create type\nTestClassLoader = ObjLoader[TestClass]\n\nobj_loader = TestClassLoader(kwargs={\"x\":1, \"y\":3})\nloaded = obj_loader.load()\nloaded\n</pre> # create type TestClassLoader = ObjLoader[TestClass]  obj_loader = TestClassLoader(kwargs={\"x\":1, \"y\":3}) loaded = obj_loader.load() loaded Out[23]: <pre>&lt;__main__.TestClass at 0x7fb1084dbc70&gt;</pre> <p>Can do this for a generic object like <code>ThreadPoolExecutor</code>:</p> In\u00a0[24]: Copied! <pre># create Type\nTPELoader = ObjLoader[ThreadPoolExecutor]\n\ntpe_loader = TPELoader(kwargs={\"max_workers\":1})\ntpe = tpe_loader.load()\ntpe\ntpe_loader_json  = tpe_loader.json()\ntpe_loader_json\ntpe_loader_from_json = TPELoader.parse_raw(tpe_loader_json)\n\n\n# shutdown tpe\ntpe.shutdown()\n</pre> # create Type TPELoader = ObjLoader[ThreadPoolExecutor]  tpe_loader = TPELoader(kwargs={\"max_workers\":1}) tpe = tpe_loader.load() tpe tpe_loader_json  = tpe_loader.json() tpe_loader_json tpe_loader_from_json = TPELoader.parse_raw(tpe_loader_json)   # shutdown tpe tpe.shutdown()  In\u00a0[25]: Copied! <pre>from xopt.pydantic import BaseExecutor, NormalExecutor\n</pre> from xopt.pydantic import BaseExecutor, NormalExecutor <p>Create some NormalExecutors: (must manually shutdown)</p> In\u00a0[26]: Copied! <pre># ThreadPool\n# create type\nNormTPExecutor = NormalExecutor[ThreadPoolExecutor]\n\ntpe_exec = NormTPExecutor(kwargs={\"max_workers\":1})\n# submit\ntpe_exec.submit(fn=test_function, x=1, y=8)\n</pre> # ThreadPool # create type NormTPExecutor = NormalExecutor[ThreadPoolExecutor]  tpe_exec = NormTPExecutor(kwargs={\"max_workers\":1}) # submit tpe_exec.submit(fn=test_function, x=1, y=8) Out[26]: <pre>&lt;Future at 0x7fb0b319e3d0 state=finished returned int&gt;</pre> In\u00a0[27]: Copied! <pre># map\ntpe_exec.map(test_function, ((1, 4), (3, 4)))\n</pre> # map tpe_exec.map(test_function, ((1, 4), (3, 4))) Out[27]: <pre>&lt;generator object Executor.map.&lt;locals&gt;.result_iterator at 0x7fb0b318e9e0&gt;</pre> In\u00a0[28]: Copied! <pre>tpe_exec.shutdown()\n</pre> tpe_exec.shutdown() In\u00a0[29]: Copied! <pre># Dask\nfrom distributed import Client\nfrom distributed.cfexecutor import ClientExecutor\n\n# Using an existing executor\nclient = Client(silence_logs=logging.ERROR)\nexecutor = client.get_executor()\n\n# create type\nNormalDaskExecutor =  NormalExecutor[type(executor)]\n\ndask_executor = NormalDaskExecutor(executor=executor)\ndask_executor.submit(fn=test_function, x=1, y=8)\n</pre> # Dask from distributed import Client from distributed.cfexecutor import ClientExecutor  # Using an existing executor client = Client(silence_logs=logging.ERROR) executor = client.get_executor()  # create type NormalDaskExecutor =  NormalExecutor[type(executor)]  dask_executor = NormalDaskExecutor(executor=executor) dask_executor.submit(fn=test_function, x=1, y=8) Out[29]: <pre>&lt;Future at 0x7fb0b16c10a0 state=pending&gt;</pre> In\u00a0[30]: Copied! <pre>dask_executor_json = dask_executor.json()\ndask_executor_json\n</pre> dask_executor_json = dask_executor.json() dask_executor_json Out[30]: <pre>'{\"loader\":{\"object\":null,\"loader\":{\"callable\":\"distributed.cfexecutor.ClientExecutor\",\"signature\":{\"args\":[],\"client\":\"inspect._empty\"}},\"object_type\":\"distributed.cfexecutor.ClientExecutor\"},\"submit_callable\":\"submit\",\"map_callable\":\"map\",\"shutdown_callable\":\"shutdown\",\"executor\":\"distributed.cfexecutor.ClientExecutor\"}'</pre> In\u00a0[31]: Copied! <pre>dask_executor.shutdown()\n</pre> dask_executor.shutdown() In\u00a0[32]: Copied! <pre># this raises error because client not passed...\n# dask_executor_from_json = NormalDaskExecutor.parse_raw(dask_executor_json)\n</pre> # this raises error because client not passed... # dask_executor_from_json = NormalDaskExecutor.parse_raw(dask_executor_json) <p>Context managers handle shutdown for us:</p> In\u00a0[33]: Copied! <pre># ContexExecutor with context handling on submission and no executor persistence\nclass ContextExecutor(\n    BaseExecutor[ObjType],\n    Generic[ObjType],\n    arbitrary_types_allowed=True,\n    json_encoders=JSON_ENCODERS,\n):\n    @contextlib.contextmanager\n    def context(self):\n\n        try:\n            self.executor = self.loader.load()\n            yield self.executor\n\n        finally:\n            self.shutdown()\n            self.executor = None\n\n    def submit(self, fn, **kwargs) -&gt; Future:\n        with self.context() as ctxt:\n            submit_fn = getattr(ctxt, self.submit_callable)\n            return submit_fn(fn, **kwargs)\n        \n    def map(self, fn, iter: Iterable) -&gt; Iterable[Future]:\n        with self.context() as ctxt:\n            map_fn = getattr(ctxt, self.map_callable)\n            return map_fn(fn, iter)\n</pre> # ContexExecutor with context handling on submission and no executor persistence class ContextExecutor(     BaseExecutor[ObjType],     Generic[ObjType],     arbitrary_types_allowed=True,     json_encoders=JSON_ENCODERS, ):     @contextlib.contextmanager     def context(self):          try:             self.executor = self.loader.load()             yield self.executor          finally:             self.shutdown()             self.executor = None      def submit(self, fn, **kwargs) -&gt; Future:         with self.context() as ctxt:             submit_fn = getattr(ctxt, self.submit_callable)             return submit_fn(fn, **kwargs)              def map(self, fn, iter: Iterable) -&gt; Iterable[Future]:         with self.context() as ctxt:             map_fn = getattr(ctxt, self.map_callable)             return map_fn(fn, iter)  <p>Create some ContextExecutors</p> In\u00a0[34]: Copied! <pre># ThreadPoolExecutor\n# create type\n\nContextTPExecutor = ContextExecutor[ThreadPoolExecutor]\n\ncontext_exec = ContextTPExecutor(kwargs={\"max_workers\":1})\ncontext_exec.submit(fn=test_function, x=1, y=8)\n</pre> # ThreadPoolExecutor # create type  ContextTPExecutor = ContextExecutor[ThreadPoolExecutor]  context_exec = ContextTPExecutor(kwargs={\"max_workers\":1}) context_exec.submit(fn=test_function, x=1, y=8) Out[34]: <pre>&lt;Future at 0x7fb0b04a1a90 state=finished returned int&gt;</pre> In\u00a0[35]: Copied! <pre>context_exec.map(test_function, ((1, 4), (3, 4)))\n</pre> context_exec.map(test_function, ((1, 4), (3, 4))) Out[35]: <pre>&lt;generator object Executor.map.&lt;locals&gt;.result_iterator at 0x7fb0b0444510&gt;</pre> In\u00a0[36]: Copied! <pre>context_exec_json = context_exec.json()\ncontext_exec_json\n</pre> context_exec_json = context_exec.json() context_exec_json Out[36]: <pre>'{\"loader\":{\"object\":null,\"loader\":{\"callable\":\"concurrent.futures.thread.ThreadPoolExecutor\",\"signature\":{\"args\":[],\"initializer\":null,\"initargs\":null,\"max_workers\":1,\"thread_name_prefix\":\"\"}},\"object_type\":\"concurrent.futures.thread.ThreadPoolExecutor\"},\"submit_callable\":\"submit\",\"map_callable\":\"map\",\"shutdown_callable\":\"shutdown\",\"executor\":null}'</pre> In\u00a0[37]: Copied! <pre>context_exec_from_json = ContextTPExecutor.parse_raw(\n        context_exec_json\n    )\ncontext_exec_from_json.submit(fn=test_function, x=1, y=8)\n</pre> context_exec_from_json = ContextTPExecutor.parse_raw(         context_exec_json     ) context_exec_from_json.submit(fn=test_function, x=1, y=8) Out[37]: <pre>&lt;Future at 0x7fb0b3137d60 state=finished returned int&gt;</pre> In\u00a0[38]: Copied! <pre>context_exec_from_json.map(test_function, ((1, 4), (3, 4)))\n</pre> context_exec_from_json.map(test_function, ((1, 4), (3, 4))) Out[38]: <pre>&lt;generator object Executor.map.&lt;locals&gt;.result_iterator at 0x7fb0b04449e0&gt;</pre> <p>Some executors are generated with Clients that manage sessions: ** will require gathering results before shutdown...</p> In\u00a0[39]: Copied! <pre>import yaml\n\ndef evaluate(inputs, y=5, z=None):\n    return {'result': inputs['x'] + y }\n\nfn = CallableModel(callable=evaluate, kwargs={\"y\":100})\nfn_json = fn.json(exclude_none=True)\n\n\nprint(yaml.dump(yaml.safe_load(fn_json)))\n</pre> import yaml  def evaluate(inputs, y=5, z=None):     return {'result': inputs['x'] + y }  fn = CallableModel(callable=evaluate, kwargs={\"y\":100}) fn_json = fn.json(exclude_none=True)   print(yaml.dump(yaml.safe_load(fn_json))) <pre>callable: __main__.evaluate\nsignature:\n  args: []\n  inputs: inspect._empty\n  y: 100\n\n</pre> In\u00a0[40]: Copied! <pre>fn_from_json = CallableModel.parse_raw(fn_json)\nfn_from_json\n</pre> fn_from_json = CallableModel.parse_raw(fn_json) fn_from_json Out[40]: <pre>CallableModel(callable=&lt;function evaluate at 0x7fb0f8d9b310&gt;, signature=Kwargs_evaluate(args=[], z=None, kwarg_order=['inputs', 'y', 'z'], inputs='inspect._empty', y=100))</pre> In\u00a0[41]: Copied! <pre>fn_from_json({\"x\":5}, z=2)\n</pre> fn_from_json({\"x\":5}, z=2) Out[41]: <pre>{'result': 105}</pre>"},{"location":"examples/developer/executors/#pydantic-based-executors","title":"Pydantic-based executors\u00b6","text":"<p>The PEP-3184 executor standard allows us to create an interface for executor objects and provide intelligent context for their execution. Pydantic validators allow the dynamic validation of executor initialization and execution based on signature inspection.</p> <p>Before you start, make sure you're using Pydantic &gt;= 1.9.0. 1.8 has all sorts of bugs with json encoder propagation.</p>"},{"location":"examples/developer/executors/#generics","title":"GENERICS\u00b6","text":"<p>Because the executor classes take many forms, we'll be making use of Pydantic's generic class composition for executor type interpolation. We are able to do this by creating a placeholder TypeVar. Here, this is names ObjType, because the executor classes make use of a generalizable loading approach that could be extented to objects generally.</p>"},{"location":"examples/developer/executors/#json-encoders","title":"JSON Encoders\u00b6","text":"<p>Pydantic does not propogate JSON encoders to child classes, so we'll define a set of common encoders:</p>"},{"location":"examples/developer/executors/#utility-functions-for-validating-signatures-and-getting-callables-from-strings","title":"Utility functions for validating signatures and getting callables from strings\u00b6","text":"<p>Central to generalizablity between executors is the ability to validate signatures args/kwargs against the executor class.</p>"},{"location":"examples/developer/executors/#representing-callables-as-pydantic-models","title":"Representing callables as Pydantic models\u00b6","text":"<p>Representing callables as pydantic models allows us to take advantage of both pydantic serialization to json and pydantic's validation hooks for the kwarg validation upon creation, with possibility of delaying load. Here <code>CallableModel</code>, we can provide initialization kwargs for a to-be-instantiated-later object and reap the benefit of additional kwarg validation.</p>"},{"location":"examples/developer/executors/#with-classes","title":"With Classes\u00b6","text":""},{"location":"examples/developer/executors/#executors","title":"Executors\u00b6","text":"<p>The previous classes were an attempt to demonstrate generic utility. The Executors to follow will build off of those common utilities to parameterize generic executors complying with the pep-3148 standard (the callables have been typified in case of deviation). Likewise, the following BaseExecutor outlines common executor fields and methods.</p>"},{"location":"examples/developer/executors/#normal-contextexecutor","title":"Normal, ContextExecutor\u00b6","text":"<p>Now, we subclass base to create two executors: <code>NormalExecutor</code>, and <code>ContextExecutor</code>. In the case that the user would like to create a persistent executor passed to the Evaluator, they would use the NormalExecutor. The ContextExecutor provides a context manager to dynamically create executor instances during execution.</p>"},{"location":"examples/developer/logger_example/","title":"Logging example","text":"In\u00a0[1]: Copied! <pre>from xopt.log import configure_logger\n</pre> from xopt.log import configure_logger In\u00a0[2]: Copied! <pre># Notebook names are this:\n__name__\n</pre> # Notebook names are this: __name__ Out[2]: <pre>'__main__'</pre> In\u00a0[3]: Copied! <pre># Put this in the top of any .py file or notebook\nimport logging\n\nlogger = logging.getLogger(__name__)\n</pre> # Put this in the top of any .py file or notebook import logging  logger = logging.getLogger(__name__) In\u00a0[4]: Copied! <pre># Put this at the top-level application. For texting, just use the notebook name.\nconfigure_logger(logger_name=__name__, file=\"log.txt\", level=\"INFO\")\n</pre> # Put this at the top-level application. For texting, just use the notebook name. configure_logger(logger_name=__name__, file=\"log.txt\", level=\"INFO\") In\u00a0[5]: Copied! <pre>logger.debug(\"some debug message\")  # This will write to log.txt\nlogger.info(\"some info message\")\n</pre> logger.debug(\"some debug message\")  # This will write to log.txt logger.info(\"some info message\") In\u00a0[6]: Copied! <pre>!cat log.txt\n</pre> !cat log.txt <pre>2023-05-01T17:19:05+0000 - __main__ - INFO - some info message\r\n</pre> In\u00a0[7]: Copied! <pre># Change level to debug\nconfigure_logger(logger_name=__name__, file=\"log.txt\", level=\"DEBUG\")\n</pre> # Change level to debug configure_logger(logger_name=__name__, file=\"log.txt\", level=\"DEBUG\") In\u00a0[8]: Copied! <pre>logger.debug(\"another debug message\")\nlogger.info(\"another info message\")\n</pre> logger.debug(\"another debug message\") logger.info(\"another info message\") In\u00a0[9]: Copied! <pre>!cat log.txt # Now we see debug messages\n</pre> !cat log.txt # Now we see debug messages <pre>2023-05-01T17:19:05+0000 - __main__ - INFO - some info message\r\n2023-05-01T17:19:06+0000 - __main__ - DEBUG - another debug message\r\n2023-05-01T17:19:06+0000 - __main__ - INFO - another info message\r\n</pre> In\u00a0[10]: Copied! <pre># With no file=, the default will go to stdout (here)\nconfigure_logger(logger_name=__name__, level=\"INFO\")\n</pre> # With no file=, the default will go to stdout (here) configure_logger(logger_name=__name__, level=\"INFO\") In\u00a0[11]: Copied! <pre>logger.debug(\"yet another debug message\")\nlogger.info(\"yet another info message\")\n</pre> logger.debug(\"yet another debug message\") logger.info(\"yet another info message\") <pre>yet another info message\n</pre> In\u00a0[12]: Copied! <pre>!cat log.txt # Notice this function onlu\n</pre> !cat log.txt # Notice this function onlu <pre>2023-05-01T17:19:05+0000 - __main__ - INFO - some info message\r\n2023-05-01T17:19:06+0000 - __main__ - DEBUG - another debug message\r\n2023-05-01T17:19:06+0000 - __main__ - INFO - another info message\r\n</pre> In\u00a0[13]: Copied! <pre># configure_logger only manages one handler. For more, look at the xopt.log code\nlogger.handlers\n</pre> # configure_logger only manages one handler. For more, look at the xopt.log code logger.handlers Out[13]: <pre>[&lt;StreamHandler stdout (INFO)&gt;]</pre> In\u00a0[14]: Copied! <pre>TEXT = \"\"\"\n\nimport logging\nlogger = logging.getLogger(__name__)\n\n\ndef f():\n    print('__name__ =', __name__)\n    logger.info('this is f')\n\"\"\"\nwith open(\"myf.py\", \"w\") as f:\n    f.write(TEXT)\n</pre> TEXT = \"\"\"  import logging logger = logging.getLogger(__name__)   def f():     print('__name__ =', __name__)     logger.info('this is f') \"\"\" with open(\"myf.py\", \"w\") as f:     f.write(TEXT) In\u00a0[15]: Copied! <pre>from myf import f\n</pre> from myf import f In\u00a0[16]: Copied! <pre>f()  # Notice no logger message.\n</pre> f()  # Notice no logger message. <pre>__name__ = myf\n</pre> In\u00a0[17]: Copied! <pre>configure_logger(logger_name=\"myf\", level=\"INFO\")\n</pre> configure_logger(logger_name=\"myf\", level=\"INFO\") In\u00a0[18]: Copied! <pre>f()  # Now we see the logger message.\n</pre> f()  # Now we see the logger message. <pre>__name__ = myf\nthis is f\n</pre> In\u00a0[19]: Copied! <pre>!rm log.txt\n!rm myf.py\n</pre> !rm log.txt !rm myf.py"},{"location":"examples/developer/logger_example/#logging-example","title":"Logging example\u00b6","text":""},{"location":"examples/developer/logger_example/#adding-logging-to-py-files","title":"Adding logging to .py files\u00b6","text":"<p>Importing .py files outside of a package have named based on their filename.</p>"},{"location":"examples/developer/logger_example/#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/es/extremum_seeking/","title":"Extremum seeking","text":"In\u00a0[1]: Copied! <pre># If you encounter the \"Initializing libomp.dylib, but found libomp.dylib already initialized.\" error\n# Please run this cell\n\nimport os\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n</pre> # If you encounter the \"Initializing libomp.dylib, but found libomp.dylib already initialized.\" error # Please run this cell  import os  os.environ['KMP_DUPLICATE_LIB_OK']='True' In\u00a0[2]: Copied! <pre>import numpy as np\nfrom xopt.generators.es.extremumseeking import ExtremumSeekingGenerator\nfrom xopt.vocs import VOCS\nfrom xopt.evaluator import Evaluator\nfrom xopt import Xopt\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> import numpy as np from xopt.generators.es.extremumseeking import ExtremumSeekingGenerator from xopt.vocs import VOCS from xopt.evaluator import Evaluator from xopt import Xopt from tqdm.auto import tqdm import warnings warnings.filterwarnings(\"ignore\") In\u00a0[3]: Copied! <pre>np.random.seed(42)  # set deterministic run\n\nnES = 10\nES_steps = 5000  # maximum number of evaluations\n\n# This global dict is used as a counter to emulate drifting\nstates = {\n    'count': 0\n}\n\nnoise = 0.1 * np.random.randn(ES_steps)\n\n# This is the unknown optimal point\np_opt = 1.5 * (2 * np.random.rand(nES) - 1)\n\n# Various frequencies for unknown points\nw_opt = 0.25 + 2 * np.random.rand(nES)\n\ndef f_ES_minimize(input_dict):\n    p = []\n    for i in range(10):\n        p.append(input_dict[f'p{i}'])\n    p = np.array(p)\n    \n    # Vary the optimal point with time\n    p_opt_i = np.zeros(nES)\n    i = states['count']\n    \n    outcome_dict = {}\n    for n in np.arange(nES):\n        p_opt_i[n] = p_opt[n] * (1 + np.sin(2 * np.pi * w_opt[n] * i / 2000))\n    # This simple cost will be distance from the optimal point\n    f_val = np.sum((p - p_opt_i) ** 2) + noise[i]\n    \n    states['count'] += 1\n    outcome_dict = {'f': f_val, 'p_opt': p_opt_i}\n    \n    return outcome_dict\n</pre> np.random.seed(42)  # set deterministic run  nES = 10 ES_steps = 5000  # maximum number of evaluations  # This global dict is used as a counter to emulate drifting states = {     'count': 0 }  noise = 0.1 * np.random.randn(ES_steps)  # This is the unknown optimal point p_opt = 1.5 * (2 * np.random.rand(nES) - 1)  # Various frequencies for unknown points w_opt = 0.25 + 2 * np.random.rand(nES)  def f_ES_minimize(input_dict):     p = []     for i in range(10):         p.append(input_dict[f'p{i}'])     p = np.array(p)          # Vary the optimal point with time     p_opt_i = np.zeros(nES)     i = states['count']          outcome_dict = {}     for n in np.arange(nES):         p_opt_i[n] = p_opt[n] * (1 + np.sin(2 * np.pi * w_opt[n] * i / 2000))     # This simple cost will be distance from the optimal point     f_val = np.sum((p - p_opt_i) ** 2) + noise[i]          states['count'] += 1     outcome_dict = {'f': f_val, 'p_opt': p_opt_i}          return outcome_dict In\u00a0[4]: Copied! <pre>YAML = \"\"\"\nxopt:\n    max_evaluations: 5000\ngenerator:\n    name: extremum_seeking\n    k: 2.0\n    oscillation_size: 0.1\n    decay_rate: 1.0\nevaluator:\n    function: __main__.f_ES_minimize\nvocs:\n    variables:\n        p0: [-2, 2]\n        p1: [-2, 2]\n        p2: [-2, 2]\n        p3: [-2, 2]\n        p4: [-2, 2]\n        p5: [-2, 2]\n        p6: [-2, 2]\n        p7: [-2, 2]\n        p8: [-2, 2]\n        p9: [-2, 2]\n    objectives:\n        f: MINIMIZE\n\"\"\"\n\nX = Xopt(YAML)\nX\n</pre> YAML = \"\"\" xopt:     max_evaluations: 5000 generator:     name: extremum_seeking     k: 2.0     oscillation_size: 0.1     decay_rate: 1.0 evaluator:     function: __main__.f_ES_minimize vocs:     variables:         p0: [-2, 2]         p1: [-2, 2]         p2: [-2, 2]         p3: [-2, 2]         p4: [-2, 2]         p5: [-2, 2]         p6: [-2, 2]         p7: [-2, 2]         p8: [-2, 2]         p9: [-2, 2]     objectives:         f: MINIMIZE \"\"\"  X = Xopt(YAML) X Out[4]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g0676a82\nData size: 0\nConfig as YAML:\nxopt: {asynch: false, strict: false, dump_file: null, max_evaluations: 5000}\ngenerator: {name: extremum_seeking, k: 2.0, oscillation_size: 0.1, decay_rate: 1.0}\nevaluator:\n  function: __main__.f_ES_minimize\n  max_workers: 1\n  function_kwargs: {}\n  vectorized: false\nvocs:\n  variables:\n    p0: [-2.0, 2.0]\n    p1: [-2.0, 2.0]\n    p2: [-2.0, 2.0]\n    p3: [-2.0, 2.0]\n    p4: [-2.0, 2.0]\n    p5: [-2.0, 2.0]\n    p6: [-2.0, 2.0]\n    p7: [-2.0, 2.0]\n    p8: [-2.0, 2.0]\n    p9: [-2.0, 2.0]\n  constraints: {}\n  objectives: {f: MINIMIZE}\n  constants: {}\n  linked_variables: {}\n</pre> In\u00a0[5]: Copied! <pre># Reset global counter to guarantee deterministic optimization\nstates['count'] = 0\n\nX.run()\n</pre> # Reset global counter to guarantee deterministic optimization states['count'] = 0  X.run() <p>Now you can go directly to the Visualization section and check out the results.</p> In\u00a0[6]: Copied! <pre>variables = {}\nfor i in range(nES):\n    variables[f'p{i}'] = [-2, 2]\n\nvocs = VOCS(\n    variables=variables,\n    objectives={'f': 'MINIMIZE'},\n)\n</pre> variables = {} for i in range(nES):     variables[f'p{i}'] = [-2, 2]  vocs = VOCS(     variables=variables,     objectives={'f': 'MINIMIZE'}, ) In\u00a0[7]: Copied! <pre>vocs\n</pre> vocs Out[7]: <pre>VOCS(variables={'p0': [-2.0, 2.0], 'p1': [-2.0, 2.0], 'p2': [-2.0, 2.0], 'p3': [-2.0, 2.0], 'p4': [-2.0, 2.0], 'p5': [-2.0, 2.0], 'p6': [-2.0, 2.0], 'p7': [-2.0, 2.0], 'p8': [-2.0, 2.0], 'p9': [-2.0, 2.0]}, constraints={}, objectives={'f': 'MINIMIZE'}, constants={}, linked_variables={})</pre> In\u00a0[8]: Copied! <pre>evaluator = Evaluator(function=f_ES_minimize)\n</pre> evaluator = Evaluator(function=f_ES_minimize) In\u00a0[9]: Copied! <pre>generator = ExtremumSeekingGenerator(vocs)\n</pre> generator = ExtremumSeekingGenerator(vocs) In\u00a0[10]: Copied! <pre>generator.options\n</pre> generator.options Out[10]: <pre>ExtremumSeekingOptions(k=2.0, oscillation_size=0.1, decay_rate=1.0)</pre> <p>Note that ES has 3 hyper-parameters: <code>k</code>, <code>oscillation_size</code>, and <code>decay_rate</code>.</p> <ul> <li><code>k</code>: ES feedback gain (set <code>k &lt; 0</code> for maximization instead of minimization)</li> <li><code>oscillation_size</code>: ES dithering size</li> <li><code>decay_rate</code>: This value is optional, it causes the oscillation sizes to naturally decay. If you want the parameters to persistently oscillate without decay, set <code>decay_rate = 1.0</code></li> </ul> In\u00a0[11]: Copied! <pre>X = Xopt(vocs=vocs, evaluator=evaluator, generator=generator)\n</pre> X = Xopt(vocs=vocs, evaluator=evaluator, generator=generator) In\u00a0[12]: Copied! <pre>X.options.max_evaluations = ES_steps\n</pre> X.options.max_evaluations = ES_steps In\u00a0[13]: Copied! <pre># Reset global counter to guarantee deterministic optimization\nstates['count'] = 0\n\nfor i in tqdm(range(ES_steps)):\n    X.step()\n</pre> # Reset global counter to guarantee deterministic optimization states['count'] = 0  for i in tqdm(range(ES_steps)):     X.step() In\u00a0[14]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[15]: Copied! <pre># Plot all results\nplt.figure(1,figsize=(8,10))\n\nplt.subplot(2,1,1)\nplt.plot(X.data['f'])\nplt.ylabel('ES cost')\nplt.xticks([])\n\n\nplt.subplot(2,1,2)\nplt.plot(X.data[[f'p{i}' for i in range(10)]],alpha=0.25)\n_p_opt = np.vstack(X.data['p_opt'].values).astype(float)  # do not use p_opt as var name!\nplt.plot(_p_opt, 'k--')\nplt.plot(2+np.zeros(ES_steps),'r')\nplt.plot(-2+np.zeros(ES_steps),'r')\nplt.legend(frameon=False)\nplt.ylabel('ES parameter')\nplt.xlabel('ES step')\n\nplt.tight_layout()\n</pre> # Plot all results plt.figure(1,figsize=(8,10))  plt.subplot(2,1,1) plt.plot(X.data['f']) plt.ylabel('ES cost') plt.xticks([])   plt.subplot(2,1,2) plt.plot(X.data[[f'p{i}' for i in range(10)]],alpha=0.25) _p_opt = np.vstack(X.data['p_opt'].values).astype(float)  # do not use p_opt as var name! plt.plot(_p_opt, 'k--') plt.plot(2+np.zeros(ES_steps),'r') plt.plot(-2+np.zeros(ES_steps),'r') plt.legend(frameon=False) plt.ylabel('ES parameter') plt.xlabel('ES step')  plt.tight_layout() <pre>No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n</pre> In\u00a0[16]: Copied! <pre># Plot Individual Parameter Trajectories\nplt.figure(2,figsize=(15,8))\n\nfor n in np.arange(nES):\n    \n    \n    plt.subplot(2,5,n+1)\n    plt.plot(X.data[f'p{n}'],label=f'$p^{{ES}}_{n+1}$')\n    plt.plot(_p_opt[:,n],'k--',label=f'$p^*_{n+1}$')\n    plt.plot(2+np.zeros(ES_steps),'r--')\n    plt.plot(-2+np.zeros(ES_steps),'r--')\n    plt.ylim([-3,5])\n    plt.legend(frameon=False,loc=1)\n    if n == 0:\n        plt.ylabel('parameters')\n    elif n == 5:\n        plt.ylabel('parameters')\n    else:\n        plt.yticks([])\n    if n &gt; 4:\n        plt.xlabel('ES step')\n    else:\n        plt.xticks([])\n\nplt.tight_layout()\n</pre> # Plot Individual Parameter Trajectories plt.figure(2,figsize=(15,8))  for n in np.arange(nES):               plt.subplot(2,5,n+1)     plt.plot(X.data[f'p{n}'],label=f'$p^{{ES}}_{n+1}$')     plt.plot(_p_opt[:,n],'k--',label=f'$p^*_{n+1}$')     plt.plot(2+np.zeros(ES_steps),'r--')     plt.plot(-2+np.zeros(ES_steps),'r--')     plt.ylim([-3,5])     plt.legend(frameon=False,loc=1)     if n == 0:         plt.ylabel('parameters')     elif n == 5:         plt.ylabel('parameters')     else:         plt.yticks([])     if n &gt; 4:         plt.xlabel('ES step')     else:         plt.xticks([])  plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/es/extremum_seeking/#extremum-seeking-optimization","title":"Extremum Seeking Optimization\u00b6","text":"<p>In this example we demonstrate extremum seeking optimization. The optimum of the test evaluate function would drift around a center point and we would be trying to follow the trend by applying extremum seeking technique.</p>"},{"location":"examples/es/extremum_seeking/#extremum-seeking-test-problem","title":"Extremum seeking test problem\u00b6","text":"<p>This test problem is a 10-D quadratic function, with its optimum drifting around the initial position. We also add some noise to make the problem more realistic.</p>"},{"location":"examples/es/extremum_seeking/#run-es-on-the-test-problem-yaml-method","title":"Run ES on the test problem (YAML method)\u00b6","text":""},{"location":"examples/es/extremum_seeking/#run-es-on-the-test-problem-api-method","title":"Run ES on the test problem (API method)\u00b6","text":""},{"location":"examples/es/extremum_seeking/#vocs","title":"VOCS\u00b6","text":"<p>We'll set the bounds for all the variables pi to [-2, 2].</p>"},{"location":"examples/es/extremum_seeking/#evaluator","title":"Evaluator\u00b6","text":""},{"location":"examples/es/extremum_seeking/#generator","title":"Generator\u00b6","text":""},{"location":"examples/es/extremum_seeking/#run-the-optimization","title":"Run the optimization\u00b6","text":""},{"location":"examples/es/extremum_seeking/#visualization","title":"Visualization\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mggpo/","title":"Multi-objective Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre>%reset -f\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom copy import deepcopy\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators import MGGPOGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\n\nevaluator = Evaluator(function=evaluate_TNK)\nevaluator.max_workers = 10\n\n# test check options\nvocs = deepcopy(tnk_vocs)\ngen = MGGPOGenerator(vocs)\ngen.options.acq.reference_point = {\"y1\":1.5,\"y2\":1.5}\nX = Xopt(evaluator=evaluator, generator=gen, vocs=vocs)\nX.evaluate_data(pd.DataFrame({\"x1\": [1.0, 0.75], \"x2\": [0.75, 1.0]}))\n\nfor i in range(10):\n    print(i)\n    X.step()\n</pre> %reset -f  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  from copy import deepcopy  import pandas as pd import numpy as np import torch  from xopt import Xopt, Evaluator from xopt.generators import MGGPOGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs   evaluator = Evaluator(function=evaluate_TNK) evaluator.max_workers = 10  # test check options vocs = deepcopy(tnk_vocs) gen = MGGPOGenerator(vocs) gen.options.acq.reference_point = {\"y1\":1.5,\"y2\":1.5} X = Xopt(evaluator=evaluator, generator=gen, vocs=vocs) X.evaluate_data(pd.DataFrame({\"x1\": [1.0, 0.75], \"x2\": [0.75, 1.0]}))  for i in range(10):     print(i)     X.step() <pre>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n</pre> In\u00a0[2]: Copied! <pre>X.generator.data\n</pre> X.generator.data Out[2]: x1 x2 y1 y2 c1 c2 some_array xopt_runtime xopt_error a 1 1.000000 0.750000 1.000000 0.750000 0.626888 0.312500 [1, 2, 3] 0.000058 False NaN 2 0.750000 1.000000 0.750000 1.000000 0.626888 0.312500 [1, 2, 3] 0.000013 False NaN 3 1.081365 2.527768 1.081365 2.527768 6.460658 4.449827 [1, 2, 3] 0.000046 False dummy_constant 4 1.452405 2.229344 1.452405 2.229344 6.177731 3.897707 [1, 2, 3] 0.000015 False dummy_constant 5 0.093052 2.407746 0.093052 2.407746 4.724397 3.805100 [1, 2, 3] 0.000016 False dummy_constant ... ... ... ... ... ... ... ... ... ... ... 98 0.950426 0.489711 0.950426 0.489711 0.119204 0.202989 [1, 2, 3] 0.000010 False dummy_constant 99 0.950426 0.503751 0.950426 0.503751 0.151486 0.202898 [1, 2, 3] 0.000010 False dummy_constant 100 0.855587 0.552471 0.855587 0.552471 0.134120 0.129195 [1, 2, 3] 0.000009 False dummy_constant 101 1.058823 0.324355 1.058823 0.324355 0.221939 0.343135 [1, 2, 3] 0.000010 False dummy_constant 102 0.853469 0.566749 0.853469 0.566749 0.149509 0.129396 [1, 2, 3] 0.000010 False dummy_constant <p>102 rows \u00d7 10 columns</p> In\u00a0[3]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\ntheta = np.linspace(0, np.pi / 2)\nr = np.sqrt(1 + 0.1 * np.cos(16 * theta))\nx_1 = r * np.sin(theta)\nx_2_lower = r * np.cos(theta)\nx_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5\n\nz = np.zeros_like(x_1)\n\n# ax2.plot(x_1, x_2_lower,'r')\nax.fill_between(x_1, z, x_2_lower, fc=\"white\")\ncircle = plt.Circle(\n    (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\"\n)\nax.add_patch(circle)\nhistory = pd.concat(\n    [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False\n)\n\nax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\nax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\nax.set_xlim(0, 3.14)\nax.set_ylim(0, 3.14)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots()  theta = np.linspace(0, np.pi / 2) r = np.sqrt(1 + 0.1 * np.cos(16 * theta)) x_1 = r * np.sin(theta) x_2_lower = r * np.cos(theta) x_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5  z = np.zeros_like(x_1)  # ax2.plot(x_1, x_2_lower,'r') ax.fill_between(x_1, z, x_2_lower, fc=\"white\") circle = plt.Circle(     (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\" ) ax.add_patch(circle) history = pd.concat(     [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False )  ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\") ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")  ax.set_xlim(0, 3.14) ax.set_ylim(0, 3.14) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") ax.set_aspect(\"equal\") In\u00a0[4]: Copied! <pre>from matplotlib import pyplot as plt  # plot model predictions\n\ndata = X.data\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.train_model(X.generator.data)\n\n# create mesh\nn = 100\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\noutputs = X.generator.vocs.output_names\nwith torch.no_grad():\n    post = model.posterior(pts)\n\n    for i in range(len(vocs.output_names)):\n        mean = post.mean[...,i]\n        fig, ax = plt.subplots()\n        ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C1\")\n        c = ax.pcolor(\n            xx, yy, mean.squeeze().reshape(n, n),\n            cmap=\"seismic\",\n            vmin=-10.0,\n            vmax=10.0)\n        fig.colorbar(c)\n        ax.set_title(f\"Posterior mean: {outputs[i]}\")\n</pre> from matplotlib import pyplot as plt  # plot model predictions  data = X.data  bounds = X.generator.vocs.bounds model = X.generator.train_model(X.generator.data)  # create mesh n = 100 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  outputs = X.generator.vocs.output_names with torch.no_grad():     post = model.posterior(pts)      for i in range(len(vocs.output_names)):         mean = post.mean[...,i]         fig, ax = plt.subplots()         ax.plot(*data[[\"x1\", \"x2\"]].to_numpy().T, \"+C1\")         c = ax.pcolor(             xx, yy, mean.squeeze().reshape(n, n),             cmap=\"seismic\",             vmin=-10.0,             vmax=10.0)         fig.colorbar(c)         ax.set_title(f\"Posterior mean: {outputs[i]}\") In\u00a0[5]: Copied! <pre># plot the acquisition function\nfrom xopt.generators.bayesian.objectives import feasibility\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.model\n\n# create mesh\nn = 50\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nacq_func = X.generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\n    ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")\n\n    feas = feasibility(pts.unsqueeze(1), model, X.generator.sampler, tnk_vocs).flatten()\n\n    fig2, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n\ncandidate = X.generator.generate(1)\nprint(candidate[[\"x1\", \"x2\"]].to_numpy())\nax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\")\n</pre> # plot the acquisition function from xopt.generators.bayesian.objectives import feasibility  bounds = X.generator.vocs.bounds model = X.generator.model  # create mesh n = 50 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  acq_func = X.generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")     fig.colorbar(c)     ax.set_title(\"Acquisition function\")      ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")     ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")      ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")      feas = feasibility(pts.unsqueeze(1), model, X.generator.sampler, tnk_vocs).flatten()      fig2, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(\"Feasible Region\")  candidate = X.generator.generate(1) print(candidate[[\"x1\", \"x2\"]].to_numpy()) ax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\") <pre>[[0.843574   0.54157539]]\n</pre> Out[5]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f9b4ea99a90&gt;]</pre> In\u00a0[5]: Copied! <pre>\n</pre>"},{"location":"examples/multi_objective_bayes_opt/mggpo/#multi-objective-bayesian-optimization","title":"Multi-objective Bayesian Optimization\u00b6","text":"<p>TNK function $n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/multi_objective_bayes_opt/mggpo/#plot-results","title":"plot results\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mobo/","title":"Multi-objective Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre>%reset -f\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom copy import deepcopy\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import MOBOGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\n\nevaluator = Evaluator(function=evaluate_TNK)\nprint(tnk_vocs.dict())\n</pre> %reset -f  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  from copy import deepcopy  import pandas as pd import numpy as np import torch  from xopt import Xopt, Evaluator from xopt.generators.bayesian import MOBOGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs   evaluator = Evaluator(function=evaluate_TNK) print(tnk_vocs.dict()) <pre>{'variables': {'x1': [0.0, 3.14159], 'x2': [0.0, 3.14159]}, 'constraints': {'c1': ['GREATER_THAN', 0.0], 'c2': ['LESS_THAN', 0.5]}, 'objectives': {'y1': 'MINIMIZE', 'y2': 'MINIMIZE'}, 'constants': {'a': 'dummy_constant'}, 'linked_variables': {}}\n</pre> In\u00a0[2]: Copied! <pre>options = MOBOGenerator.default_options()\noptions.n_initial = (2,)\noptions.optim.num_restarts = 1\noptions.optim.raw_samples = 50\noptions.acq.proximal_lengthscales = [1.0, 1.0]\noptions.acq.reference_point = {\"y1\":1.5,\"y2\":1.5}\n\nprint(options)\ngenerator = MOBOGenerator(tnk_vocs, options)\n\nX = Xopt(generator=generator, evaluator=evaluator, vocs=tnk_vocs)\nX.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.75, 1.0]}))\nfor i in range(30):\n    print(i)\n    X.step()\n</pre> options = MOBOGenerator.default_options() options.n_initial = (2,) options.optim.num_restarts = 1 options.optim.raw_samples = 50 options.acq.proximal_lengthscales = [1.0, 1.0] options.acq.reference_point = {\"y1\":1.5,\"y2\":1.5}  print(options) generator = MOBOGenerator(tnk_vocs, options)  X = Xopt(generator=generator, evaluator=evaluator, vocs=tnk_vocs) X.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.75, 1.0]})) for i in range(30):     print(i)     X.step() <pre>optim=OptimOptions(num_restarts=1, raw_samples=50, sequential=True, max_travel_distances=None, use_turbo=False) acq=MOBOAcqOptions(proximal_lengthscales=[1.0, 1.0], use_transformed_proximal_weights=True, monte_carlo_samples=128, reference_point={'y1': 1.5, 'y2': 1.5}) model=ModelOptions(name='standard', custom_constructor=None, use_low_noise_prior=True, covar_modules={}, mean_modules={}) n_initial=(2,) use_cuda=False\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n</pre> In\u00a0[3]: Copied! <pre>X.generator.data\n</pre> X.generator.data Out[3]: x1 x2 y1 y2 c1 c2 some_array xopt_runtime xopt_error a 1 1.000000 0.750000 1.000000 0.750000 0.626888 0.312500 [1, 2, 3] 0.000054 False NaN 2 0.750000 1.000000 0.750000 1.000000 0.626888 0.312500 [1, 2, 3] 0.000013 False NaN 3 1.627468 1.880181 1.627468 1.880181 5.142953 3.176084 [1, 2, 3] 0.000168 False dummy_constant 4 0.430376 0.491122 0.430376 0.491122 -0.623055 0.004926 [1, 2, 3] 0.000064 False dummy_constant 5 0.929451 0.061216 0.929451 0.061216 -0.181934 0.376960 [1, 2, 3] 0.000045 False dummy_constant 6 0.014581 1.032709 0.014581 1.032709 -0.030760 0.519410 [1, 2, 3] 0.000045 False dummy_constant 7 0.406258 1.027154 0.406258 1.027154 0.123375 0.286679 [1, 2, 3] 0.000077 False dummy_constant 8 1.061598 0.317948 1.061598 0.317948 0.233721 0.348535 [1, 2, 3] 0.000076 False dummy_constant 9 0.803610 0.681991 0.803610 0.681991 0.084819 0.125300 [1, 2, 3] 0.000044 False dummy_constant 10 0.953267 0.398040 0.953267 0.398040 -0.032742 0.215847 [1, 2, 3] 0.000043 False dummy_constant 11 0.600323 0.831432 0.600323 0.831432 0.135258 0.119912 [1, 2, 3] 0.000043 False dummy_constant 12 1.014587 0.056755 1.014587 0.056755 -0.030014 0.461266 [1, 2, 3] 0.000045 False dummy_constant 13 0.220558 1.111481 0.220558 1.111481 0.384033 0.451997 [1, 2, 3] 0.000047 False dummy_constant 14 0.962376 0.440592 0.962376 0.440592 0.036983 0.217321 [1, 2, 3] 0.000047 False dummy_constant 15 0.887074 0.578866 0.887074 0.578866 0.220476 0.156047 [1, 2, 3] 0.000044 False dummy_constant 16 1.042538 0.062373 1.042538 0.062373 0.033107 0.485865 [1, 2, 3] 0.000043 False dummy_constant 17 0.950478 0.210692 0.950478 0.210692 0.041782 0.286630 [1, 2, 3] 0.000043 False dummy_constant 18 0.488221 0.867099 0.488221 0.867099 0.024599 0.134901 [1, 2, 3] 0.000063 False dummy_constant 19 0.660858 0.767693 0.660858 0.767693 -0.010676 0.097535 [1, 2, 3] 0.000042 False dummy_constant 20 0.073310 1.049635 0.073310 1.049635 0.063151 0.484163 [1, 2, 3] 0.000041 False dummy_constant 21 0.422113 0.935878 0.422113 0.935878 -0.033891 0.196056 [1, 2, 3] 0.000043 False dummy_constant 22 0.422933 0.935827 0.422933 0.935827 -0.032717 0.195884 [1, 2, 3] 0.000046 False dummy_constant 23 0.422520 0.934821 0.422520 0.934821 -0.034920 0.195072 [1, 2, 3] 0.000040 False dummy_constant 24 0.784836 0.590940 0.784836 0.590940 0.027232 0.089402 [1, 2, 3] 0.000044 False dummy_constant 25 0.847062 0.521445 0.847062 0.521445 0.072184 0.120912 [1, 2, 3] 0.000044 False dummy_constant 26 0.878652 0.484224 0.878652 0.484224 0.026858 0.143626 [1, 2, 3] 0.000044 False dummy_constant 27 0.878708 0.484250 0.878708 0.484250 0.026977 0.143668 [1, 2, 3] 0.000045 False dummy_constant 28 0.878751 0.484362 0.878751 0.484362 0.027279 0.143697 [1, 2, 3] 0.000059 False dummy_constant 29 0.979539 0.142058 0.979539 0.142058 0.046629 0.358081 [1, 2, 3] 0.000056 False dummy_constant 30 0.934495 0.215536 0.934495 0.215536 0.008191 0.269706 [1, 2, 3] 0.000045 False dummy_constant 31 0.953645 0.161588 0.953645 0.161588 0.025331 0.320316 [1, 2, 3] 0.000044 False dummy_constant 32 0.955659 0.163886 0.955659 0.163886 0.031279 0.320598 [1, 2, 3] 0.000061 False dummy_constant In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\ntheta = np.linspace(0, np.pi / 2)\nr = np.sqrt(1 + 0.1 * np.cos(16 * theta))\nx_1 = r * np.sin(theta)\nx_2_lower = r * np.cos(theta)\nx_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5\n\nz = np.zeros_like(x_1)\n\n# ax2.plot(x_1, x_2_lower,'r')\nax.fill_between(x_1, z, x_2_lower, fc=\"white\")\ncircle = plt.Circle(\n    (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\"\n)\nax.add_patch(circle)\nhistory = pd.concat(\n    [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False\n)\n\n\nax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\nax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\nax.set_xlim(0, 3.14)\nax.set_ylim(0, 3.14)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots()  theta = np.linspace(0, np.pi / 2) r = np.sqrt(1 + 0.1 * np.cos(16 * theta)) x_1 = r * np.sin(theta) x_2_lower = r * np.cos(theta) x_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5  z = np.zeros_like(x_1)  # ax2.plot(x_1, x_2_lower,'r') ax.fill_between(x_1, z, x_2_lower, fc=\"white\") circle = plt.Circle(     (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\" ) ax.add_patch(circle) history = pd.concat(     [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False )   ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\") ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")  ax.set_xlim(0, 3.14) ax.set_ylim(0, 3.14) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") ax.set_aspect(\"equal\") In\u00a0[5]: Copied! <pre>ax = history.plot(\"x1\", \"x2\")\nax.set_ylim(0, 3.14)\nax.set_xlim(0, 3.14)\nax.set_aspect(\"equal\")\n</pre> ax = history.plot(\"x1\", \"x2\") ax.set_ylim(0, 3.14) ax.set_xlim(0, 3.14) ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre># plot the acquisition function\nfrom xopt.generators.bayesian.objectives import feasibility\n\nbounds = generator.vocs.bounds\nmodel = generator.model\n\n# create mesh\nn = 200\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nacq_func = generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\n    ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")\n\n    feas = feasibility(pts.unsqueeze(1), model, generator.sampler, tnk_vocs).flatten()\n\n    fig2, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n\ncandidate = generator.generate(1)\nprint(candidate[[\"x1\", \"x2\"]].to_numpy())\nax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\")\n</pre> # plot the acquisition function from xopt.generators.bayesian.objectives import feasibility  bounds = generator.vocs.bounds model = generator.model  # create mesh n = 200 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  acq_func = generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")     fig.colorbar(c)     ax.set_title(\"Acquisition function\")      ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")     ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")      ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")      feas = feasibility(pts.unsqueeze(1), model, generator.sampler, tnk_vocs).flatten()      fig2, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(\"Feasible Region\")  candidate = generator.generate(1) print(candidate[[\"x1\", \"x2\"]].to_numpy()) ax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\") <pre>[[0.66867355 0.77070018]]\n</pre> Out[6]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f2179241040&gt;]</pre> In\u00a0[7]: Copied! <pre>%%time\ncandidate = generator.generate(1)\n</pre> %%time candidate = generator.generate(1) <pre>CPU times: user 1.32 s, sys: 75 ms, total: 1.4 s\nWall time: 705 ms\n</pre>"},{"location":"examples/multi_objective_bayes_opt/mobo/#multi-objective-bayesian-optimization","title":"Multi-objective Bayesian Optimization\u00b6","text":"<p>TNK function $n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/multi_objective_bayes_opt/mobo/#plot-results","title":"plot results\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mobo/#plot-path-through-input-space","title":"Plot path through input space\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mobo_from_yaml/","title":"Multi-objective Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre>%reset -f\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nfrom xopt import Xopt\nimport yaml\n\n\nYAML = \"\"\"\nxopt: {}\ngenerator:\n    name: mobo\n    n_initial: 5\n    optim:\n        num_restarts: 1\n        raw_samples: 20\n    acq:\n        reference_point: {y1: 1.5, y2: 1.5}\n        proximal_lengthscales: [1.5, 1.5]\n\nevaluator:\n    function: xopt.resources.test_functions.tnk.evaluate_TNK\n\nvocs:\n    variables:\n        x1: [0, 3.14159]\n        x2: [0, 3.14159]\n    objectives: {y1: MINIMIZE, y2: MINIMIZE}\n    constraints:\n        c1: [GREATER_THAN, 0]\n        c2: [LESS_THAN, 0.5]\n    constants: {a: dummy_constant}\n\n\"\"\"\n</pre> %reset -f  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import torch from xopt import Xopt import yaml   YAML = \"\"\" xopt: {} generator:     name: mobo     n_initial: 5     optim:         num_restarts: 1         raw_samples: 20     acq:         reference_point: {y1: 1.5, y2: 1.5}         proximal_lengthscales: [1.5, 1.5]  evaluator:     function: xopt.resources.test_functions.tnk.evaluate_TNK  vocs:     variables:         x1: [0, 3.14159]         x2: [0, 3.14159]     objectives: {y1: MINIMIZE, y2: MINIMIZE}     constraints:         c1: [GREATER_THAN, 0]         c2: [LESS_THAN, 0.5]     constants: {a: dummy_constant}  \"\"\" In\u00a0[2]: Copied! <pre>X = Xopt(config=yaml.safe_load(YAML))\nX.step()\nfor i in range(50):\n    print(i)\n    X.step()\n</pre> X = Xopt(config=yaml.safe_load(YAML)) X.step() for i in range(50):     print(i)     X.step() <pre>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n</pre> In\u00a0[3]: Copied! <pre>X.generator.data\n</pre> X.generator.data Out[3]: x1 x2 a y1 y2 c1 c2 some_array xopt_runtime xopt_error 1 1.198094 3.018017 dummy_constant 1.198094 3.018017 9.446648 6.827747 [1, 2, 3] 0.000054 False 2 0.317271 2.635401 dummy_constant 0.317271 2.635401 6.079929 4.593325 [1, 2, 3] 0.000014 False 3 2.692789 2.725837 dummy_constant 2.692789 2.725837 13.581773 9.762672 [1, 2, 3] 0.000010 False 4 0.718955 1.903640 dummy_constant 0.718955 1.903640 3.053242 2.018146 [1, 2, 3] 0.000009 False 5 2.611992 2.019072 dummy_constant 2.611992 2.019072 9.944140 6.768090 [1, 2, 3] 0.000008 False 6 2.613024 2.017321 dummy_constant 2.613024 2.017321 9.943332 6.767130 [1, 2, 3] 0.000037 False 7 2.611819 2.017226 dummy_constant 2.611819 2.017226 9.936375 6.761757 [1, 2, 3] 0.000039 False 8 2.612054 2.017779 dummy_constant 2.612054 2.017779 9.939704 6.764425 [1, 2, 3] 0.000060 False 9 2.611874 2.017635 dummy_constant 2.611874 2.017635 9.938183 6.763227 [1, 2, 3] 0.000044 False 10 2.612222 2.017510 dummy_constant 2.612222 2.017510 9.939633 6.764319 [1, 2, 3] 0.000061 False 11 2.612122 2.017669 dummy_constant 2.612122 2.017669 9.939671 6.764379 [1, 2, 3] 0.000040 False 12 2.613522 2.019478 dummy_constant 2.613522 2.019478 9.954041 6.775789 [1, 2, 3] 0.000042 False 13 2.612318 2.020757 dummy_constant 2.612318 2.020757 9.952164 6.774592 [1, 2, 3] 0.000042 False 14 2.610862 2.022622 dummy_constant 2.610862 2.022622 9.951067 6.774117 [1, 2, 3] 0.000043 False 15 2.611632 2.021843 dummy_constant 2.611632 2.021843 9.952410 6.774996 [1, 2, 3] 0.000045 False 16 2.611958 2.022309 dummy_constant 2.611958 2.022309 9.955925 6.777791 [1, 2, 3] 0.000043 False 17 2.612431 2.022278 dummy_constant 2.612431 2.022278 9.958408 6.779696 [1, 2, 3] 0.000041 False 18 2.612129 2.021363 dummy_constant 2.612129 2.021363 9.953363 6.775634 [1, 2, 3] 0.000061 False 19 2.611734 2.021568 dummy_constant 2.611734 2.021568 9.951956 6.774592 [1, 2, 3] 0.000039 False 20 2.611573 2.021318 dummy_constant 2.611573 2.021318 9.950144 6.773148 [1, 2, 3] 0.000041 False 21 2.612572 2.020833 dummy_constant 2.612572 2.020833 9.953839 6.775896 [1, 2, 3] 0.000041 False 22 2.612593 2.020860 dummy_constant 2.612593 2.020860 9.954049 6.776063 [1, 2, 3] 0.000043 False 23 2.611142 2.022437 dummy_constant 2.611142 2.022437 9.951916 6.774733 [1, 2, 3] 0.000044 False 24 2.610829 2.022411 dummy_constant 2.610829 2.022411 9.950107 6.773336 [1, 2, 3] 0.000037 False 25 2.610927 2.022184 dummy_constant 2.610927 2.022184 9.949802 6.773057 [1, 2, 3] 0.000041 False 26 2.609943 2.022498 dummy_constant 2.609943 2.022498 9.945566 6.769861 [1, 2, 3] 0.000040 False 27 2.610227 2.022580 dummy_constant 2.610227 2.022580 9.947425 6.771308 [1, 2, 3] 0.000074 False 28 2.610193 2.022503 dummy_constant 2.610193 2.022503 9.946955 6.770930 [1, 2, 3] 0.000036 False 29 2.610192 2.022486 dummy_constant 2.610192 2.022486 9.946882 6.770871 [1, 2, 3] 0.000040 False 30 2.610154 2.022484 dummy_constant 2.610154 2.022484 9.946672 6.770709 [1, 2, 3] 0.000036 False 31 2.610154 2.022487 dummy_constant 2.610154 2.022487 9.946678 6.770714 [1, 2, 3] 0.000040 False 32 2.611257 2.023604 dummy_constant 2.611257 2.023604 9.956871 6.778777 [1, 2, 3] 0.000060 False 33 2.611296 2.023629 dummy_constant 2.611296 2.023629 9.957175 6.779016 [1, 2, 3] 0.000039 False 34 2.611379 2.025359 dummy_constant 2.611379 2.025359 9.964039 6.784642 [1, 2, 3] 0.000042 False 35 2.613880 2.028296 dummy_constant 2.613880 2.028296 9.988665 6.804176 [1, 2, 3] 0.000037 False 36 2.613862 2.028348 dummy_constant 2.613862 2.028348 9.988762 6.804261 [1, 2, 3] 0.000042 False 37 2.615835 2.028200 dummy_constant 2.615835 2.028200 9.999058 6.812152 [1, 2, 3] 0.000040 False 38 2.616147 2.026705 dummy_constant 2.616147 2.026705 9.995229 6.808907 [1, 2, 3] 0.000037 False 39 2.616141 2.026708 dummy_constant 2.616141 2.026708 9.995205 6.808889 [1, 2, 3] 0.000089 False 40 2.616218 2.026611 dummy_constant 2.616218 2.026611 9.995268 6.808918 [1, 2, 3] 0.000039 False 41 2.616473 2.026667 dummy_constant 2.616473 2.026667 9.996882 6.810172 [1, 2, 3] 0.000039 False 42 2.616472 2.026652 dummy_constant 2.616472 2.026652 9.996822 6.810122 [1, 2, 3] 0.000039 False 43 2.615477 2.028983 dummy_constant 2.615477 2.028983 9.999994 6.813029 [1, 2, 3] 0.000040 False 44 2.615486 2.029090 dummy_constant 2.615486 2.029090 10.000446 6.813399 [1, 2, 3] 0.000053 False 45 2.615515 2.029086 dummy_constant 2.615515 2.029086 10.000588 6.813508 [1, 2, 3] 0.000040 False 46 2.614931 2.029069 dummy_constant 2.614931 2.029069 9.997315 6.810986 [1, 2, 3] 0.000042 False 47 2.613132 2.030843 dummy_constant 2.613132 2.030843 9.994014 6.808809 [1, 2, 3] 0.000034 False 48 2.613452 2.028715 dummy_constant 2.613452 2.028715 9.987872 6.803650 [1, 2, 3] 0.000034 False 49 2.612824 2.028512 dummy_constant 2.612824 2.028512 9.983663 6.800372 [1, 2, 3] 0.000055 False 50 2.611838 2.026743 dummy_constant 2.611838 2.026743 9.971688 6.790804 [1, 2, 3] 0.000041 False 51 2.614683 2.028291 dummy_constant 2.614683 2.028291 9.993061 6.807556 [1, 2, 3] 0.000043 False 52 2.614713 2.028314 dummy_constant 2.614713 2.028314 9.993311 6.807753 [1, 2, 3] 0.000043 False 53 2.614703 2.028330 dummy_constant 2.614703 2.028330 9.993319 6.807762 [1, 2, 3] 0.000046 False 54 2.614126 2.029353 dummy_constant 2.614126 2.029353 9.993943 6.808450 [1, 2, 3] 0.000044 False 55 2.614006 2.029166 dummy_constant 2.614006 2.029166 9.992586 6.807368 [1, 2, 3] 0.000044 False In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfig, ax = plt.subplots()\n\ntheta = np.linspace(0, np.pi / 2)\nr = np.sqrt(1 + 0.1 * np.cos(16 * theta))\nx_1 = r * np.sin(theta)\nx_2_lower = r * np.cos(theta)\nx_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5\n\nz = np.zeros_like(x_1)\n\n# ax2.plot(x_1, x_2_lower,'r')\nax.fill_between(x_1, z, x_2_lower, fc=\"white\")\ncircle = plt.Circle(\n    (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\"\n)\nax.add_patch(circle)\nhistory = pd.concat(\n    [X.data, X.vocs.feasibility_data(X.data)], axis=1, ignore_index=False\n)\n\n\nax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\nax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\nax.set_xlim(0, 3.14)\nax.set_ylim(0, 3.14)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\n</pre> import matplotlib.pyplot as plt import numpy as np import pandas as pd  fig, ax = plt.subplots()  theta = np.linspace(0, np.pi / 2) r = np.sqrt(1 + 0.1 * np.cos(16 * theta)) x_1 = r * np.sin(theta) x_2_lower = r * np.cos(theta) x_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5  z = np.zeros_like(x_1)  # ax2.plot(x_1, x_2_lower,'r') ax.fill_between(x_1, z, x_2_lower, fc=\"white\") circle = plt.Circle(     (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\" ) ax.add_patch(circle) history = pd.concat(     [X.data, X.vocs.feasibility_data(X.data)], axis=1, ignore_index=False )   ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\") ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")  ax.set_xlim(0, 3.14) ax.set_ylim(0, 3.14) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") ax.set_aspect(\"equal\") In\u00a0[5]: Copied! <pre>ax = history.plot(\"x1\", \"x2\")\nax.set_ylim(0, 3.14)\nax.set_xlim(0, 3.14)\nax.set_aspect(\"equal\")\n</pre> ax = history.plot(\"x1\", \"x2\") ax.set_ylim(0, 3.14) ax.set_xlim(0, 3.14) ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre># plot the acquisition function\nfrom xopt.generators.bayesian.objectives import feasibility\n\nbounds = X.generator.vocs.bounds\nmodel = X.generator.model\n\n# create mesh\nn = 100\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nacq_func = X.generator.get_acquisition(model)\nwith torch.no_grad():\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n    c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n    fig.colorbar(c)\n    ax.set_title(\"Acquisition function\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\n    ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\n    ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")\n\n    feas = feasibility(pts.unsqueeze(1), model, X.generator.sampler, X.vocs).flatten()\n\n    fig2, ax2 = plt.subplots(figsize=(8,8))\n    c = ax2.pcolor(xx, yy, feas.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(\"Feasible Region\")\n\ncandidate = X.generator.generate(1)\nprint(candidate[[\"x1\", \"x2\"]].to_numpy())\nax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\")\n</pre> # plot the acquisition function from xopt.generators.bayesian.objectives import feasibility  bounds = X.generator.vocs.bounds model = X.generator.model  # create mesh n = 100 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  acq_func = X.generator.get_acquisition(model) with torch.no_grad():     acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax = plt.subplots(figsize=(8,8))     c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")     fig.colorbar(c)     ax.set_title(\"Acquisition function\")      ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")     ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")      ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")      feas = feasibility(pts.unsqueeze(1), model, X.generator.sampler, X.vocs).flatten()      fig2, ax2 = plt.subplots(figsize=(8,8))     c = ax2.pcolor(xx, yy, feas.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(\"Feasible Region\")  candidate = X.generator.generate(1) print(candidate[[\"x1\", \"x2\"]].to_numpy()) ax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\") <pre>[[2.61434193 2.02903047]]\n</pre> Out[6]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f5a1c7dceb0&gt;]</pre>"},{"location":"examples/multi_objective_bayes_opt/mobo_from_yaml/#multi-objective-bayesian-optimization","title":"Multi-objective Bayesian Optimization\u00b6","text":"<p>TNK function $n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/multi_objective_bayes_opt/mobo_from_yaml/#plot-results","title":"plot results\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/mobo_from_yaml/#plot-path-through-input-space","title":"Plot path through input space\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/","title":"Multi-fidelity Multi-objective Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre>%reset -f\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom xopt import Xopt, Evaluator\nfrom xopt.generators.bayesian import MultiFidelityGenerator\nfrom xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs\n\nevaluator = Evaluator(function=evaluate_TNK)\nprint(tnk_vocs.dict())\n</pre> %reset -f  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import pandas as pd import numpy as np import torch  from xopt import Xopt, Evaluator from xopt.generators.bayesian import MultiFidelityGenerator from xopt.resources.test_functions.tnk import evaluate_TNK, tnk_vocs  evaluator = Evaluator(function=evaluate_TNK) print(tnk_vocs.dict()) <pre>{'variables': {'x1': [0.0, 3.14159], 'x2': [0.0, 3.14159]}, 'constraints': {'c1': ['GREATER_THAN', 0.0], 'c2': ['LESS_THAN', 0.5]}, 'objectives': {'y1': 'MINIMIZE', 'y2': 'MINIMIZE'}, 'constants': {'a': 'dummy_constant'}, 'linked_variables': {}}\n</pre> In\u00a0[2]: Copied! <pre>options = MultiFidelityGenerator.default_options()\noptions.acq.reference_point = {\"y1\":1.5,\"y2\":1.5}\n\n# set cost function according to approximate scaling of laser plasma accelerator\n# problem, see https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.5.013063\noptions.acq.cost_function = lambda s: s**3.5\noptions.optim.num_restarts = 8\noptions.optim.raw_samples = 512\n\ngenerator = MultiFidelityGenerator(tnk_vocs, options)\nX = Xopt(generator=generator, evaluator=evaluator, vocs=tnk_vocs)\n\n# evaluate at some explicit initial points\nX.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.75, 1.0],\"s\":[0.0,0.1]}))\n\nX.generator.options.dict()\n</pre> options = MultiFidelityGenerator.default_options() options.acq.reference_point = {\"y1\":1.5,\"y2\":1.5}  # set cost function according to approximate scaling of laser plasma accelerator # problem, see https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.5.013063 options.acq.cost_function = lambda s: s**3.5 options.optim.num_restarts = 8 options.optim.raw_samples = 512  generator = MultiFidelityGenerator(tnk_vocs, options) X = Xopt(generator=generator, evaluator=evaluator, vocs=tnk_vocs)  # evaluate at some explicit initial points X.evaluate_data(pd.DataFrame({\"x1\":[1.0, 0.75],\"x2\":[0.75, 1.0],\"s\":[0.0,0.1]}))  X.generator.options.dict() Out[2]: <pre>{'optim': {'num_restarts': 8,\n  'raw_samples': 512,\n  'sequential': True,\n  'max_travel_distances': None,\n  'use_turbo': False},\n 'acq': {'proximal_lengthscales': None,\n  'use_transformed_proximal_weights': True,\n  'monte_carlo_samples': 128,\n  'cost_function': &lt;function __main__.&lt;lambda&gt;(s)&gt;,\n  'reference_point': {'y1': 1.5, 'y2': 1.5}},\n 'model': {'name': 'standard',\n  'custom_constructor': None,\n  'use_low_noise_prior': True,\n  'covar_modules': {},\n  'mean_modules': {},\n  'fidelity_parameter': 's'},\n 'n_initial': 3,\n 'use_cuda': False}</pre> In\u00a0[3]: Copied! <pre>budget = 10\nwhile X.generator.calculate_total_cost() &lt; budget:\n    X.step()\n    print(f\"n_samples: {len(X.data)} \"\n          f\"budget used: {X.generator.calculate_total_cost():.4} \"\n          f\"hypervolume: {X.generator.calculate_hypervolume():.4}\")\n</pre> budget = 10 while X.generator.calculate_total_cost() &lt; budget:     X.step()     print(f\"n_samples: {len(X.data)} \"           f\"budget used: {X.generator.calculate_total_cost():.4} \"           f\"hypervolume: {X.generator.calculate_hypervolume():.4}\") <pre>n_samples: 3 budget used: 0.006088 hypervolume: 0.0375\nn_samples: 4 budget used: 0.01071 hypervolume: 0.09567\nn_samples: 5 budget used: 0.02071 hypervolume: 0.1251\nn_samples: 6 budget used: 0.03071 hypervolume: 0.1251\nn_samples: 7 budget used: 0.04193 hypervolume: 0.1251\nn_samples: 8 budget used: 0.05193 hypervolume: 0.1251\nn_samples: 9 budget used: 0.06193 hypervolume: 0.2282\nn_samples: 10 budget used: 0.07633 hypervolume: 0.2282\nn_samples: 11 budget used: 0.09154 hypervolume: 0.2965\nn_samples: 12 budget used: 0.1918 hypervolume: 0.2965\nn_samples: 13 budget used: 0.2487 hypervolume: 0.3891\nn_samples: 14 budget used: 0.3608 hypervolume: 0.3891\nn_samples: 15 budget used: 0.4368 hypervolume: 0.4681\nn_samples: 16 budget used: 0.6065 hypervolume: 0.4681\nn_samples: 17 budget used: 0.7261 hypervolume: 0.4681\nn_samples: 18 budget used: 0.8651 hypervolume: 0.559\nn_samples: 19 budget used: 1.029 hypervolume: 0.6177\nn_samples: 20 budget used: 1.05 hypervolume: 0.6429\nn_samples: 21 budget used: 1.342 hypervolume: 0.7135\nn_samples: 22 budget used: 1.89 hypervolume: 0.8132\nn_samples: 23 budget used: 2.587 hypervolume: 0.8786\nn_samples: 24 budget used: 3.587 hypervolume: 0.9562\nn_samples: 25 budget used: 3.645 hypervolume: 0.9845\nn_samples: 26 budget used: 3.79 hypervolume: 1.022\nn_samples: 27 budget used: 4.125 hypervolume: 1.08\nn_samples: 28 budget used: 5.047 hypervolume: 1.161\nn_samples: 29 budget used: 6.047 hypervolume: 1.187\nn_samples: 30 budget used: 6.053 hypervolume: 1.187\nn_samples: 31 budget used: 6.549 hypervolume: 1.208\nn_samples: 32 budget used: 7.549 hypervolume: 1.229\nn_samples: 33 budget used: 8.549 hypervolume: 1.263\nn_samples: 34 budget used: 9.549 hypervolume: 1.263\nn_samples: 35 budget used: 10.55 hypervolume: 1.276\n</pre> In\u00a0[4]: Copied! <pre>X.data\n</pre> X.data Out[4]: x1 x2 s y1 y2 c1 c2 some_array xopt_runtime xopt_error a 1 1.000000 0.750000 0.000000 1.000000 0.750000 0.626888 0.312500 [1, 2, 3] 0.000051 False NaN 2 0.750000 1.000000 0.100000 0.750000 1.000000 0.626888 0.312500 [1, 2, 3] 0.000012 False NaN 3 0.261923 1.590727 0.229282 0.261923 1.590727 1.685271 1.246366 [1, 2, 3] 0.000043 False dummy_constant 4 0.869085 0.839015 0.215139 0.869085 0.839015 0.363195 0.251155 [1, 2, 3] 0.000045 False dummy_constant 5 1.047180 0.727568 0.268270 1.047180 0.727568 0.721745 0.351194 [1, 2, 3] 0.000043 False dummy_constant 6 0.413103 0.667884 0.268267 0.413103 0.667884 -0.298653 0.035736 [1, 2, 3] 0.000045 False dummy_constant 7 0.258833 0.156077 0.277301 0.258833 0.156077 -0.834987 0.176445 [1, 2, 3] 0.000057 False dummy_constant 8 0.178849 0.908665 0.268263 0.178849 0.908665 -0.042392 0.270145 [1, 2, 3] 0.000046 False dummy_constant 9 0.130792 0.974786 0.268270 0.130792 0.974786 0.020710 0.361737 [1, 2, 3] 0.000048 False dummy_constant 10 0.906365 0.349337 0.297715 0.906365 0.349337 -0.148685 0.187832 [1, 2, 3] 0.000043 False dummy_constant 11 0.975833 0.400594 0.302411 0.975833 0.400594 0.012853 0.236298 [1, 2, 3] 0.000040 False dummy_constant 12 0.290820 0.218593 0.518358 0.290820 0.218593 -0.804547 0.122946 [1, 2, 3] 0.000047 False dummy_constant 13 0.818295 0.668832 0.440728 0.818295 0.668832 0.120131 0.129816 [1, 2, 3] 0.000046 False dummy_constant 14 1.080531 0.005359 0.535186 1.080531 0.005359 0.067890 0.581686 [1, 2, 3] 0.000077 False dummy_constant 15 1.068474 0.183788 0.478989 1.068474 0.183788 0.266881 0.423152 [1, 2, 3] 0.000042 False dummy_constant 16 0.854375 0.353420 0.602396 0.854375 0.353420 -0.245134 0.147067 [1, 2, 3] 0.000045 False dummy_constant 17 0.363463 0.902525 0.545145 0.363463 0.902525 -0.152103 0.180669 [1, 2, 3] 0.000046 False dummy_constant 18 1.025577 0.084373 0.569058 1.025577 0.084373 0.033464 0.448977 [1, 2, 3] 0.000124 False dummy_constant 19 0.726887 0.783897 0.596587 0.726887 0.783897 0.060523 0.132075 [1, 2, 3] 0.000044 False dummy_constant 20 0.075677 1.037686 0.329410 0.075677 1.037686 0.043025 0.469156 [1, 2, 3] 0.000059 False dummy_constant 21 0.889963 0.583179 0.703638 0.889963 0.583179 0.231104 0.158990 [1, 2, 3] 0.000043 False dummy_constant 22 0.799651 0.615219 0.842003 0.799651 0.615219 0.066155 0.103066 [1, 2, 3] 0.000035 False dummy_constant 23 1.071915 0.303737 0.902130 1.071915 0.303737 0.270279 0.365606 [1, 2, 3] 0.000041 False dummy_constant 24 0.903856 0.499419 1.000000 0.903856 0.499419 0.088462 0.163100 [1, 2, 3] 0.000041 False dummy_constant 25 0.150077 1.069817 0.443577 0.150077 1.069817 0.228278 0.447138 [1, 2, 3] 0.000044 False dummy_constant 26 0.119120 1.058493 0.575284 0.119120 1.058493 0.156641 0.456984 [1, 2, 3] 0.000047 False dummy_constant 27 0.077182 1.041974 0.731892 0.077182 1.041974 0.053853 0.472511 [1, 2, 3] 0.000043 False dummy_constant 28 0.136624 1.042737 0.977221 0.136624 1.042737 0.155109 0.426605 [1, 2, 3] 0.000045 False dummy_constant 29 0.207377 0.999781 1.000000 0.207377 0.999781 0.141714 0.335410 [1, 2, 3] 0.000041 False dummy_constant 30 0.209552 0.927814 0.224783 0.209552 0.927814 -0.003636 0.267385 [1, 2, 3] 0.000076 False dummy_constant 31 1.062909 0.114027 0.818675 1.062909 0.114027 0.156644 0.465841 [1, 2, 3] 0.000051 False dummy_constant 32 0.632304 0.845276 1.000000 0.632304 0.845276 0.180208 0.136720 [1, 2, 3] 0.000046 False dummy_constant 33 1.042377 0.082155 1.000000 1.042377 0.082155 0.062569 0.468767 [1, 2, 3] 0.000062 False dummy_constant 34 0.268661 0.947264 1.000000 0.268661 0.947264 -0.001857 0.253563 [1, 2, 3] 0.000063 False dummy_constant 35 0.501680 0.890507 1.000000 0.501680 0.890507 0.079422 0.152499 [1, 2, 3] 0.000043 False dummy_constant In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\ntheta = np.linspace(0, np.pi / 2)\nr = np.sqrt(1 + 0.1 * np.cos(16 * theta))\nx_1 = r * np.sin(theta)\nx_2_lower = r * np.cos(theta)\nx_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5\n\nz = np.zeros_like(x_1)\n\n# ax2.plot(x_1, x_2_lower,'r')\nax.fill_between(x_1, z, x_2_lower, fc=\"white\")\ncircle = plt.Circle(\n    (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\"\n)\nax.add_patch(circle)\nhistory = pd.concat(\n    [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False\n)\n\nax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\nax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\nax.set_xlim(0, 3.14)\nax.set_ylim(0, 3.14)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_aspect(\"equal\")\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots()  theta = np.linspace(0, np.pi / 2) r = np.sqrt(1 + 0.1 * np.cos(16 * theta)) x_1 = r * np.sin(theta) x_2_lower = r * np.cos(theta) x_2_upper = (0.5 - (x_1 - 0.5) ** 2) ** 0.5 + 0.5  z = np.zeros_like(x_1)  # ax2.plot(x_1, x_2_lower,'r') ax.fill_between(x_1, z, x_2_lower, fc=\"white\") circle = plt.Circle(     (0.5, 0.5), 0.5 ** 0.5, color=\"r\", alpha=0.25, zorder=0, label=\"Valid Region\" ) ax.add_patch(circle) history = pd.concat(     [X.data, tnk_vocs.feasibility_data(X.data)], axis=1, ignore_index=False )  ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\") ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")  ax.set_xlim(0, 3.14) ax.set_ylim(0, 3.14) ax.set_xlabel(\"x1\") ax.set_ylabel(\"x2\") ax.set_aspect(\"equal\") In\u00a0[6]: Copied! <pre>ax = history.hist([\"x1\", \"x2\", \"s\"],bins=20)\n</pre> ax = history.hist([\"x1\", \"x2\", \"s\"],bins=20) In\u00a0[7]: Copied! <pre>history.plot(y=[\"x1\", \"x2\", \"s\"])\n</pre> history.plot(y=[\"x1\", \"x2\", \"s\"]) Out[7]: <pre>&lt;Axes: &gt;</pre> In\u00a0[8]: Copied! <pre># plot the acquisition function\nfrom xopt.generators.bayesian.objectives import feasibility\n\nbounds = generator.vocs.bounds\nmodel = generator.model\n\n# create mesh over non-fidelity parameters\nn = 100\nx = torch.linspace(*bounds.T[1], n)\ny = torch.linspace(*bounds.T[2], n)\nxx, yy = torch.meshgrid(x, y)\n\n# plot function(s) at a single fidelity parameter\nfidelities = [0.0, 0.5, 1.0]\nfor fidelity in fidelities:\n    pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n    pts = torch.cat((torch.ones(pts.shape[0],1)*fidelity, pts), dim=-1)\n\n    acq_func = generator.get_acquisition(model)\n    with torch.no_grad():\n        acq_pts = pts.unsqueeze(1)\n        acq = acq_func(acq_pts)\n\n        fig, ax = plt.subplots()\n        c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n        fig.colorbar(c)\n        ax.set_title(f\"Acquisition function - s: {fidelity}\")\n\n        ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")\n        ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")\n\n        ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")\n\n\ncandidate = generator.generate(1)\nprint(candidate[[\"x1\", \"x2\"]].to_numpy())\nax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\")\n</pre> # plot the acquisition function from xopt.generators.bayesian.objectives import feasibility  bounds = generator.vocs.bounds model = generator.model  # create mesh over non-fidelity parameters n = 100 x = torch.linspace(*bounds.T[1], n) y = torch.linspace(*bounds.T[2], n) xx, yy = torch.meshgrid(x, y)  # plot function(s) at a single fidelity parameter fidelities = [0.0, 0.5, 1.0] for fidelity in fidelities:     pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()     pts = torch.cat((torch.ones(pts.shape[0],1)*fidelity, pts), dim=-1)      acq_func = generator.get_acquisition(model)     with torch.no_grad():         acq_pts = pts.unsqueeze(1)         acq = acq_func(acq_pts)          fig, ax = plt.subplots()         c = ax.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")         fig.colorbar(c)         ax.set_title(f\"Acquisition function - s: {fidelity}\")          ax.plot(*history[[\"x1\", \"x2\"]][history[\"feasible\"]].to_numpy().T, \".C1\")         ax.plot(*history[[\"x1\", \"x2\"]][~history[\"feasible\"]].to_numpy().T, \".C2\")          ax.plot(*history[[\"x1\", \"x2\"]].to_numpy()[-1].T, \"+\")   candidate = generator.generate(1) print(candidate[[\"x1\", \"x2\"]].to_numpy()) ax.plot(*candidate[[\"x1\", \"x2\"]].to_numpy()[0], \"o\") <pre>[[0.70621641 0.72569783]]\n</pre> Out[8]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f6227f16910&gt;]</pre> In\u00a0[9]: Copied! <pre># examine lengthscale of the first objective\nlist(model.models[0].named_parameters())\n</pre> # examine lengthscale of the first objective list(model.models[0].named_parameters()) Out[9]: <pre>[('likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-25.8965], dtype=torch.float64, requires_grad=True)),\n ('mean_module.raw_constant',\n  Parameter containing:\n  tensor(-0.1042, dtype=torch.float64, requires_grad=True)),\n ('covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(0.8571, dtype=torch.float64, requires_grad=True)),\n ('covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[0.9434, 1.4519, 1.5700]], dtype=torch.float64, requires_grad=True))]</pre> In\u00a0[10]: Copied! <pre># dump results to fiel\nX.options.dump_file = \"results.yaml\"\nX.dump_state()\n</pre> # dump results to fiel X.options.dump_file = \"results.yaml\" X.dump_state() In\u00a0[12]: Copied! <pre>\n</pre>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#multi-fidelity-multi-objective-bayesian-optimization","title":"Multi-fidelity Multi-objective Bayesian Optimization\u00b6","text":"<p>Here we attempt to solve for the constrained Pareto front of the TNK multi-objective optimization problem using Multi-Fidelity Multi-Objective Bayesian optimization. For simplicity we assume that the objective and constraint functions at lower fidelities is exactly equal to the functions at higher fidelities (this is obviously not a requirement, although for the best results lower fidelity calculations should correlate with higher fidelity ones). The algorithm should learn this relationship and use information gathered at lower fidelities to gather samples to improve the hypervolume of the Pareto front at the maximum fidelity.</p> <p>TNK function $n=2$ variables: $x_i \\in [0, \\pi], i=1,2$</p> <p>Objectives:</p> <ul> <li>$f_i(x) = x_i$</li> </ul> <p>Constraints:</p> <ul> <li>$g_1(x) = -x_1^2 -x_2^2 + 1 + 0.1 \\cos\\left(16 \\arctan \\frac{x_1}{x_2}\\right) \\le 0$</li> <li>$g_2(x) = (x_1 - 1/2)^2 + (x_2-1/2)^2 \\le 0.5$</li> </ul>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#set-up-the-multi-fidelity-multi-objective-optimization-algorithm","title":"Set up the Multi-Fidelity Multi-objective optimization algorithm\u00b6","text":"<p>Here we create the Multi-Fidelity generator object which can solve both single and multi-objective optimization problems depending on the number of objectives in VOCS. We specify a cost function as a function of fidelity parameter $s=[0,1]$ as $C(s) = s^{3.5}$ as an example from a real life multi-fidelity simulation problem.</p>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#run-optimization-routine","title":"Run optimization routine\u00b6","text":"<p>Instead of ending the optimization routine after an explict number of samples we end optimization once a given optimization budget has been exceeded. WARNING: This will slightly exceed the given budget</p>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#show-results","title":"Show results\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#plot-results","title":"Plot results\u00b6","text":"<p>Here we plot the resulting observations in input space, colored by feasibility (neglecting the fact that these data points are at varying fidelities).</p>"},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#plot-path-through-input-space","title":"Plot path through input space\u00b6","text":""},{"location":"examples/multi_objective_bayes_opt/multi_fidelity_mobo/#plot-the-acqusisition-function","title":"Plot the acqusisition function\u00b6","text":"<p>Here we plot the acquisition function at a small set of fidelities $[0, 0.5, 1.0]$.</p>"},{"location":"examples/rcds/rcds/","title":"RCDS","text":"In\u00a0[1]: Copied! <pre># If you encounter the \"Initializing libomp.dylib, but found libomp.dylib already initialized.\" error\n# Please run this cell\n\nimport os\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n</pre> # If you encounter the \"Initializing libomp.dylib, but found libomp.dylib already initialized.\" error # Please run this cell  import os  os.environ['KMP_DUPLICATE_LIB_OK']='True' In\u00a0[2]: Copied! <pre>import time\nimport numpy as np\nfrom xopt.generators.rcds.rcds import RCDSGenerator\nfrom xopt.vocs import VOCS\nfrom xopt.evaluator import Evaluator\nfrom xopt import Xopt\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> import time import numpy as np from xopt.generators.rcds.rcds import RCDSGenerator from xopt.vocs import VOCS from xopt.evaluator import Evaluator from xopt import Xopt import pandas as pd from tqdm.auto import tqdm import warnings warnings.filterwarnings(\"ignore\") In\u00a0[3]: Copied! <pre>def f_RCDS_minimize(input_dict):\n    p = []\n    for i in range(2):\n        p.append(input_dict[f'p{i}'])\n    \n    obj = np.linalg.norm(p)\n    outcome_dict = {'f': obj}\n    \n    return outcome_dict\n</pre> def f_RCDS_minimize(input_dict):     p = []     for i in range(2):         p.append(input_dict[f'p{i}'])          obj = np.linalg.norm(p)     outcome_dict = {'f': obj}          return outcome_dict In\u00a0[4]: Copied! <pre>YAML = \"\"\"\nxopt:\n    max_evaluations: 400\ngenerator:\n    name: rcds\n    x0: null\n    init_mat: null\n    noise: 0.00001\n    step: 0.01\n    tol: 0.00001\nevaluator:\n    function: __main__.f_RCDS_minimize\nvocs:\n    variables:\n        p0: [0, 1]\n        p1: [0, 1]\n    objectives:\n        f: MINIMIZE\n\"\"\"\n\nX = Xopt(YAML)\nX\n</pre> YAML = \"\"\" xopt:     max_evaluations: 400 generator:     name: rcds     x0: null     init_mat: null     noise: 0.00001     step: 0.01     tol: 0.00001 evaluator:     function: __main__.f_RCDS_minimize vocs:     variables:         p0: [0, 1]         p1: [0, 1]     objectives:         f: MINIMIZE \"\"\"  X = Xopt(YAML) X Out[4]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g0676a82\nData size: 0\nConfig as YAML:\nxopt: {asynch: false, strict: false, dump_file: null, max_evaluations: 400}\ngenerator: {name: rcds, x0: null, init_mat: null, noise: 1.0e-05, step: 0.01, tol: 1.0e-05}\nevaluator:\n  function: __main__.f_RCDS_minimize\n  max_workers: 1\n  function_kwargs: {}\n  vectorized: false\nvocs:\n  variables:\n    p0: [0.0, 1.0]\n    p1: [0.0, 1.0]\n  constraints: {}\n  objectives: {f: MINIMIZE}\n  constants: {}\n  linked_variables: {}\n</pre> In\u00a0[5]: Copied! <pre>X.run()\n</pre> X.run() <p>Now you can go directly to the Visualization section and check out the results.</p> In\u00a0[6]: Copied! <pre>n_var = 2\n</pre> n_var = 2 In\u00a0[7]: Copied! <pre>variables = {}\nfor i in range(n_var):\n    variables[f'p{i}'] = [0, 1]\n\nvocs = VOCS(\n    variables=variables,\n    objectives={'f': 'MINIMIZE'},\n)\n</pre> variables = {} for i in range(n_var):     variables[f'p{i}'] = [0, 1]  vocs = VOCS(     variables=variables,     objectives={'f': 'MINIMIZE'}, ) In\u00a0[8]: Copied! <pre>vocs\n</pre> vocs Out[8]: <pre>VOCS(variables={'p0': [0.0, 1.0], 'p1': [0.0, 1.0]}, constraints={}, objectives={'f': 'MINIMIZE'}, constants={}, linked_variables={})</pre> In\u00a0[9]: Copied! <pre>evaluator = Evaluator(function=f_RCDS_minimize)\n</pre> evaluator = Evaluator(function=f_RCDS_minimize) In\u00a0[10]: Copied! <pre>options = RCDSGenerator.default_options()\n</pre> options = RCDSGenerator.default_options() In\u00a0[11]: Copied! <pre>options.dict()\n</pre> options.dict() Out[11]: <pre>{'x0': None, 'init_mat': None, 'noise': 1e-05, 'step': 0.01, 'tol': 1e-05}</pre> In\u00a0[12]: Copied! <pre>generator = RCDSGenerator(vocs, options)\n</pre> generator = RCDSGenerator(vocs, options) In\u00a0[13]: Copied! <pre>X = Xopt(vocs=vocs, evaluator=evaluator, generator=generator)\n</pre> X = Xopt(vocs=vocs, evaluator=evaluator, generator=generator) In\u00a0[14]: Copied! <pre>max_eval = 400\n\nfor i in tqdm(range(max_eval)):\n    X.step()\n</pre> max_eval = 400  for i in tqdm(range(max_eval)):     X.step() In\u00a0[15]: Copied! <pre>X.data.plot(y='f')\n</pre> X.data.plot(y='f') Out[15]: <pre>&lt;Axes: &gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/rcds/rcds/#rcds-optimization","title":"RCDS Optimization\u00b6","text":"<p>In this example we demonstrate RCDS optimization.</p>"},{"location":"examples/rcds/rcds/#rcds-test-problem","title":"RCDS test problem\u00b6","text":"<p>This test problem is a 2-D quadratic function.</p>"},{"location":"examples/rcds/rcds/#run-rcds-on-the-test-problem-yaml-method","title":"Run RCDS on the test problem (YAML method)\u00b6","text":""},{"location":"examples/rcds/rcds/#run-rcds-on-the-test-problem-api-method","title":"Run RCDS on the test problem (API method)\u00b6","text":""},{"location":"examples/rcds/rcds/#vocs","title":"VOCS\u00b6","text":"<p>We'll set the bounds for all the variables pi to [0, 1].</p>"},{"location":"examples/rcds/rcds/#evaluator","title":"Evaluator\u00b6","text":""},{"location":"examples/rcds/rcds/#generator","title":"Generator\u00b6","text":""},{"location":"examples/rcds/rcds/#run-the-optimization","title":"Run the optimization\u00b6","text":""},{"location":"examples/rcds/rcds/#visualization","title":"Visualization\u00b6","text":""},{"location":"examples/scipy/neldermead/","title":"Nelder-Mead Generator adapted from SciPy","text":"In\u00a0[1]: Copied! <pre>from xopt import Xopt\nimport numpy as np\n\n#from xopt import output_notebook\n#output_notebook()\n\nimport matplotlib.pyplot as plt\n</pre> from xopt import Xopt import numpy as np  #from xopt import output_notebook #output_notebook()  import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre>YAML = \"\"\"\nxopt: {}\ngenerator:\n  name: neldermead\n  initial_point: {x0: -1, x1: -1}\n  adaptive: true\n  xatol: 0.0001\n  fatol: 0.0001  \nevaluator:\n  function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock\nvocs:\n  variables:\n    x0: [-5, 5]\n    x1: [-5, 5]\n  objectives: {y: MINIMIZE}\n\"\"\"\nX = Xopt(YAML)\n</pre> YAML = \"\"\" xopt: {} generator:   name: neldermead   initial_point: {x0: -1, x1: -1}   adaptive: true   xatol: 0.0001   fatol: 0.0001   evaluator:   function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock vocs:   variables:     x0: [-5, 5]     x1: [-5, 5]   objectives: {y: MINIMIZE} \"\"\" X = Xopt(YAML) In\u00a0[3]: Copied! <pre>XMIN = [1,1] # True minimum\n</pre> XMIN = [1,1] # True minimum In\u00a0[4]: Copied! <pre># Test \nX.evaluate({\"x0\":XMIN[0], \"x1\": XMIN[1]})\n</pre> # Test  X.evaluate({\"x0\":XMIN[0], \"x1\": XMIN[1]}) Out[4]: <pre>{'y': 0, 'xopt_runtime': 1.650000012887176e-05, 'xopt_error': False}</pre> In\u00a0[5]: Copied! <pre>X.run()\nX.data\n</pre> X.run() X.data Out[5]: x0 x1 y xopt_runtime xopt_error 1 -1.000000 -1.000000 4.040000e+02 0.000017 False 2 -1.050000 -1.000000 4.462531e+02 0.000015 False 3 -1.000000 -1.050000 4.242500e+02 0.000012 False 4 -0.950000 -1.050000 3.850281e+02 0.000012 False 5 -0.900000 -1.075000 3.589325e+02 0.000011 False ... ... ... ... ... ... 122 0.999877 0.999764 2.587916e-08 0.000016 False 123 0.999999 0.999995 5.309344e-10 0.000017 False 124 1.000045 1.000097 7.751675e-09 0.000029 False 125 0.999963 0.999925 1.412126e-09 0.000015 False 126 0.999963 0.999925 1.412126e-09 0.000014 False <p>126 rows \u00d7 5 columns</p> In\u00a0[6]: Copied! <pre># Evaluation progression\nX.data['y'].plot(marker='.')\nplt.yscale('log')\nplt.xlabel('iteration')\nplt.ylabel('Rosenbrock value')\n</pre> # Evaluation progression X.data['y'].plot(marker='.') plt.yscale('log') plt.xlabel('iteration') plt.ylabel('Rosenbrock value') Out[6]: <pre>Text(0, 0.5, 'Rosenbrock value')</pre> In\u00a0[7]: Copied! <pre># Minimum\ndict(X.data.iloc[X.data[\"y\"].argmin()])\n</pre> # Minimum dict(X.data.iloc[X.data[\"y\"].argmin()]) Out[7]: <pre>{'x0': 0.9999988592114838,\n 'x1': 0.9999954170486077,\n 'y': 5.309343918637161e-10,\n 'xopt_runtime': 1.6899999991437653e-05,\n 'xopt_error': False}</pre> In\u00a0[8]: Copied! <pre>from xopt.resources.test_functions.rosenbrock import rosenbrock\n</pre> from xopt.resources.test_functions.rosenbrock import rosenbrock In\u00a0[9]: Copied! <pre>fig, ax = plt.subplots(figsize=(8,8))\n\nXgrid, Ygrid = np.meshgrid(np.linspace(-2, 2, 201), np.linspace(-2, 2, 201) )\n\nZgrid = np.vectorize(lambda x, y: rosenbrock([x, y]))(Xgrid, Ygrid)\nZgrid = np.log(Zgrid+1)\n\nax.pcolormesh(Xgrid, Ygrid, Zgrid)\nax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black')\nax.set_xlabel('x0')\nax.set_ylabel('x1')\n\n\n# Add all evaluations\nax.plot(X.data[\"x0\"], X.data[\"x1\"], color='red', alpha=0.5, marker='.')\nax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\")\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\n#plt.legend()\nax.set_title(\"Xopt's Nelder-Mead progression\")\n</pre> fig, ax = plt.subplots(figsize=(8,8))  Xgrid, Ygrid = np.meshgrid(np.linspace(-2, 2, 201), np.linspace(-2, 2, 201) )  Zgrid = np.vectorize(lambda x, y: rosenbrock([x, y]))(Xgrid, Ygrid) Zgrid = np.log(Zgrid+1)  ax.pcolormesh(Xgrid, Ygrid, Zgrid) ax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black') ax.set_xlabel('x0') ax.set_ylabel('x1')   # Add all evaluations ax.plot(X.data[\"x0\"], X.data[\"x1\"], color='red', alpha=0.5, marker='.') ax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\") ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) #plt.legend() ax.set_title(\"Xopt's Nelder-Mead progression\") Out[9]: <pre>Text(0.5, 1.0, \"Xopt's Nelder-Mead progression\")</pre> In\u00a0[10]: Copied! <pre># Manually step the algorithm and collect simplexes\nX = Xopt(YAML)\nsimplexes = []\nwhile not X.is_done:\n    X.step()\n    simplexes.append(X.generator.simplex)\n</pre> # Manually step the algorithm and collect simplexes X = Xopt(YAML) simplexes = [] while not X.is_done:     X.step()     simplexes.append(X.generator.simplex)  In\u00a0[11]: Copied! <pre>def plot_simplex(simplex, ax=None):\n    x0 = simplex[\"x0\"]\n    x1 = simplex[\"x1\"]\n    x0 = np.append(x0, x0[0])\n    x1 = np.append(x1, x1[0])\n    ax.plot(x0, x1)\n\nfig, ax = plt.subplots(figsize=(8,8))\nax.pcolormesh(Xgrid, Ygrid, Zgrid)\n#ax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black')\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)  \nax.set_xlabel('x0')\nax.set_ylabel('x1')\nax.set_title('Nelder-Mead simplex progression')\n\nax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\")\n\nfor simplex in simplexes:\n    plot_simplex(simplex, ax)\n</pre> def plot_simplex(simplex, ax=None):     x0 = simplex[\"x0\"]     x1 = simplex[\"x1\"]     x0 = np.append(x0, x0[0])     x1 = np.append(x1, x1[0])     ax.plot(x0, x1)  fig, ax = plt.subplots(figsize=(8,8)) ax.pcolormesh(Xgrid, Ygrid, Zgrid) #ax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black') ax.set_xlim(-2, 2) ax.set_ylim(-2, 2)   ax.set_xlabel('x0') ax.set_ylabel('x1') ax.set_title('Nelder-Mead simplex progression')  ax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\")  for simplex in simplexes:     plot_simplex(simplex, ax) In\u00a0[12]: Copied! <pre>from scipy.optimize import fmin\n</pre> from scipy.optimize import fmin In\u00a0[13]: Copied! <pre>result = fmin(rosenbrock, [-1, -1])\nresult\n</pre> result = fmin(rosenbrock, [-1, -1]) result <pre>Optimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 67\n         Function evaluations: 125\n</pre> Out[13]: <pre>array([0.99999886, 0.99999542])</pre> In\u00a0[14]: Copied! <pre>X = Xopt(YAML)\n</pre> X = Xopt(YAML) In\u00a0[15]: Copied! <pre>X.run()\n# Almost exactly the same number evaluations. \nlen(X.data)\n</pre> X.run() # Almost exactly the same number evaluations.  len(X.data)  Out[15]: <pre>126</pre> In\u00a0[16]: Copied! <pre># results are the same\nxbest = X.data.iloc[X.data[\"y\"].argmin()]\nxbest['x0'] == result[0], xbest['x1'] == result[1]\n</pre> # results are the same xbest = X.data.iloc[X.data[\"y\"].argmin()] xbest['x0'] == result[0], xbest['x1'] == result[1] Out[16]: <pre>(True, True)</pre> In\u00a0[17]: Copied! <pre>from xopt.generators.scipy.neldermead import NelderMeadGenerator\nfrom xopt import Evaluator, VOCS\n</pre> from xopt.generators.scipy.neldermead import NelderMeadGenerator from xopt import Evaluator, VOCS In\u00a0[18]: Copied! <pre># default options\nNelderMeadGenerator.default_options()\n</pre> # default options NelderMeadGenerator.default_options() Out[18]: <pre>NelderMeadOptions(initial_point=None, initial_simplex=None, adaptive=True, xatol=0.0001, fatol=0.0001)</pre> In\u00a0[19]: Copied! <pre>Xbest = [33, 44]\n\ndef f(inputs, verbose=False):\n\n    if verbose:\n        print(f'evaluate f({x})')\n    x0 = inputs[\"x0\"]\n    x1 = inputs[\"x1\"]\n    \n    #if x0 &lt; 10:\n    #    raise ValueError('test XXXX')\n\n    y = (x0-Xbest[0])**2  + (x1-Xbest[1])**2\n\n    return {\"y\":y}\n\nev = Evaluator(function=f)\nvocs = VOCS(variables={\"x0\": [-100, 100], \"x1\": [-100,100]}, objectives={\"y\":\"MINIMIZE\"})\nvocs.json()\n</pre> Xbest = [33, 44]  def f(inputs, verbose=False):      if verbose:         print(f'evaluate f({x})')     x0 = inputs[\"x0\"]     x1 = inputs[\"x1\"]          #if x0 &lt; 10:     #    raise ValueError('test XXXX')      y = (x0-Xbest[0])**2  + (x1-Xbest[1])**2      return {\"y\":y}  ev = Evaluator(function=f) vocs = VOCS(variables={\"x0\": [-100, 100], \"x1\": [-100,100]}, objectives={\"y\":\"MINIMIZE\"}) vocs.json() Out[19]: <pre>'{\"variables\":{\"x0\":[-100.0,100.0],\"x1\":[-100.0,100.0]},\"constraints\":{},\"objectives\":{\"y\":\"MINIMIZE\"},\"constants\":{},\"linked_variables\":{}}'</pre> In\u00a0[20]: Copied! <pre># check output\nf(vocs.random_inputs())\n</pre> # check output f(vocs.random_inputs()) Out[20]: <pre>{'y': 2076.341258173415}</pre> In\u00a0[21]: Copied! <pre>G = NelderMeadGenerator(vocs)\nG.generate(1)\n</pre> G = NelderMeadGenerator(vocs) G.generate(1) Out[21]: <pre>[{'x0': -11.955606907138659, 'x1': -62.828701876637695}]</pre> In\u00a0[22]: Copied! <pre># This will throw an exception \ntry:\n    G.generate(1)\nexcept Exception as ex:\n    print(ex)\n</pre> # This will throw an exception  try:     G.generate(1) except Exception as ex:     print(ex) <pre>Generation is locked via ._lock. Please call `add_data` before any further generate(1)\n</pre> In\u00a0[23]: Copied! <pre># This will unlock\nG.add_data([ev.evaluate(G.inputs[0])])\nG.generate(1)\n</pre> # This will unlock G.add_data([ev.evaluate(G.inputs[0])]) G.generate(1) Out[23]: <pre>[{'x0': -12.553387252495591, 'x1': -62.828701876637695}]</pre> In\u00a0[24]: Copied! <pre># Create Xopt object\nX = Xopt(evaluator=ev, vocs=vocs, generator=NelderMeadGenerator(vocs=vocs))\n\n# Optional: give an initial pioint\nX.generator.initial_point = {'x0':0, 'x1':0}\n</pre> # Create Xopt object X = Xopt(evaluator=ev, vocs=vocs, generator=NelderMeadGenerator(vocs=vocs))  # Optional: give an initial pioint X.generator.initial_point = {'x0':0, 'x1':0} In\u00a0[25]: Copied! <pre>X.run()\n</pre> X.run() In\u00a0[26]: Copied! <pre># This shows the latest simplex\nX.generator.simplex\n</pre> # This shows the latest simplex X.generator.simplex Out[26]: <pre>{'x0': array([32.99996111, 32.99996171, 33.00002688]),\n 'x1': array([44.00000851, 44.00006811, 44.00003045])}</pre> In\u00a0[27]: Copied! <pre>X.data['y'].plot()\nplt.yscale('log')\n</pre> X.data['y'].plot() plt.yscale('log') In\u00a0[28]: Copied! <pre>fig, ax = plt.subplots()\nX.data.plot('x0', 'x1', ax=ax, color='black', alpha=0.5)\nax.scatter(Xbest[0], Xbest[1], marker='x', color='red')\n</pre> fig, ax = plt.subplots() X.data.plot('x0', 'x1', ax=ax, color='black', alpha=0.5) ax.scatter(Xbest[0], Xbest[1], marker='x', color='red')     Out[28]: <pre>&lt;matplotlib.collections.PathCollection at 0x7fa6a1639b80&gt;</pre> In\u00a0[29]: Copied! <pre># This is the raw internal simplex points\na = X.generator.state\na\n</pre> # This is the raw internal simplex points a = X.generator.state a Out[29]: <pre>array([[32.99996111, 44.00000851],\n       [32.99996171, 44.00006811],\n       [33.00002688, 44.00003045]])</pre> In\u00a0[30]: Copied! <pre># Check JSON representation of options\nX.generator.options.json()\n</pre> # Check JSON representation of options X.generator.options.json() Out[30]: <pre>'{\"initial_point\":{\"x0\":0,\"x1\":0},\"initial_simplex\":null,\"adaptive\":true,\"xatol\":0.0001,\"fatol\":0.0001}'</pre> In\u00a0[31]: Copied! <pre># Set the initial simplex to be the latest, \nX.generator.options.initial_simplex = X.generator.simplex\nX.generator.options.xatol = 1e-9\nX.generator.options.fatol = 1e-9\nX.generator._is_done = False # Unlock\nX.run()\n\n\nX.data['y'].plot()\nplt.yscale('log')\n</pre> # Set the initial simplex to be the latest,  X.generator.options.initial_simplex = X.generator.simplex X.generator.options.xatol = 1e-9 X.generator.options.fatol = 1e-9 X.generator._is_done = False # Unlock X.run()   X.data['y'].plot() plt.yscale('log') In\u00a0[32]: Copied! <pre>YAML = \"\"\"\nxopt: {}\ngenerator:\n  name: neldermead\nevaluator:\n  function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock\nvocs:\n  variables:\n    x1: [-5, 5]\n    x2: [-5, 5]\n    x3: [-5, 5]\n    x4: [-5, 5]\n    x5: [-5, 5]\n  objectives:\n    y: MINIMIZE\n\"\"\"\nX = Xopt(YAML)\n</pre> YAML = \"\"\" xopt: {} generator:   name: neldermead evaluator:   function: xopt.resources.test_functions.rosenbrock.evaluate_rosenbrock vocs:   variables:     x1: [-5, 5]     x2: [-5, 5]     x3: [-5, 5]     x4: [-5, 5]     x5: [-5, 5]   objectives:     y: MINIMIZE \"\"\" X = Xopt(YAML) In\u00a0[33]: Copied! <pre>X.run()\nX.data['y'].plot()\nplt.yscale('log')\n</pre> X.run() X.data['y'].plot() plt.yscale('log') In\u00a0[34]: Copied! <pre>fig, ax = plt.subplots(figsize=(8,8))\n\nXgrid, Ygrid = np.meshgrid(np.linspace(-2, 2, 201), np.linspace(-2, 2, 201) )\n\nZgrid = np.vectorize(lambda x, y: rosenbrock([x, y, 1, 1, 1]))(Xgrid, Ygrid)  # The minimum is at 1,1,1,1,1\nZgrid = np.log(Zgrid+1)\n\nax.pcolormesh(Xgrid, Ygrid, Zgrid)\nax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black')\nax.set_xlabel('x0')\nax.set_ylabel('x1')\n\n\n# Add all evaluations\nax.plot(X.data[\"x1\"], X.data[\"x2\"], color='red', alpha=0.5, marker='.')\nax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\")\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\n#plt.legend()\nax.set_title(\"Xopt's Nelder-Mead progression\")\n</pre> fig, ax = plt.subplots(figsize=(8,8))  Xgrid, Ygrid = np.meshgrid(np.linspace(-2, 2, 201), np.linspace(-2, 2, 201) )  Zgrid = np.vectorize(lambda x, y: rosenbrock([x, y, 1, 1, 1]))(Xgrid, Ygrid)  # The minimum is at 1,1,1,1,1 Zgrid = np.log(Zgrid+1)  ax.pcolormesh(Xgrid, Ygrid, Zgrid) ax.contour(Xgrid, Ygrid, Zgrid, levels=10, colors='black') ax.set_xlabel('x0') ax.set_ylabel('x1')   # Add all evaluations ax.plot(X.data[\"x1\"], X.data[\"x2\"], color='red', alpha=0.5, marker='.') ax.scatter(XMIN[0], XMIN[1], 50, marker='o', color='orange', label=\"True minimum\") ax.set_xlim(-2, 2) ax.set_ylim(-2, 2) #plt.legend() ax.set_title(\"Xopt's Nelder-Mead progression\") Out[34]: <pre>Text(0.5, 1.0, \"Xopt's Nelder-Mead progression\")</pre>"},{"location":"examples/scipy/neldermead/#nelder-mead-generator-adapted-from-scipy","title":"Nelder-Mead Generator adapted from SciPy\u00b6","text":"<p>Most of the algorithms in scipy.optimize are self-contained functions that operate on the user-provided <code>func</code>. Xopt has adapted the Nelder-Mead directly from scipy.optimize to be in a generator form. This allows for the manual stepping through the algorithm.</p>"},{"location":"examples/scipy/neldermead/#nelder-mead-optimization-of-the-rosenbrock-function-with-xopt","title":"Nelder-Mead optimization of the Rosenbrock function with Xopt\u00b6","text":""},{"location":"examples/scipy/neldermead/#visualize","title":"Visualize\u00b6","text":""},{"location":"examples/scipy/neldermead/#compare-with-scipyoptimizefmin-nelder-mead","title":"Compare with scipy.optimize.fmin Nelder-Mead\u00b6","text":"<p>Notice that fmin is much faster here. This is because the function runs very fast, so the internal Xopt bookkeeping overhead dominates.</p>"},{"location":"examples/scipy/neldermead/#neldermeadgenerator-object","title":"NelderMeadGenerator object\u00b6","text":""},{"location":"examples/scipy/neldermead/#5-dimensional-rosenbrock","title":"5-dimensional Rosenbrock\u00b6","text":"<p><code>evaluate_rosenbrock</code> works for arbitrary dimensions, so adding more variables to <code>vocs</code> transforms this problem.</p>"},{"location":"examples/single_objective_bayes_opt/benchmarking/","title":"Normal Model with Standard transforms and no constraints","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom copy import deepcopy\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt.vocs import VOCS\n\nvocs = VOCS(\n    variables = {\"x\":[0,1]},\n    objectives = {\"y\":\"MAXIMIZE\"},\n    constraints = {\"c\": [\"LESS_THAN\", 0]}\n)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import matplotlib.pyplot as plt import pandas as pd import torch from copy import deepcopy from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt.vocs import VOCS  vocs = VOCS(     variables = {\"x\":[0,1]},     objectives = {\"y\":\"MAXIMIZE\"},     constraints = {\"c\": [\"LESS_THAN\", 0]} )  In\u00a0[2]: Copied! <pre># define test functions\ndef y(x):\n    return torch.sin(2*3.14*x)\n\ndef c(x):\n    return 10.0*torch.cos(2*3.14*x + 0.25)\n\ntest_x = torch.linspace(*torch.tensor(vocs.bounds.flatten()), 100)\n\n# define training data to pass to the generator\ntrain_x = torch.tensor((0.2,0.5, 0.6))\ntrain_y = y(train_x)\ntrain_c = c(train_x)\n\ndata = pd.DataFrame(\n    {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c}\n)\n\ndef plot_ground_truth():\n    fig,ax = plt.subplots()\n    ax.plot(test_x, y(test_x),'--C0')\n    ax.plot(test_x, c(test_x),'--C1')\n    ax.plot(train_x, train_y,'oC0')\n    ax.plot(train_x, train_c,'oC1')\n\n    return ax\nplot_ground_truth()\n</pre> # define test functions def y(x):     return torch.sin(2*3.14*x)  def c(x):     return 10.0*torch.cos(2*3.14*x + 0.25)  test_x = torch.linspace(*torch.tensor(vocs.bounds.flatten()), 100)  # define training data to pass to the generator train_x = torch.tensor((0.2,0.5, 0.6)) train_y = y(train_x) train_c = c(train_x)  data = pd.DataFrame(     {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c} )  def plot_ground_truth():     fig,ax = plt.subplots()     ax.plot(test_x, y(test_x),'--C0')     ax.plot(test_x, c(test_x),'--C1')     ax.plot(train_x, train_y,'oC0')     ax.plot(train_x, train_c,'oC1')      return ax plot_ground_truth() Out[2]: <pre>&lt;Axes: &gt;</pre> In\u00a0[3]: Copied! <pre>from xopt.utils import visualize_model\n\n# plot the generator model and acquisition function\ngenerator = UpperConfidenceBoundGenerator(deepcopy(vocs), UpperConfidenceBoundGenerator.default_options())\ngenerator.vocs.constraints = {}\n\nvisualize_model(generator, data)\n</pre> from xopt.utils import visualize_model  # plot the generator model and acquisition function generator = UpperConfidenceBoundGenerator(deepcopy(vocs), UpperConfidenceBoundGenerator.default_options()) generator.vocs.constraints = {}  visualize_model(generator, data) In\u00a0[4]: Copied! <pre># plot the generator model and acquisition function\ngenerator = UpperConfidenceBoundGenerator(deepcopy(vocs), UpperConfidenceBoundGenerator.default_options())\ngenerator.vocs.constraints = {\"c\": [\"LESS_THAN\", 0]}\n\nvisualize_model(generator, data)\n</pre> # plot the generator model and acquisition function generator = UpperConfidenceBoundGenerator(deepcopy(vocs), UpperConfidenceBoundGenerator.default_options()) generator.vocs.constraints = {\"c\": [\"LESS_THAN\", 0]}  visualize_model(generator, data)  In\u00a0[5]: Copied! <pre># plot the generator model and acquisition function\ngenerator = UpperConfidenceBoundGenerator(deepcopy(vocs), UpperConfidenceBoundGenerator.default_options())\n\nvisualize_model(generator, data)\n</pre> # plot the generator model and acquisition function generator = UpperConfidenceBoundGenerator(deepcopy(vocs), UpperConfidenceBoundGenerator.default_options())  visualize_model(generator, data)  In\u00a0[6]: Copied! <pre># plot the generator model and acquisition function\ngenerator = UpperConfidenceBoundGenerator(deepcopy(vocs), UpperConfidenceBoundGenerator.default_options())\n\nvisualize_model(generator, data)\n</pre> # plot the generator model and acquisition function generator = UpperConfidenceBoundGenerator(deepcopy(vocs), UpperConfidenceBoundGenerator.default_options())  visualize_model(generator, data)  In\u00a0[7]: Copied! <pre># plot the generator model and acquisition function\n\nbiasing_factor = [1.0, 0.5, 0.1, 0.01,0.001]\n\nfor ele in biasing_factor:\n    generator = UpperConfidenceBoundGenerator(\n        deepcopy(vocs), UpperConfidenceBoundGenerator.default_options()\n    )\n\n    generator.options.acq.proximal_lengthscales = [ele]\n    visualize_model(generator, data)\n</pre> # plot the generator model and acquisition function  biasing_factor = [1.0, 0.5, 0.1, 0.01,0.001]  for ele in biasing_factor:     generator = UpperConfidenceBoundGenerator(         deepcopy(vocs), UpperConfidenceBoundGenerator.default_options()     )      generator.options.acq.proximal_lengthscales = [ele]     visualize_model(generator, data)  In\u00a0[8]: Copied! <pre># plot the generator model and acquisition function\nnew_data = deepcopy(data)\ninput_scale = 1e2\noutput_scale = 1e-5\n\nnew_data['x'] = new_data[\"x\"] * input_scale\nnew_data[\"y\"] = new_data[\"y\"] * output_scale\n\nnew_vocs = deepcopy(vocs)\nnew_vocs.variables[\"x\"] = [0.0, input_scale]\n\nbiasing_factor = [1.0, 0.5, 0.1, 0.01,0.001]\n\nnew_test_x = test_x*input_scale\n\nfor ele in biasing_factor:\n    generator = UpperConfidenceBoundGenerator(deepcopy(new_vocs), UpperConfidenceBoundGenerator.default_options())\n    generator.add_data(new_data)\n\n    generator.options.acq.proximal_lengthscales = [ele]\n\n    visualize_model(generator, data)\n</pre> # plot the generator model and acquisition function new_data = deepcopy(data) input_scale = 1e2 output_scale = 1e-5  new_data['x'] = new_data[\"x\"] * input_scale new_data[\"y\"] = new_data[\"y\"] * output_scale  new_vocs = deepcopy(vocs) new_vocs.variables[\"x\"] = [0.0, input_scale]  biasing_factor = [1.0, 0.5, 0.1, 0.01,0.001]  new_test_x = test_x*input_scale  for ele in biasing_factor:     generator = UpperConfidenceBoundGenerator(deepcopy(new_vocs), UpperConfidenceBoundGenerator.default_options())     generator.add_data(new_data)      generator.options.acq.proximal_lengthscales = [ele]      visualize_model(generator, data)  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/benchmarking/#normal-model-with-standard-transforms-and-no-constraints","title":"Normal Model with Standard transforms and no constraints\u00b6","text":"<ul> <li>acquisition function is UCB with beta = 2</li> </ul>"},{"location":"examples/single_objective_bayes_opt/benchmarking/#normal-model-with-standard-transforms-and-constraints","title":"Normal Model with Standard transforms and constraints\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/benchmarking/#normal-model-with-standard-transforms-constraints-and-conservative-prior-mean","title":"Normal Model with Standard transforms, constraints and conservative prior mean\u00b6","text":"<ul> <li>acquisition function should be biased away from the sides due to the prior</li> </ul> <p>mean (note that the lengthscale is long so its not heavily biased)</p>"},{"location":"examples/single_objective_bayes_opt/benchmarking/#conservative-model-with-standard-transforms-constraints-and-conservative-prior-mean","title":"Conservative Model with Standard transforms, constraints and conservative prior mean\u00b6","text":"<ul> <li>acquisition function should be heavily biased away from the sides due to the prior</li> </ul> <p>mean and short prior lengthscale</p> <ul> <li>make sure to use a strong prior on the noise to reduce it</li> </ul>"},{"location":"examples/single_objective_bayes_opt/benchmarking/#normal-model-with-standard-transforms-and-proximal-biasing","title":"Normal model with standard transforms and proximal biasing\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/benchmarking/#model-with-standard-transforms-and-proximal-biasing-modified-ranges","title":"Model with standard transforms and proximal biasing + modified ranges\u00b6","text":"<ul> <li>model with large input space and small output space</li> <li>Observe that numerical instabilities exist for plotting the posterior but not the</li> </ul> <p>acquisition function since the acquisition function operates on the standardized model.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/","title":"Bayesian optimization tutorial","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\nimport math\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> from xopt.vocs import VOCS import math  # define variables and function objectives vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\ndef sin_function(input_dict):\n    return {\"f\": np.sin(input_dict[\"x\"])}\n</pre> # define a test function to optimize import numpy as np  def sin_function(input_dict):     return {\"f\": np.sin(input_dict[\"x\"])} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt import Xopt\n\nevaluator = Evaluator(function=sin_function)\ngenerator = UpperConfidenceBoundGenerator(vocs)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt import Xopt  evaluator = Evaluator(function=sin_function) generator = UpperConfidenceBoundGenerator(vocs) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre># print initial number of points to be generated\nprint(X.generator.options.n_initial)\n\n# call X.step() to generate + evaluate initial points\nX.step()\n\n# inspect the gathered data\nX.data\n</pre> # print initial number of points to be generated print(X.generator.options.n_initial)  # call X.step() to generate + evaluate initial points X.step()  # inspect the gathered data X.data <pre>3\n</pre> Out[4]: x f xopt_runtime xopt_error 1 1.771527 0.979921 0.000025 False 2 2.420890 0.659912 0.000003 False 3 2.075035 0.875543 0.000002 False In\u00a0[5]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\nn_steps = 3\n\n# test points for plotting\ntest_x = torch.linspace(*X.vocs.bounds.flatten(),50).double()\n\nfor i in range(5):\n    # get the Gaussian process model from the generator\n    model = X.generator.train_model()\n\n    # get acquisition function from generator\n    acq = X.generator.get_acquisition(model)\n\n    # calculate model posterior and acquisition function at each test point\n    # NOTE: need to add a dimension to the input tensor for evaluating the\n    # posterior and another for the acquisition function, see\n    # https://botorch.org/docs/batching for details\n    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n    # skipping calculations for backpropagation\n    with torch.no_grad():\n        posterior = model.posterior(test_x.unsqueeze(1))\n        acq_val = acq(test_x.reshape(-1,1,1))\n\n    # get mean function and confidence regions\n    mean = posterior.mean\n    l,u = posterior.mvn.confidence_region()\n\n    # plot model and acquisition function\n    fig,ax = plt.subplots(2,1,sharex=\"all\")\n\n    # plot model posterior\n    ax[0].plot(test_x, mean, label=\"Posterior mean\")\n    ax[0].fill_between(test_x, l, u,alpha=0.25, label=\"Posterior confidence region\")\n\n    # add data to model plot\n    ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")\n\n    # plot true function\n    true_f = sin_function({\"x\": test_x})[\"f\"]\n    ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")\n\n    # add legend\n    ax[0].legend()\n\n    # plot acquisition function\n    ax[1].plot(test_x, acq_val.flatten())\n\n    ax[0].set_ylabel(\"f\")\n    ax[1].set_ylabel(r\"$\\alpha(x)$\")\n    ax[1].set_xlabel(\"x\")\n\n    # do the optimization step\n    X.step()\n</pre> import torch import matplotlib.pyplot as plt n_steps = 3  # test points for plotting test_x = torch.linspace(*X.vocs.bounds.flatten(),50).double()  for i in range(5):     # get the Gaussian process model from the generator     model = X.generator.train_model()      # get acquisition function from generator     acq = X.generator.get_acquisition(model)      # calculate model posterior and acquisition function at each test point     # NOTE: need to add a dimension to the input tensor for evaluating the     # posterior and another for the acquisition function, see     # https://botorch.org/docs/batching for details     # NOTE: we use the `torch.no_grad()` environment to speed up computation by     # skipping calculations for backpropagation     with torch.no_grad():         posterior = model.posterior(test_x.unsqueeze(1))         acq_val = acq(test_x.reshape(-1,1,1))      # get mean function and confidence regions     mean = posterior.mean     l,u = posterior.mvn.confidence_region()      # plot model and acquisition function     fig,ax = plt.subplots(2,1,sharex=\"all\")      # plot model posterior     ax[0].plot(test_x, mean, label=\"Posterior mean\")     ax[0].fill_between(test_x, l, u,alpha=0.25, label=\"Posterior confidence region\")      # add data to model plot     ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")      # plot true function     true_f = sin_function({\"x\": test_x})[\"f\"]     ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")      # add legend     ax[0].legend()      # plot acquisition function     ax[1].plot(test_x, acq_val.flatten())      ax[0].set_ylabel(\"f\")     ax[1].set_ylabel(r\"$\\alpha(x)$\")     ax[1].set_xlabel(\"x\")      # do the optimization step     X.step()  In\u00a0[6]: Copied! <pre># access the collected data\nX.data\n</pre> # access the collected data X.data Out[6]: x f xopt_runtime xopt_error 1 1.771527 9.799211e-01 0.000025 False 2 2.420890 6.599124e-01 0.000003 False 3 2.075035 8.755426e-01 0.000002 False 4 4.002604 -7.585022e-01 0.000259 False 5 6.283185 -2.449294e-16 0.000014 False 6 4.791795 -9.968490e-01 0.000013 False 7 4.584998 -9.918968e-01 0.000013 False 8 4.711728 -9.999998e-01 0.000014 False In\u00a0[7]: Copied! <pre>X.generator.get_optimum()\n</pre> X.generator.get_optimum() Out[7]: x 0 4.705026 In\u00a0[8]: Copied! <pre>X.generator.options.dict()\n</pre> X.generator.options.dict() Out[8]: <pre>{'optim': {'num_restarts': 20,\n  'raw_samples': 20,\n  'sequential': True,\n  'max_travel_distances': None,\n  'use_turbo': False},\n 'acq': {'proximal_lengthscales': None,\n  'use_transformed_proximal_weights': True,\n  'monte_carlo_samples': 128,\n  'beta': 2.0},\n 'model': {'name': 'standard',\n  'custom_constructor': None,\n  'use_low_noise_prior': True,\n  'covar_modules': {},\n  'mean_modules': {}},\n 'n_initial': 3,\n 'use_cuda': False}</pre> In\u00a0[9]: Copied! <pre># example: add a Gamma(1.0,10.0) prior to the noise hyperparameter to reduce model noise\n# (good for optimizing noise-free simulations)\nX.generator.options.model.use_low_noise_prior = True\n</pre> # example: add a Gamma(1.0,10.0) prior to the noise hyperparameter to reduce model noise # (good for optimizing noise-free simulations) X.generator.options.model.use_low_noise_prior = True In\u00a0[9]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#basic-bayesian-optimization","title":"Basic Bayesian Optimization\u00b6","text":"<p>In this tutorial we demonstrate the use of Xopt to preform Bayesian Optimization on a  simple test problem.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize the sin function in the domian [0,2*pi]. Note that the function used to evaluate the objective function takes a dictionary as input and returns a dictionary as the output.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Upper Confidence Bound acqusition function to perform Bayesian Optimization.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the  generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning  the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and  proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>To perform optimization we simply call <code>X.step()</code> in a loop. This allows us to do intermediate tasks in between optimization steps, such as examining the model and acquisition function at each step (as we demonstrate here).</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#getting-the-optimization-result","title":"Getting the optimization result\u00b6","text":"<p>To get the ideal point (without evaluating the point) we ask the generator to generate a new point.</p>"},{"location":"examples/single_objective_bayes_opt/bo_tutorial/#customizing-optimization","title":"Customizing optimization\u00b6","text":"<p>Each generator has a set of options that can be modified to effect optimization behavior</p>"},{"location":"examples/single_objective_bayes_opt/custom_model/","title":"Custom GP modeling for BO","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom xopt.vocs import VOCS\n\nmy_vocs = VOCS(\n    variables = {\"x\":[0,1]},\n    objectives = {\"y\":\"MAXIMIZE\"},\n    constraints = {\"c\": [\"LESS_THAN\", 0]}\n)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import matplotlib.pyplot as plt import pandas as pd import torch from xopt.vocs import VOCS  my_vocs = VOCS(     variables = {\"x\":[0,1]},     objectives = {\"y\":\"MAXIMIZE\"},     constraints = {\"c\": [\"LESS_THAN\", 0]} )  In\u00a0[2]: Copied! <pre># define test functions\ndef y(x):\n    return torch.sin(2*3.14*x)\n\ndef c(x):\n    return 5.0*torch.cos(2*3.14*x + 0.25)\n\ntest_x = torch.linspace(*torch.tensor(my_vocs.bounds.flatten()), 100)\n\n# define training data to pass to the generator\ntrain_x = torch.tensor((0.2,0.5, 0.6))\ntrain_y = y(train_x)\ntrain_c = c(train_x)\n\ntraining_data = pd.DataFrame(\n    {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c}\n)\n\ndef plot_ground_truth():\n    fig,ax = plt.subplots()\n    ax.plot(test_x, y(test_x),'--C0')\n    ax.plot(test_x, c(test_x),'--C1')\n    ax.plot(train_x, train_y,'oC0')\n    ax.plot(train_x, train_c,'oC1')\n\n    return ax\nplot_ground_truth()\n</pre> # define test functions def y(x):     return torch.sin(2*3.14*x)  def c(x):     return 5.0*torch.cos(2*3.14*x + 0.25)  test_x = torch.linspace(*torch.tensor(my_vocs.bounds.flatten()), 100)  # define training data to pass to the generator train_x = torch.tensor((0.2,0.5, 0.6)) train_y = y(train_x) train_c = c(train_x)  training_data = pd.DataFrame(     {\"x\": train_x.numpy(), \"y\": train_y.numpy(), \"c\": train_c} )  def plot_ground_truth():     fig,ax = plt.subplots()     ax.plot(test_x, y(test_x),'--C0')     ax.plot(test_x, c(test_x),'--C1')     ax.plot(train_x, train_y,'oC0')     ax.plot(train_x, train_c,'oC1')      return ax plot_ground_truth() Out[2]: <pre>&lt;Axes: &gt;</pre> In\u00a0[3]: Copied! <pre>from xopt.generators.bayesian.expected_improvement import ExpectedImprovementGenerator\nfrom xopt.generators.bayesian.options import ModelOptions\nfrom xopt.generators.bayesian.expected_improvement import BayesianOptions\nfrom gpytorch.kernels import PeriodicKernel, ScaleKernel\n\n\n# note the creation of options beforehand\n# specify a periodic kernel for each output (objectives and constraints)\ncovar_module = ScaleKernel(PeriodicKernel())\n\nmodel_options = ModelOptions(covar_modules=covar_module)\ngenerator_options = BayesianOptions(model=model_options)\ngenerator = ExpectedImprovementGenerator(my_vocs, options=generator_options)\ngenerator.options\n</pre> from xopt.generators.bayesian.expected_improvement import ExpectedImprovementGenerator from xopt.generators.bayesian.options import ModelOptions from xopt.generators.bayesian.expected_improvement import BayesianOptions from gpytorch.kernels import PeriodicKernel, ScaleKernel   # note the creation of options beforehand # specify a periodic kernel for each output (objectives and constraints) covar_module = ScaleKernel(PeriodicKernel())  model_options = ModelOptions(covar_modules=covar_module) generator_options = BayesianOptions(model=model_options) generator = ExpectedImprovementGenerator(my_vocs, options=generator_options) generator.options Out[3]: <pre>BayesianOptions(optim=OptimOptions(num_restarts=20, raw_samples=20, sequential=True, max_travel_distances=None, use_turbo=False), acq=AcqOptions(proximal_lengthscales=None, use_transformed_proximal_weights=True, monte_carlo_samples=128), model=ModelOptions(name='standard', custom_constructor=None, use_low_noise_prior=True, covar_modules=ScaleKernel(\n  (base_kernel): PeriodicKernel(\n    (raw_lengthscale_constraint): Positive()\n    (raw_period_length_constraint): Positive()\n  )\n  (raw_outputscale_constraint): Positive()\n), mean_modules={}), n_initial=3, use_cuda=False)</pre> In\u00a0[4]: Copied! <pre># view custom model from data\ngenerator.add_data(training_data)\nmodel = generator.train_model()\n\nfig,ax = plt.subplots(2,1, sharex=\"all\")\nfig.set_size_inches(6,6)\nwith torch.no_grad():\n    post = model.posterior(test_x.reshape(-1,1,1).double())\n\n    for i in range(post.event_shape[-1]):\n        mean = post.mean[...,i].squeeze()\n        l,u = post.mvn.confidence_region()\n        ax[0].plot(test_x, mean,f\"C{i}\", label=generator.vocs.output_names[i])\n        ax[0].fill_between(test_x, l[...,i].squeeze(), u[...,i].squeeze(), alpha=0.5)\n\n    # plot ground truth\n    ax[0].plot(test_x, y(test_x),'C0--', label=\"y ground truth\")\n    ax[0].plot(test_x, c(test_x),'C1--', label=\"c ground truth\")\n\n    # plot training data\n    ax[0].plot(train_x, train_y,\"C0o\", label=\"y data\")\n    ax[0].plot(train_x, train_c,\"C1o\", label=\"c data\")\n    ax[0].legend()\n\n\n    acq = generator.get_acquisition(model)(test_x.reshape(-1,1,1).double())\n\n    ax[1].plot(test_x, acq, label='Acquisition Function')\n    ax[1].legend()\n</pre> # view custom model from data generator.add_data(training_data) model = generator.train_model()  fig,ax = plt.subplots(2,1, sharex=\"all\") fig.set_size_inches(6,6) with torch.no_grad():     post = model.posterior(test_x.reshape(-1,1,1).double())      for i in range(post.event_shape[-1]):         mean = post.mean[...,i].squeeze()         l,u = post.mvn.confidence_region()         ax[0].plot(test_x, mean,f\"C{i}\", label=generator.vocs.output_names[i])         ax[0].fill_between(test_x, l[...,i].squeeze(), u[...,i].squeeze(), alpha=0.5)      # plot ground truth     ax[0].plot(test_x, y(test_x),'C0--', label=\"y ground truth\")     ax[0].plot(test_x, c(test_x),'C1--', label=\"c ground truth\")      # plot training data     ax[0].plot(train_x, train_y,\"C0o\", label=\"y data\")     ax[0].plot(train_x, train_c,\"C1o\", label=\"c data\")     ax[0].legend()       acq = generator.get_acquisition(model)(test_x.reshape(-1,1,1).double())      ax[1].plot(test_x, acq, label='Acquisition Function')     ax[1].legend() In\u00a0[5]: Copied! <pre>model\n</pre> model Out[5]: <pre>ModelListGP(\n  (models): ModuleList(\n    (0-1): 2 x SingleTaskGP(\n      (likelihood): GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n      (mean_module): ConstantMean()\n      (covar_module): ScaleKernel(\n        (base_kernel): PeriodicKernel(\n          (raw_lengthscale_constraint): Positive()\n          (raw_period_length_constraint): Positive()\n        )\n        (raw_outputscale_constraint): Positive()\n      )\n      (outcome_transform): Standardize()\n      (input_transform): Normalize()\n    )\n  )\n  (likelihood): LikelihoodList(\n    (likelihoods): ModuleList(\n      (0-1): 2 x GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n    )\n  )\n)</pre> In\u00a0[6]: Copied! <pre># get the next point from the generator\ngenerator.generate(1)\n</pre> # get the next point from the generator generator.generate(1) Out[6]: x 0 0.2892 In\u00a0[7]: Copied! <pre>class ConstraintPrior(torch.nn.Module):\n    def forward(self, X):\n        return c(X).squeeze(dim=-1)\n\nmodel_options = ModelOptions(mean_modules={\"c\":ConstraintPrior()})\ngenerator_options = BayesianOptions(model=model_options)\ngenerator = ExpectedImprovementGenerator(my_vocs, options=generator_options)\n</pre> class ConstraintPrior(torch.nn.Module):     def forward(self, X):         return c(X).squeeze(dim=-1)  model_options = ModelOptions(mean_modules={\"c\":ConstraintPrior()}) generator_options = BayesianOptions(model=model_options) generator = ExpectedImprovementGenerator(my_vocs, options=generator_options) In\u00a0[8]: Copied! <pre># view custom model from data\ngenerator.add_data(training_data)\nmodel = generator.train_model()\ntest_x = torch.linspace(0,5, 100)\n\n\nfig,ax = plt.subplots(2,1, sharex=\"all\")\nfig.set_size_inches(6,6)\nwith torch.no_grad():\n    post = model.posterior(test_x.reshape(-1,1,1).double())\n\n    for i in range(post.event_shape[-1]):\n        mean = post.mean[...,i].squeeze()\n        l,u = post.mvn.confidence_region()\n        ax[0].plot(test_x, mean,f\"C{i}\", label=generator.vocs.output_names[i])\n        ax[0].fill_between(test_x, l[...,i].squeeze(), u[...,i].squeeze(), alpha=0.5)\n\n    # plot ground truth\n    ax[0].plot(test_x, y(test_x),'C0--', label=\"y ground truth\")\n    ax[0].plot(test_x, c(test_x),'C1--', label=\"c ground truth\")\n\n    # plot training data\n    ax[0].plot(train_x, train_y,\"C0o\", label=\"y data\")\n    ax[0].plot(train_x, train_c,\"C1o\", label=\"c data\")\n    ax[0].legend()\n\n\n    acq = generator.get_acquisition(model)(test_x.reshape(-1,1,1).double())\n\n    ax[1].plot(test_x, acq, label='Acquisition Function')\n    ax[1].legend()\n</pre> # view custom model from data generator.add_data(training_data) model = generator.train_model() test_x = torch.linspace(0,5, 100)   fig,ax = plt.subplots(2,1, sharex=\"all\") fig.set_size_inches(6,6) with torch.no_grad():     post = model.posterior(test_x.reshape(-1,1,1).double())      for i in range(post.event_shape[-1]):         mean = post.mean[...,i].squeeze()         l,u = post.mvn.confidence_region()         ax[0].plot(test_x, mean,f\"C{i}\", label=generator.vocs.output_names[i])         ax[0].fill_between(test_x, l[...,i].squeeze(), u[...,i].squeeze(), alpha=0.5)      # plot ground truth     ax[0].plot(test_x, y(test_x),'C0--', label=\"y ground truth\")     ax[0].plot(test_x, c(test_x),'C1--', label=\"c ground truth\")      # plot training data     ax[0].plot(train_x, train_y,\"C0o\", label=\"y data\")     ax[0].plot(train_x, train_c,\"C1o\", label=\"c data\")     ax[0].legend()       acq = generator.get_acquisition(model)(test_x.reshape(-1,1,1).double())      ax[1].plot(test_x, acq, label='Acquisition Function')     ax[1].legend() In\u00a0[9]: Copied! <pre>model\n</pre> model Out[9]: <pre>ModelListGP(\n  (models): ModuleList(\n    (0): SingleTaskGP(\n      (likelihood): GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n      (mean_module): ConstantMean()\n      (covar_module): ScaleKernel(\n        (base_kernel): MaternKernel(\n          (lengthscale_prior): GammaPrior()\n          (raw_lengthscale_constraint): Positive()\n        )\n        (outputscale_prior): GammaPrior()\n        (raw_outputscale_constraint): Positive()\n      )\n      (outcome_transform): Standardize()\n      (input_transform): Normalize()\n    )\n    (1): SingleTaskGP(\n      (likelihood): GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n      (mean_module): CustomMean(\n        (model): ConstraintPrior()\n        (input_transformer): Normalize()\n        (outcome_transformer): Standardize()\n      )\n      (covar_module): ScaleKernel(\n        (base_kernel): MaternKernel(\n          (lengthscale_prior): GammaPrior()\n          (raw_lengthscale_constraint): Positive()\n        )\n        (outputscale_prior): GammaPrior()\n        (raw_outputscale_constraint): Positive()\n      )\n      (outcome_transform): Standardize()\n      (input_transform): Normalize()\n    )\n  )\n  (likelihood): LikelihoodList(\n    (likelihoods): ModuleList(\n      (0-1): 2 x GaussianLikelihood(\n        (noise_covar): HomoskedasticNoise(\n          (noise_prior): GammaPrior()\n          (raw_noise_constraint): GreaterThan(1.000E-04)\n        )\n      )\n    )\n  )\n)</pre> In\u00a0[10]: Copied! <pre>list(model.named_parameters())\n</pre> list(model.named_parameters()) Out[10]: <pre>[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-25.3852], dtype=torch.float64, requires_grad=True)),\n ('models.0.mean_module.raw_constant',\n  Parameter containing:\n  tensor(-0.2222, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(3.0726, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[-0.7308]], dtype=torch.float64, requires_grad=True)),\n ('models.1.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(-6.7637, dtype=torch.float64, requires_grad=True)),\n ('models.1.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[-0.5597]], dtype=torch.float64, requires_grad=True))]</pre> In\u00a0[10]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/custom_model/#custom-gp-modeling-for-bo","title":"Custom GP modeling for BO\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/custom_model/#custom-kernel-definition","title":"Custom kernel definition\u00b6","text":"<p>In this example we know that the target optimization function is periodic, so it makes sense to use a periodic kernel for the GP model with no noise. Here we define a function to create that model.</p>"},{"location":"examples/single_objective_bayes_opt/custom_model/#custom-prior-mean-function","title":"Custom prior mean function\u00b6","text":"<p>Here we assume we have some knowledge of the ground truth function, which we can take  advantage of to speed up optimization. This \"prior mean\" function is specified by a  pytorch module.</p>"},{"location":"examples/single_objective_bayes_opt/expected_improvement_rosenbrock/","title":"Expected Improvement BO","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Import the class\nimport torch\nimport yaml\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  # Import the class import torch import yaml In\u00a0[2]: Copied! <pre>from xopt.generators import get_generator_and_defaults\nei_gen, ei_options = get_generator_and_defaults(\"expected_improvement\")\nprint(yaml.dump(ei_options.dict()))\n</pre> from xopt.generators import get_generator_and_defaults ei_gen, ei_options = get_generator_and_defaults(\"expected_improvement\") print(yaml.dump(ei_options.dict())) <pre>acq:\n  monte_carlo_samples: 128\n  proximal_lengthscales: null\n  use_transformed_proximal_weights: true\nmodel:\n  covar_modules: {}\n  custom_constructor: null\n  mean_modules: {}\n  name: standard\n  use_low_noise_prior: true\nn_initial: 3\noptim:\n  max_travel_distances: null\n  num_restarts: 20\n  raw_samples: 20\n  sequential: true\n  use_turbo: false\nuse_cuda: false\n\n</pre> In\u00a0[3]: Copied! <pre>from xopt.resources.test_functions.rosenbrock import make_rosenbrock_vocs, evaluate_rosenbrock\nfrom xopt import Xopt, Evaluator\n\nvocs = make_rosenbrock_vocs(2)\n\ngenerator_options = ei_gen.default_options()\ngenerator_options.optim.num_restarts = 20\ngenerator_options.optim.max_travel_distances = [0.1, 0.1]\n\nevaluator = Evaluator(function=evaluate_rosenbrock)\ngenerator = ei_gen(vocs, generator_options)\n</pre> from xopt.resources.test_functions.rosenbrock import make_rosenbrock_vocs, evaluate_rosenbrock from xopt import Xopt, Evaluator  vocs = make_rosenbrock_vocs(2)  generator_options = ei_gen.default_options() generator_options.optim.num_restarts = 20 generator_options.optim.max_travel_distances = [0.1, 0.1]  evaluator = Evaluator(function=evaluate_rosenbrock) generator = ei_gen(vocs, generator_options)  In\u00a0[4]: Copied! <pre>X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\n\nfor i in range(100):\n    X.step()\n    print(f\"step {i}: best: {X.data['y'].min()}\")\n</pre> X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)  for i in range(100):     X.step()     print(f\"step {i}: best: {X.data['y'].min()}\")  <pre>step 0: best: 8.159433615587364\nstep 1: best: 8.159433615587364\nstep 2: best: 8.159433615587364\nstep 3: best: 8.159433615587364\nstep 4: best: 8.159433615587364\nstep 5: best: 8.159433615587364\nstep 6: best: 8.159433615587364\nstep 7: best: 8.159433615587364\nstep 8: best: 8.159433615587364\nstep 9: best: 8.159433615587364\nstep 10: best: 8.159433615587364\nstep 11: best: 8.159433615587364\nstep 12: best: 8.159433615587364\nstep 13: best: 8.159433615587364\nstep 14: best: 8.159433615587364\nstep 15: best: 8.159433615587364\nstep 16: best: 8.159433615587364\nstep 17: best: 8.159433615587364\nstep 18: best: 8.159433615587364\nstep 19: best: 8.159433615587364\nstep 20: best: 8.159433615587364\nstep 21: best: 8.159433615587364\nstep 22: best: 8.159433615587364\nstep 23: best: 8.159433615587364\nstep 24: best: 8.159433615587364\nstep 25: best: 8.159433615587364\nstep 26: best: 8.159433615587364\nstep 27: best: 8.159433615587364\nstep 28: best: 8.159433615587364\nstep 29: best: 8.159433615587364\nstep 30: best: 8.159433615587364\nstep 31: best: 8.159433615587364\nstep 32: best: 8.159433615587364\nstep 33: best: 8.159433615587364\nstep 34: best: 8.159433615587364\nstep 35: best: 8.159433615587364\nstep 36: best: 8.159433615587364\nstep 37: best: 8.159433615587364\nstep 38: best: 8.159433615587364\nstep 39: best: 8.159433615587364\nstep 40: best: 8.159433615587364\nstep 41: best: 8.159433615587364\nstep 42: best: 8.159433615587364\nstep 43: best: 8.159433615587364\nstep 44: best: 8.159433615587364\nstep 45: best: 8.159433615587364\nstep 46: best: 8.159433615587364\nstep 47: best: 8.159433615587364\nstep 48: best: 8.159433615587364\nstep 49: best: 8.159433615587364\nstep 50: best: 8.159433615587364\nstep 51: best: 8.159433615587364\nstep 52: best: 8.159433615587364\nstep 53: best: 8.159433615587364\nstep 54: best: 8.159433615587364\nstep 55: best: 8.159433615587364\nstep 56: best: 8.159433615587364\nstep 57: best: 8.159433615587364\nstep 58: best: 5.872951026547568\nstep 59: best: 3.6793799836319456\nstep 60: best: 1.6264423438617484\nstep 61: best: 1.6264423438617484\nstep 62: best: 1.6264423438617484\nstep 63: best: 0.2839666058530994\nstep 64: best: 0.2839666058530994\nstep 65: best: 0.2839666058530994\nstep 66: best: 0.2839666058530994\nstep 67: best: 0.2839666058530994\nstep 68: best: 0.2839666058530994\nstep 69: best: 0.2839666058530994\nstep 70: best: 0.2839666058530994\nstep 71: best: 0.2839666058530994\nstep 72: best: 0.2839666058530994\nstep 73: best: 0.2839666058530994\nstep 74: best: 0.2839666058530994\nstep 75: best: 0.2839666058530994\nstep 76: best: 0.2839666058530994\nstep 77: best: 0.2839666058530994\nstep 78: best: 0.2839666058530994\nstep 79: best: 0.2839666058530994\nstep 80: best: 0.2839666058530994\nstep 81: best: 0.2839666058530994\nstep 82: best: 0.2839666058530994\nstep 83: best: 0.2839666058530994\nstep 84: best: 0.2839666058530994\nstep 85: best: 0.2839666058530994\nstep 86: best: 0.2839666058530994\nstep 87: best: 0.2839666058530994\nstep 88: best: 0.2839666058530994\nstep 89: best: 0.2839666058530994\nstep 90: best: 0.1526588905681717\nstep 91: best: 0.1526588905681717\nstep 92: best: 0.1526588905681717\nstep 93: best: 0.1526588905681717\nstep 94: best: 0.1526588905681717\nstep 95: best: 0.1526588905681717\nstep 96: best: 0.1526588905681717\nstep 97: best: 0.1526588905681717\nstep 98: best: 0.1526588905681717\nstep 99: best: 0.016345681963622963\n</pre> In\u00a0[5]: Copied! <pre>X.data\n</pre> X.data Out[5]: x0 x1 y xopt_runtime xopt_error 1 -1.389588 1.774451 8.159434 0.000013 False 2 0.831995 1.923792 151.706235 0.000006 False 3 -1.366797 -0.326815 487.381861 0.000003 False 4 -1.420905 -0.132125 468.581951 0.000014 False 5 -1.642434 -0.437976 990.159769 0.000014 False ... ... ... ... ... ... 98 0.665302 0.928510 23.720241 0.000015 False 99 0.845534 1.294523 33.616951 0.000015 False 100 0.535476 0.934367 42.158635 0.000015 False 101 0.851576 1.247212 27.273642 0.000016 False 102 1.114444 1.247685 0.016346 0.000018 False <p>102 rows \u00d7 5 columns</p> In\u00a0[6]: Copied! <pre># plot results\nax = X.data.plot(*vocs.variable_names)\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(*vocs.variable_names) ax.set_aspect(\"equal\") In\u00a0[7]: Copied! <pre>from matplotlib import pyplot as plt  # plot model predictions\n\ndata = X.data\n\nbounds = generator.vocs.bounds\nmodel = generator.train_model(generator.data)\n\n# create mesh\nn = 200\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\noutputs = generator.vocs.output_names\nwith torch.no_grad():\n    post = model.posterior(pts)\n\n    mean = post.mean\n    std = torch.sqrt(post.variance)\n\n    fig, ax = plt.subplots()\n    ax.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")\n    c = ax.pcolor(xx, yy, mean.reshape(n, n))\n    fig.colorbar(c)\n    ax.set_title(f\"Posterior mean: {outputs[0]}\")\n\n    fig2, ax2 = plt.subplots()\n    ax2.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")\n    c = ax2.pcolor(xx, yy, std.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(f\"Posterior std: {outputs[0]}\")\n</pre> from matplotlib import pyplot as plt  # plot model predictions  data = X.data  bounds = generator.vocs.bounds model = generator.train_model(generator.data)  # create mesh n = 200 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  outputs = generator.vocs.output_names with torch.no_grad():     post = model.posterior(pts)      mean = post.mean     std = torch.sqrt(post.variance)      fig, ax = plt.subplots()     ax.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")     c = ax.pcolor(xx, yy, mean.reshape(n, n))     fig.colorbar(c)     ax.set_title(f\"Posterior mean: {outputs[0]}\")      fig2, ax2 = plt.subplots()     ax2.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")     c = ax2.pcolor(xx, yy, std.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(f\"Posterior std: {outputs[0]}\") In\u00a0[8]: Copied! <pre>ax = X.data.plot(y=\"y\", logy=True)\n</pre> ax = X.data.plot(y=\"y\", logy=True) In\u00a0[9]: Copied! <pre># Cleanup\n!rm dump.yaml\n</pre> # Cleanup !rm dump.yaml <pre>rm: cannot remove 'dump.yaml': No such file or directory\r\n</pre>"},{"location":"examples/single_objective_bayes_opt/expected_improvement_rosenbrock/#expected-improvement-bo","title":"Expected Improvement BO\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/expected_improvement_rosenbrock/#customizing-the-upperconfidencebound-generator","title":"Customizing the UpperConfidenceBound Generator\u00b6","text":"<p>First lets examine the possible options that we can specify for the UpperConfidenceBound generator. We can use these keys to customize optimization.</p>"},{"location":"examples/single_objective_bayes_opt/expected_improvement_rosenbrock/#view-output-data","title":"View output data\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/expected_improvement_rosenbrock/#visualize-model-used-by-upper-confidence-bound","title":"Visualize model used by upper confidence bound\u00b6","text":"<p>Models are kept in a list, in this case that list has one element, the model created for the objective <code>y1</code>.</p>"},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/","title":"Multi-fidelity BO","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\nimport pandas as pd\nimport torch\n\ndef test_function(input_dict):\n    x = input_dict[\"x\"]\n    s = input_dict[\"s\"]\n    return {\"f\":np.sin(x + (1.0 - s)) * np.exp((-s+1)/2)}\n\n\n# define vocs\nfrom xopt import VOCS\nvocs = VOCS(\n    variables={\n        \"x\": [0, 2*math.pi],\n    },\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> import matplotlib.pyplot as plt import numpy as np import math  import pandas as pd import torch  def test_function(input_dict):     x = input_dict[\"x\"]     s = input_dict[\"s\"]     return {\"f\":np.sin(x + (1.0 - s)) * np.exp((-s+1)/2)}   # define vocs from xopt import VOCS vocs = VOCS(     variables={         \"x\": [0, 2*math.pi],     },     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre>test_x = np.linspace(*vocs.bounds, 1000)\nfidelities = [0.0,0.5,1.0]\n\nfig,ax = plt.subplots()\nfor ele in fidelities:\n    f = test_function({\"x\":test_x, \"s\":ele})[\"f\"]\n    ax.plot(test_x, f,label=f\"s:{ele}\")\n\nax.legend()\n</pre> test_x = np.linspace(*vocs.bounds, 1000) fidelities = [0.0,0.5,1.0]  fig,ax = plt.subplots() for ele in fidelities:     f = test_function({\"x\":test_x, \"s\":ele})[\"f\"]     ax.plot(test_x, f,label=f\"s:{ele}\")  ax.legend() Out[2]: <pre>&lt;matplotlib.legend.Legend at 0x7f504d661280&gt;</pre> In\u00a0[3]: Copied! <pre># create xopt object\nfrom xopt.generators.bayesian import MultiFidelityGenerator\nfrom xopt import Evaluator, Xopt\n\n# get and modify default generator options\noptions = MultiFidelityGenerator.default_options()\n\n# specify a custom cost function based on the fidelity parameter\noptions.acq.cost_function = lambda s: s + 0.001\n\n# pass options to the generator\ngenerator = MultiFidelityGenerator(vocs, options=options)\nevaluator = Evaluator(function=test_function)\n\nX = Xopt(vocs=vocs, generator=generator, evaluator=evaluator)\nX.generator.options.dict()\n</pre> # create xopt object from xopt.generators.bayesian import MultiFidelityGenerator from xopt import Evaluator, Xopt  # get and modify default generator options options = MultiFidelityGenerator.default_options()  # specify a custom cost function based on the fidelity parameter options.acq.cost_function = lambda s: s + 0.001  # pass options to the generator generator = MultiFidelityGenerator(vocs, options=options) evaluator = Evaluator(function=test_function)  X = Xopt(vocs=vocs, generator=generator, evaluator=evaluator) X.generator.options.dict() Out[3]: <pre>{'optim': {'num_restarts': 20,\n  'raw_samples': 20,\n  'sequential': True,\n  'max_travel_distances': None,\n  'use_turbo': False},\n 'acq': {'proximal_lengthscales': None,\n  'use_transformed_proximal_weights': True,\n  'monte_carlo_samples': 128,\n  'cost_function': &lt;function __main__.&lt;lambda&gt;(s)&gt;,\n  'reference_point': None},\n 'model': {'name': 'standard',\n  'custom_constructor': None,\n  'use_low_noise_prior': True,\n  'covar_modules': {},\n  'mean_modules': {},\n  'fidelity_parameter': 's'},\n 'n_initial': 3,\n 'use_cuda': False}</pre> In\u00a0[4]: Copied! <pre># evaluate initial points at mixed fidelities to seed optimization\nX.evaluate_data(pd.DataFrame({\n    \"x\":[math.pi / 4, math.pi / 2., math.pi],\"s\":[0.0, 0.25, 0.0]\n}))\n</pre> # evaluate initial points at mixed fidelities to seed optimization X.evaluate_data(pd.DataFrame({     \"x\":[math.pi / 4, math.pi / 2., math.pi],\"s\":[0.0, 0.25, 0.0] })) Out[4]: x s f xopt_runtime xopt_error 1 0.785398 0.00 1.610902 0.000022 False 2 1.570796 0.25 1.064601 0.000004 False 3 3.141593 0.00 -1.387351 0.000004 False In\u00a0[5]: Copied! <pre># get the total cost of previous observations based on the cost function\nX.generator.calculate_total_cost()\n</pre> # get the total cost of previous observations based on the cost function X.generator.calculate_total_cost() Out[5]: <pre>tensor(0.2530, dtype=torch.float64)</pre> In\u00a0[6]: Copied! <pre># run optimization until the cost budget is exhausted\n# we subtract one unit to make sure we don't go over our eval budget\nbudget = 10\nwhile X.generator.calculate_total_cost() &lt; budget - 1:\n    X.step()\n    print(f\"n_samples: {len(X.data)} \"\n          f\"budget used: {X.generator.calculate_total_cost():.4} \"\n          f\"hypervolume: {X.generator.calculate_hypervolume():.4}\")\n</pre> # run optimization until the cost budget is exhausted # we subtract one unit to make sure we don't go over our eval budget budget = 10 while X.generator.calculate_total_cost() &lt; budget - 1:     X.step()     print(f\"n_samples: {len(X.data)} \"           f\"budget used: {X.generator.calculate_total_cost():.4} \"           f\"hypervolume: {X.generator.calculate_hypervolume():.4}\")  <pre>n_samples: 4 budget used: 0.285 hypervolume: 2.234\nn_samples: 5 budget used: 0.5024 hypervolume: 2.719\nn_samples: 6 budget used: 0.8243 hypervolume: 3.538\nn_samples: 7 budget used: 1.26 hypervolume: 4.84\nn_samples: 8 budget used: 1.858 hypervolume: 6.678\nn_samples: 9 budget used: 2.677 hypervolume: 9.102\nn_samples: 10 budget used: 3.678 hypervolume: 11.05\nn_samples: 11 budget used: 4.679 hypervolume: 11.05\nn_samples: 12 budget used: 5.68 hypervolume: 11.05\nn_samples: 13 budget used: 6.681 hypervolume: 11.05\nn_samples: 14 budget used: 7.682 hypervolume: 11.1\nn_samples: 15 budget used: 7.813 hypervolume: 11.15\nn_samples: 16 budget used: 8.15 hypervolume: 11.2\nn_samples: 17 budget used: 8.874 hypervolume: 11.21\nn_samples: 18 budget used: 9.361 hypervolume: 11.23\n</pre> In\u00a0[7]: Copied! <pre>X.data\n</pre> X.data Out[7]: x s f xopt_runtime xopt_error 1 0.785398 0.000000 1.610902e+00 0.000022 False 2 1.570796 0.250000 1.064601e+00 0.000004 False 3 3.141593 0.000000 -1.387351e+00 0.000004 False 4 6.283185 0.030996 1.338178e+00 0.000020 False 5 3.276123 0.216394 -1.175535e+00 0.000020 False 6 2.994836 0.320920 -7.127354e-01 0.000019 False 7 3.571623 0.434318 -1.113459e+00 0.000020 False 8 4.020026 0.597715 -1.171712e+00 0.000019 False 9 4.105980 0.818077 -9.980263e-01 0.000018 False 10 3.893605 1.000000 -6.831097e-01 0.000021 False 11 1.401300 1.000000 9.856698e-01 0.000021 False 12 2.718878 1.000000 4.102374e-01 0.000018 False 13 6.283185 1.000000 -2.449294e-16 0.000019 False 14 4.807479 1.000000 -9.954823e-01 0.000021 False 15 3.907714 0.129650 -1.541902e+00 0.000021 False 16 4.025439 0.336423 -1.393077e+00 0.000020 False 17 4.463496 0.722853 -1.148176e+00 0.000030 False 18 4.175518 0.485168 -1.293269e+00 0.000018 False In\u00a0[8]: Copied! <pre># augment the bounds to add the fidelity parameter\nbounds = generator.vocs.bounds[::-1]\n\nmodel = generator.model\n\n# create mesh\nn = 50\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\nacq_func = generator.get_acquisition(model)\n\nvariable_names = generator.vocs.variable_names\nwith torch.no_grad():\n    print(pts.shape)\n    # get the model posterior\n    post = model.posterior(pts.unsqueeze(-2))\n    f_mean = post.mean[..., 0]\n    s_mean = post.mean[..., 1]\n\n\n    acq_pts = pts.unsqueeze(1)\n    acq = acq_func(acq_pts)\n\n    fig, ax0 = plt.subplots()\n    c = ax0.pcolor(xx, yy, f_mean.reshape(n, n))\n    fig.colorbar(c)\n    ax0.set_title(\"f-Mean prediction\")\n\n    fig, ax1 = plt.subplots()\n    c = ax1.pcolor(xx, yy, s_mean.reshape(n, n))\n    fig.colorbar(c)\n    ax1.set_title(\"s-Mean prediction\")\n\n    fig, ax2 = plt.subplots()\n    c = ax2.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")\n    fig.colorbar(c)\n    ax2.set_title(\"Acquisition function\")\n\n    X.data.plot(x=variable_names[0], y=variable_names[1],ax=ax2,style=\"oC1\")\n\n    # mark the next observation\n    next_pt = pts[torch.argmax(acq)]\n    ax2.plot(*next_pt,\"*r\",ms=10)\n\n    # mark the optimum at the max fidelity\n    best_loc = [1.0, 1.5*np.pi]\n    ax2.plot(*best_loc, \"*\",c=\"C4\")\n\n    for a in [ax0,ax1,ax2]:\n        a.set_xlabel(variable_names[0])\n        a.set_ylabel(variable_names[1])\n</pre> # augment the bounds to add the fidelity parameter bounds = generator.vocs.bounds[::-1]  model = generator.model  # create mesh n = 50 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  acq_func = generator.get_acquisition(model)  variable_names = generator.vocs.variable_names with torch.no_grad():     print(pts.shape)     # get the model posterior     post = model.posterior(pts.unsqueeze(-2))     f_mean = post.mean[..., 0]     s_mean = post.mean[..., 1]       acq_pts = pts.unsqueeze(1)     acq = acq_func(acq_pts)      fig, ax0 = plt.subplots()     c = ax0.pcolor(xx, yy, f_mean.reshape(n, n))     fig.colorbar(c)     ax0.set_title(\"f-Mean prediction\")      fig, ax1 = plt.subplots()     c = ax1.pcolor(xx, yy, s_mean.reshape(n, n))     fig.colorbar(c)     ax1.set_title(\"s-Mean prediction\")      fig, ax2 = plt.subplots()     c = ax2.pcolor(xx, yy, acq.reshape(n, n), cmap=\"Blues\")     fig.colorbar(c)     ax2.set_title(\"Acquisition function\")      X.data.plot(x=variable_names[0], y=variable_names[1],ax=ax2,style=\"oC1\")      # mark the next observation     next_pt = pts[torch.argmax(acq)]     ax2.plot(*next_pt,\"*r\",ms=10)      # mark the optimum at the max fidelity     best_loc = [1.0, 1.5*np.pi]     ax2.plot(*best_loc, \"*\",c=\"C4\")      for a in [ax0,ax1,ax2]:         a.set_xlabel(variable_names[0])         a.set_ylabel(variable_names[1])  <pre>torch.Size([2500, 2])\n</pre> <pre>/usr/share/miniconda3/envs/xopt-dev/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1680607361662/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n</pre> In\u00a0[9]: Copied! <pre>X.data.plot(x=\"f\", y=\"s\", style=\"o-\")\n</pre> X.data.plot(x=\"f\", y=\"s\", style=\"o-\") Out[9]: <pre>&lt;Axes: xlabel='f'&gt;</pre> In\u00a0[10]: Copied! <pre>X.data\n</pre> X.data Out[10]: x s f xopt_runtime xopt_error 1 0.785398 0.000000 1.610902e+00 0.000022 False 2 1.570796 0.250000 1.064601e+00 0.000004 False 3 3.141593 0.000000 -1.387351e+00 0.000004 False 4 6.283185 0.030996 1.338178e+00 0.000020 False 5 3.276123 0.216394 -1.175535e+00 0.000020 False 6 2.994836 0.320920 -7.127354e-01 0.000019 False 7 3.571623 0.434318 -1.113459e+00 0.000020 False 8 4.020026 0.597715 -1.171712e+00 0.000019 False 9 4.105980 0.818077 -9.980263e-01 0.000018 False 10 3.893605 1.000000 -6.831097e-01 0.000021 False 11 1.401300 1.000000 9.856698e-01 0.000021 False 12 2.718878 1.000000 4.102374e-01 0.000018 False 13 6.283185 1.000000 -2.449294e-16 0.000019 False 14 4.807479 1.000000 -9.954823e-01 0.000021 False 15 3.907714 0.129650 -1.541902e+00 0.000021 False 16 4.025439 0.336423 -1.393077e+00 0.000020 False 17 4.463496 0.722853 -1.148176e+00 0.000030 False 18 4.175518 0.485168 -1.293269e+00 0.000018 False In\u00a0[11]: Copied! <pre># get optimal value at max fidelity, note that the actual maximum is 4.71\nX.generator.get_optimum().to_dict()\n</pre> # get optimal value at max fidelity, note that the actual maximum is 4.71 X.generator.get_optimum().to_dict() Out[11]: <pre>{'s': {0: 1.0}, 'x': {0: 4.674871694441191}}</pre>"},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/#multi-fidelity-bo","title":"Multi-fidelity BO\u00b6","text":"<p>Here we demonstrate how Multi-Fidelity Bayesian Optimization can be used to reduce the computational cost of optimization by using lower fidelity surrogate models. The goal is to learn functional dependance of the objective on input variables at low fidelities (which are cheap to compute) and use that information to quickly find the best objective value at higher fidelities (which are more expensive to compute). This assumes that there is some learnable correlation between the objective values at different fidelities.</p> <p>Xopt implements the MOMF (https://botorch.org/tutorials/Multi_objective_multi_fidelity_BO) algorithm which can be used to solve both single (this notebook) and multi-objective (see multi-objective BO section) multi-fidelity problems. Under the hood this algorithm attempts to solve a multi-objective optimization problem, where one objective is the function objective and the other is a simple fidelity objective, weighted by the <code>cost_function</code> of evaluating the objective at a given fidelity.</p>"},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/#plot-the-test-function-in-input-fidelity-space","title":"plot the test function in input + fidelity space\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/#plot-the-model-prediction-and-acquisition-function-inside-the-optimization-space","title":"Plot the model prediction and acquisition function inside the optimization space\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/multi_fidelity_simple/#plot-the-pareto-front","title":"Plot the Pareto front\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/prior_mean_model/","title":"Custom GP modeling for BO","text":"In\u00a0[1]: Copied! <pre># Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom xopt.vocs import VOCS\n\nmy_vocs = VOCS(\n    variables = {\"x\":[-1,3]},\n    objectives = {\"y\":\"MAXIMIZE\"},\n)\n</pre> # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import matplotlib.pyplot as plt import pandas as pd import torch from xopt.vocs import VOCS  my_vocs = VOCS(     variables = {\"x\":[-1,3]},     objectives = {\"y\":\"MAXIMIZE\"}, )  In\u00a0[2]: Copied! <pre># define test functions\n\ntest_x = torch.linspace(*torch.tensor(my_vocs.bounds.flatten())*3, 100)\n\n# define training data to pass to the generator\ntrain_x = torch.tensor((0.0, 0.75))\ntrain_y = train_x\n\ntraining_data = pd.DataFrame(\n    {\"x\": train_x.numpy(), \"y\": train_y.numpy()}\n)\n</pre> # define test functions  test_x = torch.linspace(*torch.tensor(my_vocs.bounds.flatten())*3, 100)  # define training data to pass to the generator train_x = torch.tensor((0.0, 0.75)) train_y = train_x  training_data = pd.DataFrame(     {\"x\": train_x.numpy(), \"y\": train_y.numpy()} ) In\u00a0[3]: Copied! <pre>from xopt.generators.bayesian.expected_improvement import ExpectedImprovementGenerator\nfrom xopt.generators.bayesian.options import ModelOptions\nfrom xopt.generators.bayesian.expected_improvement import BayesianOptions\n\nclass SquaredPrior(torch.nn.Module):\n    def forward(self, X):\n        return 3*torch.sin(X.squeeze(dim=-1) + 0.25)\n\nmodel_options = ModelOptions(\n    mean_modules={\"y\":SquaredPrior()},\n)\ngenerator_options = BayesianOptions(model=model_options)\ngenerator = ExpectedImprovementGenerator(my_vocs, options=generator_options)\ngenerator.add_data(training_data)\n</pre> from xopt.generators.bayesian.expected_improvement import ExpectedImprovementGenerator from xopt.generators.bayesian.options import ModelOptions from xopt.generators.bayesian.expected_improvement import BayesianOptions  class SquaredPrior(torch.nn.Module):     def forward(self, X):         return 3*torch.sin(X.squeeze(dim=-1) + 0.25)  model_options = ModelOptions(     mean_modules={\"y\":SquaredPrior()}, ) generator_options = BayesianOptions(model=model_options) generator = ExpectedImprovementGenerator(my_vocs, options=generator_options) generator.add_data(training_data) In\u00a0[4]: Copied! <pre># view custom model from data\nmodel = generator.train_model()\n\n\nfig,ax = plt.subplots(1,1, sharex=\"all\")\nfig.set_size_inches(8,6)\nwith torch.no_grad():\n    post = model.posterior(test_x.reshape(-1,1,1).double())\n\n    mean = post.mean.squeeze()\n    l,u = post.mvn.confidence_region()\n    ax.plot(test_x, mean)\n    ax.fill_between(test_x, l.squeeze(), u.squeeze(), alpha=0.5)\n\n    # plot prior\n    ax.plot(test_x, SquaredPrior()(test_x),'C1--', label=\"y prior\")\n\n    # plot training data\n    ax.plot(train_x, train_y,\"C0o\", label=\"y data\")\n    ax.legend()\n</pre> # view custom model from data model = generator.train_model()   fig,ax = plt.subplots(1,1, sharex=\"all\") fig.set_size_inches(8,6) with torch.no_grad():     post = model.posterior(test_x.reshape(-1,1,1).double())      mean = post.mean.squeeze()     l,u = post.mvn.confidence_region()     ax.plot(test_x, mean)     ax.fill_between(test_x, l.squeeze(), u.squeeze(), alpha=0.5)      # plot prior     ax.plot(test_x, SquaredPrior()(test_x),'C1--', label=\"y prior\")      # plot training data     ax.plot(train_x, train_y,\"C0o\", label=\"y data\")     ax.legend()  In\u00a0[5]: Copied! <pre>model.models[0].train_targets.shape\n</pre> model.models[0].train_targets.shape Out[5]: <pre>torch.Size([2])</pre> In\u00a0[6]: Copied! <pre>list(model.named_parameters())\n</pre> list(model.named_parameters()) Out[6]: <pre>[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-17.5714], dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(6.9444, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[-1.0121]], dtype=torch.float64, requires_grad=True))]</pre> In\u00a0[6]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/prior_mean_model/#custom-gp-modeling-for-bo","title":"Custom GP modeling for BO\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/prior_mean_model/#custom-prior-mean-function","title":"Custom prior mean function\u00b6","text":"<p>Here we assume we have some knowledge of the ground truth function, which we can take  advantage of to speed up optimization. This \"prior mean\" function is specified by a  pytorch module. Notice how away from the training data, the GP model reverts to the  prior mean.</p>"},{"location":"examples/single_objective_bayes_opt/time_dependent_bo/","title":"Time dependent upper confidence bound","text":"In\u00a0[1]: Copied! <pre>from xopt.generators.bayesian.upper_confidence_bound import TDUpperConfidenceBoundGenerator\nfrom xopt.vocs import VOCS\nfrom xopt.evaluator import Evaluator\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> from xopt.generators.bayesian.upper_confidence_bound import TDUpperConfidenceBoundGenerator from xopt.vocs import VOCS from xopt.evaluator import Evaluator import warnings warnings.filterwarnings(\"ignore\") In\u00a0[2]: Copied! <pre># test evaluate function and vocs\nimport time\nstart_time = time.time()\ndef f(inputs):\n    x_ = inputs[\"x\"]\n    current_time = time.time()\n    t_ = current_time - start_time\n    y_ = 5*(x_ - t_*1e-2)**2\n    return {\"y\":y_, \"time\":current_time}\n\nvariables = {\"x\":[-1,1]}\nobjectives = {\"y\": \"MINIMIZE\"}\n\nvocs = VOCS(variables=variables, objectives=objectives)\nprint(vocs)\n\nevaluator = Evaluator(function=f)\ngenerator = TDUpperConfidenceBoundGenerator(vocs)\ngenerator.options.n_initial = 3\ngenerator.options.acq.added_time=1.0\ngenerator.options.acq.beta = 2.0\ngenerator.options\n</pre> # test evaluate function and vocs import time start_time = time.time() def f(inputs):     x_ = inputs[\"x\"]     current_time = time.time()     t_ = current_time - start_time     y_ = 5*(x_ - t_*1e-2)**2     return {\"y\":y_, \"time\":current_time}  variables = {\"x\":[-1,1]} objectives = {\"y\": \"MINIMIZE\"}  vocs = VOCS(variables=variables, objectives=objectives) print(vocs)  evaluator = Evaluator(function=f) generator = TDUpperConfidenceBoundGenerator(vocs) generator.options.n_initial = 3 generator.options.acq.added_time=1.0 generator.options.acq.beta = 2.0 generator.options  <pre>variables={'x': [-1.0, 1.0]} constraints={} objectives={'y': 'MINIMIZE'} constants={} linked_variables={}\n</pre> Out[2]: <pre>TDUCBOptions(optim=OptimOptions(num_restarts=20, raw_samples=20, sequential=True, max_travel_distances=None, use_turbo=False), acq=TDUpperConfidenceBoundOptions(proximal_lengthscales=None, use_transformed_proximal_weights=True, monte_carlo_samples=128, added_time=1.0, beta=2.0), model=TimeDependentModelOptions(name='time_dependent_standard', custom_constructor=None, use_low_noise_prior=True, covar_modules={}, mean_modules={}, added_time=1.0), n_initial=3, use_cuda=False)</pre> In\u00a0[3]: Copied! <pre>from xopt import Xopt\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n\nX.step()\n\nfor _ in range(20):\n    # note that in this example we can ignore warnings if computation time is greater\n    # than added time\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n        X.step()\n        time.sleep(0.1)\n\nprint(X.generator.generate(1))\n</pre> from xopt import Xopt X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)  X.step()  for _ in range(20):     # note that in this example we can ignore warnings if computation time is greater     # than added time     with warnings.catch_warnings():         warnings.filterwarnings(\"ignore\", category=RuntimeWarning)         X.step()         time.sleep(0.1)  print(X.generator.generate(1)) <pre>          x\n0  0.259671\n</pre> In\u00a0[4]: Copied! <pre>X.data\n</pre> X.data Out[4]: x y time xopt_runtime xopt_error 1 0.575946 1.596517 1.682961e+09 0.000008 False 2 0.367632 0.636372 1.682961e+09 0.000001 False 3 0.440919 0.924679 1.682961e+09 0.000001 False 4 -0.087014 0.058238 1.682961e+09 0.000009 False 5 -0.593533 1.956165 1.682961e+09 0.000009 False 6 0.412748 0.683580 1.682961e+09 0.000008 False 7 0.032268 0.002370 1.682961e+09 0.000009 False 8 0.186444 0.073647 1.682961e+09 0.000009 False 9 -0.011706 0.038688 1.682961e+09 0.000009 False 10 0.139873 0.013822 1.682961e+09 0.000008 False 11 0.033471 0.021040 1.682961e+09 0.000009 False 12 0.186201 0.029501 1.682961e+09 0.000009 False 13 0.101839 0.001729 1.682961e+09 0.000032 False 14 0.149599 0.001517 1.682961e+09 0.000007 False 15 0.083350 0.017923 1.682961e+09 0.000009 False 16 0.186313 0.005135 1.682961e+09 0.000008 False 17 0.147314 0.001843 1.682961e+09 0.000008 False 18 0.208941 0.004923 1.682961e+09 0.000009 False 19 0.167979 0.002127 1.682961e+09 0.000009 False 20 0.220039 0.002079 1.682961e+09 0.000010 False 21 0.189432 0.002260 1.682961e+09 0.000014 False 22 0.251489 0.004167 1.682961e+09 0.000008 False 23 0.214924 0.001755 1.682961e+09 0.000009 False In\u00a0[5]: Copied! <pre># plot model\nimport torch\nfrom matplotlib import pyplot as plt  # plot model predictions\ndata = X.data\n\nxbounds = generator.vocs.bounds\ntbounds = [data[\"time\"].min(), data[\"time\"].max()]\n\ndef gt(inpts):\n    return 5*(inpts[:,1] - (inpts[:,0] - start_time)*1e-2)**2\n\nmodel = X.generator.model\nn = 200\nt = torch.linspace(*tbounds, n, dtype=torch.double)\nx = torch.linspace(*xbounds.flatten(), n, dtype=torch.double)\ntt, xx = torch.meshgrid(t, x)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (tt, xx)]).double()\n\n#NOTE: the model inputs are such that t is the last dimension\ngp_pts = torch.flip(pts, dims=[-1])\n\ngt_vals = gt(pts)\n\nwith torch.no_grad():\n    post = model.posterior(gp_pts)\n\n    mean = post.mean\n    std = torch.sqrt(post.variance)\n\n    fig, ax = plt.subplots()\n    ax.set_title(\"model mean\")\n    ax.set_xlabel(\"unix time\")\n    ax.set_ylabel(\"x\")\n    c = ax.pcolor(tt, xx, mean.reshape(n,n))\n    fig.colorbar(c)\n\n    fig2, ax2 = plt.subplots()\n    ax2.set_title(\"model uncertainty\")\n    ax2.set_xlabel(\"unix time\")\n    ax2.set_ylabel(\"x\")\n    c = ax2.pcolor(tt, xx, std.reshape(n,n))\n    fig2.colorbar(c)\n\n    ax.plot(data[\"time\"].to_numpy(), data[\"x\"].to_numpy(),\"oC1\")\n    ax2.plot(data[\"time\"].to_numpy(), data[\"x\"].to_numpy(),\"oC1\")\n\n    fig3, ax3 = plt.subplots()\n    ax3.set_title(\"ground truth value\")\n    ax3.set_xlabel(\"unix time\")\n    ax3.set_ylabel(\"x\")\n    c = ax3.pcolor(tt, xx, gt_vals.reshape(n,n))\n    fig3.colorbar(c)\n</pre> # plot model import torch from matplotlib import pyplot as plt  # plot model predictions data = X.data  xbounds = generator.vocs.bounds tbounds = [data[\"time\"].min(), data[\"time\"].max()]  def gt(inpts):     return 5*(inpts[:,1] - (inpts[:,0] - start_time)*1e-2)**2  model = X.generator.model n = 200 t = torch.linspace(*tbounds, n, dtype=torch.double) x = torch.linspace(*xbounds.flatten(), n, dtype=torch.double) tt, xx = torch.meshgrid(t, x) pts = torch.hstack([ele.reshape(-1, 1) for ele in (tt, xx)]).double()  #NOTE: the model inputs are such that t is the last dimension gp_pts = torch.flip(pts, dims=[-1])  gt_vals = gt(pts)  with torch.no_grad():     post = model.posterior(gp_pts)      mean = post.mean     std = torch.sqrt(post.variance)      fig, ax = plt.subplots()     ax.set_title(\"model mean\")     ax.set_xlabel(\"unix time\")     ax.set_ylabel(\"x\")     c = ax.pcolor(tt, xx, mean.reshape(n,n))     fig.colorbar(c)      fig2, ax2 = plt.subplots()     ax2.set_title(\"model uncertainty\")     ax2.set_xlabel(\"unix time\")     ax2.set_ylabel(\"x\")     c = ax2.pcolor(tt, xx, std.reshape(n,n))     fig2.colorbar(c)      ax.plot(data[\"time\"].to_numpy(), data[\"x\"].to_numpy(),\"oC1\")     ax2.plot(data[\"time\"].to_numpy(), data[\"x\"].to_numpy(),\"oC1\")      fig3, ax3 = plt.subplots()     ax3.set_title(\"ground truth value\")     ax3.set_xlabel(\"unix time\")     ax3.set_ylabel(\"x\")     c = ax3.pcolor(tt, xx, gt_vals.reshape(n,n))     fig3.colorbar(c) In\u00a0[6]: Copied! <pre>list(model.named_parameters())\n</pre> list(model.named_parameters()) Out[6]: <pre>[('models.0.likelihood.noise_covar.raw_noise',\n  Parameter containing:\n  tensor([-21.5955], dtype=torch.float64, requires_grad=True)),\n ('models.0.mean_module.raw_constant',\n  Parameter containing:\n  tensor(3.2435, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.raw_outputscale',\n  Parameter containing:\n  tensor(4.2421, dtype=torch.float64, requires_grad=True)),\n ('models.0.covar_module.base_kernel.raw_lengthscale',\n  Parameter containing:\n  tensor([[-0.7693,  1.7992]], dtype=torch.float64, requires_grad=True))]</pre> In\u00a0[7]: Copied! <pre># plot the acquisition function\n# note that target time is only updated during the generate call\ntarget_time = generator.target_prediction_time\nprint(target_time-start_time)\nmy_acq_func = generator.get_acquisition(model)\n\nwith torch.no_grad():\n    acq_pts = x.unsqueeze(-1).unsqueeze(-1)\n    full_acq = my_acq_func.acq_func(gp_pts.unsqueeze(1))\n    fixed_acq = my_acq_func(acq_pts)\n\n    fig, ax = plt.subplots()\n    c = ax.pcolor(tt, xx, full_acq.reshape(n,n))\n    fig.colorbar(c)\n\n    fi2, ax2 = plt.subplots()\n    ax2.plot(x.flatten(), fixed_acq.flatten())\n</pre> # plot the acquisition function # note that target time is only updated during the generate call target_time = generator.target_prediction_time print(target_time-start_time) my_acq_func = generator.get_acquisition(model)  with torch.no_grad():     acq_pts = x.unsqueeze(-1).unsqueeze(-1)     full_acq = my_acq_func.acq_func(gp_pts.unsqueeze(1))     fixed_acq = my_acq_func(acq_pts)      fig, ax = plt.subplots()     c = ax.pcolor(tt, xx, full_acq.reshape(n,n))     fig.colorbar(c)      fi2, ax2 = plt.subplots()     ax2.plot(x.flatten(), fixed_acq.flatten()) <pre>24.468086004257202\n</pre> In\u00a0[7]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/time_dependent_bo/#time-dependent-bayesian-optimization","title":"Time dependent Bayesian Optimization\u00b6","text":"<p>In this example we demonstrate time dependent optimization. In this case we are not only interested in finding an optimum point in input space, but also maintain the ideal point over time.</p>"},{"location":"examples/single_objective_bayes_opt/time_dependent_bo/#time-dependent-test-problem","title":"Time dependent test problem\u00b6","text":"<p>Optimization is carried out over a single variable <code>x</code>. The test function is a simple  quadratic, with a minimum location that drifts in the positive <code>x</code> direction over  (real) time.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/","title":"TuRBO Bayesian Optimization","text":"In\u00a0[1]: Copied! <pre>from xopt.vocs import VOCS\nimport math\n\n# define variables and function objectives\nvocs = VOCS(\n    variables={\"x\": [0, 2 * math.pi]},\n    objectives={\"f\": \"MINIMIZE\"},\n)\n</pre> from xopt.vocs import VOCS import math  # define variables and function objectives vocs = VOCS(     variables={\"x\": [0, 2 * math.pi]},     objectives={\"f\": \"MINIMIZE\"}, ) In\u00a0[2]: Copied! <pre># define a test function to optimize\nimport numpy as np\n\ndef sin_function(input_dict):\n    x = input_dict[\"x\"]\n    return {\"f\": -10*np.exp(-(x - np.pi)**2 / 0.01) + 0.5*np.sin(5*x)}\n</pre> # define a test function to optimize import numpy as np  def sin_function(input_dict):     x = input_dict[\"x\"]     return {\"f\": -10*np.exp(-(x - np.pi)**2 / 0.01) + 0.5*np.sin(5*x)} In\u00a0[3]: Copied! <pre>from xopt.evaluator import Evaluator\nfrom xopt.generators.bayesian import UpperConfidenceBoundGenerator\nfrom xopt import Xopt\n\nevaluator = Evaluator(function=sin_function)\noptions = UpperConfidenceBoundGenerator.default_options()\noptions.optim.use_turbo = True\ngenerator = UpperConfidenceBoundGenerator(vocs, options)\nX = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)\n</pre> from xopt.evaluator import Evaluator from xopt.generators.bayesian import UpperConfidenceBoundGenerator from xopt import Xopt  evaluator = Evaluator(function=sin_function) options = UpperConfidenceBoundGenerator.default_options() options.optim.use_turbo = True generator = UpperConfidenceBoundGenerator(vocs, options) X = Xopt(evaluator=evaluator, generator=generator, vocs=vocs) In\u00a0[4]: Copied! <pre>options\n</pre> options Out[4]: <pre>UCBOptions(optim=OptimOptions(num_restarts=20, raw_samples=20, sequential=True, max_travel_distances=None, use_turbo=True), acq=UpperConfidenceBoundOptions(proximal_lengthscales=None, use_transformed_proximal_weights=True, monte_carlo_samples=128, beta=2.0), model=ModelOptions(name='standard', custom_constructor=None, use_low_noise_prior=True, covar_modules={}, mean_modules={}), n_initial=3, use_cuda=False)</pre> In\u00a0[5]: Copied! <pre>import pandas as pd\nX.evaluate_data(pd.DataFrame({\"x\":[3.0, 1.75, 2.0]}))\n\n# inspect the gathered data\nX.data\n</pre> import pandas as pd X.evaluate_data(pd.DataFrame({\"x\":[3.0, 1.75, 2.0]}))  # inspect the gathered data X.data Out[5]: x f xopt_runtime xopt_error 1 3.00 -1.021664 0.000037 False 2 1.75 0.312362 0.000005 False 3 2.00 -0.272011 0.000004 False In\u00a0[6]: Copied! <pre># determine trust region from gathered data\nimport torch\ngenerator.train_model()\ngenerator.get_trust_region(torch.tensor(vocs.bounds))\n</pre> # determine trust region from gathered data import torch generator.train_model() generator.get_trust_region(torch.tensor(vocs.bounds)) Out[6]: <pre>tensor([[1.4292],\n        [4.5708]], dtype=torch.float64)</pre> In\u00a0[7]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n\n# test points for plotting\ntest_x = torch.linspace(*X.vocs.bounds.flatten(),500).double()\n\nfor i in range(15):\n    # get the Gaussian process model from the generator\n    model = X.generator.train_model()\n\n    # get trust region\n    trust_region = generator.get_trust_region(torch.tensor(vocs.bounds)).squeeze()\n    scale_factor = generator.turbo_state.length\n    region_width = trust_region[1] - trust_region[0]\n    best_value = generator.turbo_state.best_value\n\n    # get number of successes and failures\n    n_successes = generator.turbo_state.success_counter\n    n_failures = generator.turbo_state.failure_counter\n\n    # get acquisition function from generator\n    acq = X.generator.get_acquisition(model)\n\n    # calculate model posterior and acquisition function at each test point\n    # NOTE: need to add a dimension to the input tensor for evaluating the\n    # posterior and another for the acquisition function, see\n    # https://botorch.org/docs/batching for details\n    # NOTE: we use the `torch.no_grad()` environment to speed up computation by\n    # skipping calculations for backpropagation\n    with torch.no_grad():\n        posterior = model.posterior(test_x.unsqueeze(1))\n        acq_val = acq(test_x.reshape(-1,1,1))\n\n    # get mean function and confidence regions\n    mean = posterior.mean\n    l,u = posterior.mvn.confidence_region()\n\n    # plot model and acquisition function\n    fig,ax = plt.subplots(2,1,sharex=\"all\")\n\n    # add title for successes and failures\n    ax[0].set_title(f\"n_successes: {n_successes}, n_failures: {n_failures}, \"\n                    f\"scale_factor: {scale_factor}, region_width: {region_width:.2}, \"\n                    f\"best_value: {best_value:.4}\")\n\n    # plot model posterior\n    ax[0].plot(test_x, mean, label=\"Posterior mean\")\n    ax[0].fill_between(test_x, l, u,alpha=0.25, label=\"Posterior confidence region\")\n\n    # add data to model plot\n    ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")\n\n    # plot true function\n    true_f = sin_function({\"x\": test_x})[\"f\"]\n    ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")\n\n    # add legend\n    ax[0].legend()\n\n    # plot acquisition function\n    ax[1].plot(test_x, acq_val.flatten())\n\n    ax[0].set_ylabel(\"f\")\n    ax[1].set_ylabel(r\"$\\alpha(x)$\")\n    ax[1].set_xlabel(\"x\")\n\n    # plot trust region\n    for a in ax:\n        a.axvline(trust_region[0],c=\"r\")\n        a.axvline(trust_region[1],c=\"r\")\n\n    # do the optimization step\n    X.step()\n</pre> import torch import matplotlib.pyplot as plt  # test points for plotting test_x = torch.linspace(*X.vocs.bounds.flatten(),500).double()  for i in range(15):     # get the Gaussian process model from the generator     model = X.generator.train_model()      # get trust region     trust_region = generator.get_trust_region(torch.tensor(vocs.bounds)).squeeze()     scale_factor = generator.turbo_state.length     region_width = trust_region[1] - trust_region[0]     best_value = generator.turbo_state.best_value      # get number of successes and failures     n_successes = generator.turbo_state.success_counter     n_failures = generator.turbo_state.failure_counter      # get acquisition function from generator     acq = X.generator.get_acquisition(model)      # calculate model posterior and acquisition function at each test point     # NOTE: need to add a dimension to the input tensor for evaluating the     # posterior and another for the acquisition function, see     # https://botorch.org/docs/batching for details     # NOTE: we use the `torch.no_grad()` environment to speed up computation by     # skipping calculations for backpropagation     with torch.no_grad():         posterior = model.posterior(test_x.unsqueeze(1))         acq_val = acq(test_x.reshape(-1,1,1))      # get mean function and confidence regions     mean = posterior.mean     l,u = posterior.mvn.confidence_region()      # plot model and acquisition function     fig,ax = plt.subplots(2,1,sharex=\"all\")      # add title for successes and failures     ax[0].set_title(f\"n_successes: {n_successes}, n_failures: {n_failures}, \"                     f\"scale_factor: {scale_factor}, region_width: {region_width:.2}, \"                     f\"best_value: {best_value:.4}\")      # plot model posterior     ax[0].plot(test_x, mean, label=\"Posterior mean\")     ax[0].fill_between(test_x, l, u,alpha=0.25, label=\"Posterior confidence region\")      # add data to model plot     ax[0].plot(X.data[\"x\"],X.data[\"f\"],\"C1o\", label=\"Training data\")      # plot true function     true_f = sin_function({\"x\": test_x})[\"f\"]     ax[0].plot(test_x, true_f,'--', label=\"Ground truth\")      # add legend     ax[0].legend()      # plot acquisition function     ax[1].plot(test_x, acq_val.flatten())      ax[0].set_ylabel(\"f\")     ax[1].set_ylabel(r\"$\\alpha(x)$\")     ax[1].set_xlabel(\"x\")      # plot trust region     for a in ax:         a.axvline(trust_region[0],c=\"r\")         a.axvline(trust_region[1],c=\"r\")      # do the optimization step     X.step()  In\u00a0[8]: Copied! <pre># access the collected data\ngenerator.get_trust_region(torch.tensor(vocs.bounds))\n</pre> # access the collected data generator.get_trust_region(torch.tensor(vocs.bounds)) Out[8]: <pre>tensor([[2.3571],\n        [3.9279]], dtype=torch.float64)</pre> In\u00a0[9]: Copied! <pre>X.data\n</pre> X.data Out[9]: x f xopt_runtime xopt_error 1 3.000000 -1.021664 0.000037 False 2 1.750000 0.312362 0.000005 False 3 2.000000 -0.272011 0.000004 False 4 3.785398 0.038679 0.000022 False 5 2.635128 0.286134 0.000045 False 6 3.196350 -7.544618 0.000022 False 7 3.357064 -0.536663 0.000022 False 8 3.151319 -9.930157 0.000021 False 9 3.544018 -0.452093 0.000023 False 10 3.120782 -9.524227 0.000023 False 11 3.140644 -9.996729 0.000023 False 12 3.142025 -10.000894 0.000023 False 13 3.142266 -10.001231 0.000023 False 14 2.356868 -0.352360 0.000024 False 15 3.142356 -10.001326 0.000022 False 16 3.142448 -10.001407 0.000022 False 17 4.713245 -0.499995 0.000022 False 18 3.142484 -10.001434 0.000023 False In\u00a0[9]: Copied! <pre>\n</pre>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#turbo-bayesian-optimization","title":"TuRBO Bayesian Optimization\u00b6","text":"<p>In this tutorial we demonstrate the use of Xopt to preform Trust Region Bayesian Optimization (TuRBO) on a simple test problem. During optimization of high dimensional input spaces off the shelf BO tends to over-emphasize exploration which severely degrades optimization performance. TuRBO attempts to prevent this by maintaining a surrogate model over a local (trust) region centered on the best observation so far and restricting optimization inside that local region. The trust region is expanded and contracted based on the number of <code>successful</code> (observations that improve over the best observed point) or <code>unsuccessful</code> (no improvement) observations in a row. See https://botorch.org/tutorials/turbo_1 for details.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#define-the-test-problem","title":"Define the test problem\u00b6","text":"<p>Here we define a simple optimization problem, where we attempt to minimize a function in the domian [0,2*pi]. Note that the function used to evaluate the objective function takes a dictionary as input and returns a dictionary as the output.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#create-xopt-objects","title":"Create Xopt objects\u00b6","text":"<p>Create the evaluator to evaluate our test function and create a generator that uses the Upper Confidence Bound acqusition function to perform Bayesian Optimization.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#generate-and-evaluate-initial-points","title":"Generate and evaluate initial points\u00b6","text":"<p>To begin optimization, we must generate some random initial data points. The first call to <code>X.step()</code> will generate and evaluate a number of randomly points specified by the  generator. Note that if we add data to xopt before calling <code>X.step()</code> by assigning  the data to <code>X.data</code>, calls to <code>X.step()</code> will ignore the random generation and  proceed to generating points via Bayesian optimization.</p>"},{"location":"examples/single_objective_bayes_opt/turbo_tutorial/#do-bayesian-optimization-steps","title":"Do bayesian optimization steps\u00b6","text":"<p>Notice that when the number of successive successes or failures reaches 2 the trust region expands or contracts and counters are reset to zero. Counters are also reset to zero during alternate successes/failures. Finally, the model is most accurate inside the trust region, which supports our goal of local optimization.</p>"},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/","title":"Upper Confidence Bound BO","text":"In\u00a0[1]: Copied! <pre># Import the class\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport yaml\n\nimport matplotlib.pyplot as plt\nfrom xopt import Xopt\n</pre> # Import the class  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import torch import yaml  import matplotlib.pyplot as plt from xopt import Xopt In\u00a0[2]: Copied! <pre>from xopt.generators import get_generator_and_defaults\ngen, options = get_generator_and_defaults(\"upper_confidence_bound\")\nprint(yaml.dump(options.dict()))\n</pre> from xopt.generators import get_generator_and_defaults gen, options = get_generator_and_defaults(\"upper_confidence_bound\") print(yaml.dump(options.dict())) <pre>acq:\n  beta: 2.0\n  monte_carlo_samples: 128\n  proximal_lengthscales: null\n  use_transformed_proximal_weights: true\nmodel:\n  covar_modules: {}\n  custom_constructor: null\n  mean_modules: {}\n  name: standard\n  use_low_noise_prior: true\nn_initial: 3\noptim:\n  max_travel_distances: null\n  num_restarts: 20\n  raw_samples: 20\n  sequential: true\n  use_turbo: false\nuse_cuda: false\n\n</pre> <p>The <code>Xopt</code> object can be instantiated from a JSON or YAML file, or a dict, with the proper structure.</p> <p>Here we will make one</p> In\u00a0[3]: Copied! <pre># Make a proper input file.\nYAML = \"\"\"\nxopt: \n    dump_file: dump.yaml\ngenerator:\n  name: upper_confidence_bound\n  n_initial: 2\n  acq:\n    beta: 0.1\n\nevaluator:\n  function: xopt.resources.test_functions.sinusoid_1d.evaluate_sinusoid\n\nvocs:\n  variables:\n    x1: [0, 6.28]\n  objectives:\n    y1: 'MINIMIZE'\n\"\"\"\nconfig = yaml.safe_load(YAML)\n</pre> # Make a proper input file. YAML = \"\"\" xopt:      dump_file: dump.yaml generator:   name: upper_confidence_bound   n_initial: 2   acq:     beta: 0.1  evaluator:   function: xopt.resources.test_functions.sinusoid_1d.evaluate_sinusoid  vocs:   variables:     x1: [0, 6.28]   objectives:     y1: 'MINIMIZE' \"\"\" config = yaml.safe_load(YAML) In\u00a0[4]: Copied! <pre>X = Xopt(config=config)\nX\n</pre> X = Xopt(config=config) X Out[4]: <pre>\n            Xopt\n________________________________\nVersion: 0+untagged.1.g0676a82\nData size: 0\nConfig as YAML:\nxopt: {asynch: false, strict: false, dump_file: dump.yaml, max_evaluations: null}\ngenerator:\n  name: upper_confidence_bound\n  optim: {num_restarts: 20, raw_samples: 20, sequential: true, max_travel_distances: null,\n    use_turbo: false}\n  acq: {proximal_lengthscales: null, use_transformed_proximal_weights: true, monte_carlo_samples: 128,\n    beta: 0.1}\n  model:\n    name: standard\n    custom_constructor: null\n    use_low_noise_prior: true\n    covar_modules: {}\n    mean_modules: {}\n  n_initial: 2\n  use_cuda: false\nevaluator:\n  function: xopt.resources.test_functions.sinusoid_1d.evaluate_sinusoid\n  max_workers: 1\n  function_kwargs: {}\n  vectorized: false\nvocs:\n  variables:\n    x1: [0.0, 6.28]\n  constraints: {}\n  objectives: {y1: MINIMIZE}\n  constants: {}\n  linked_variables: {}\n</pre> In\u00a0[5]: Copied! <pre>for i in range(5):\n    print(i)\n    X.step()\n</pre> for i in range(5):     print(i)     X.step() <pre>0\n1\n2\n3\n4\n</pre> In\u00a0[6]: Copied! <pre>X.data\n</pre> X.data Out[6]: x1 y1 c1 xopt_runtime xopt_error 1 3.046264 0.095184 -7.929289 0.000031 False 2 0.693098 0.638923 -4.101079 0.000005 False 3 4.196937 -0.870070 -19.093813 0.000020 False 4 5.489486 -0.712952 -15.964672 0.000018 False 5 4.664601 -0.998858 -18.544015 0.000019 False 6 4.671237 -0.999153 -18.532739 0.000018 False In\u00a0[7]: Copied! <pre>model = X.generator.model\nbounds = X.vocs.bounds\n\ntest_x = torch.linspace(*bounds.flatten(), 100).double()\n\ntrain_x = torch.tensor(X.data[\"x1\"].to_numpy())\ntrain_y = torch.tensor(X.data[\"y1\"].to_numpy())\n\nfig, ax = plt.subplots()\nwith torch.no_grad():\n    post = model.models[0].posterior(test_x.reshape(-1,1,1))\n    mean = post.mean.flatten()\n    std = post.variance.sqrt().flatten()\n\n    lower = mean - std\n    upper = mean + std\n\nax.plot(test_x, mean)\nax.fill_between(test_x, lower, upper, alpha=0.5)\nax.plot(\n    train_x.flatten(),\n    train_y.flatten(),\n    \"+\"\n)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"y1\")\n</pre> model = X.generator.model bounds = X.vocs.bounds  test_x = torch.linspace(*bounds.flatten(), 100).double()  train_x = torch.tensor(X.data[\"x1\"].to_numpy()) train_y = torch.tensor(X.data[\"y1\"].to_numpy())  fig, ax = plt.subplots() with torch.no_grad():     post = model.models[0].posterior(test_x.reshape(-1,1,1))     mean = post.mean.flatten()     std = post.variance.sqrt().flatten()      lower = mean - std     upper = mean + std  ax.plot(test_x, mean) ax.fill_between(test_x, lower, upper, alpha=0.5) ax.plot(     train_x.flatten(),     train_y.flatten(),     \"+\" ) ax.set_xlabel(\"x1\") ax.set_ylabel(\"y1\")  Out[7]: <pre>Text(0, 0.5, 'y1')</pre> In\u00a0[8]: Copied! <pre># Cleanup\n!rm dump.yaml\n</pre> # Cleanup !rm dump.yaml"},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#upper-confidence-bound-bo","title":"Upper Confidence Bound BO\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#customizing-the-upperconfidencebound-generator","title":"Customizing the UpperConfidenceBound Generator\u00b6","text":"<p>First lets examine the possible options that we can specify for the UpperConfidenceBound generator. We can use these keys to customize optimization.</p>"},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#run-optimization","title":"Run Optimization\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#view-output-data","title":"View output data\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound/#visualize-model-used-by-upper-confidence-bound","title":"Visualize model used by upper confidence bound\u00b6","text":"<p>Models are kept in a list, in this case that list has one element, the model created for the objective <code>y1</code>.</p>"},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound_rosenbrock/","title":"Upper Confidence Bound BO","text":"In\u00a0[1]: Copied! <pre># Import the class\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport yaml\n</pre> # Import the class  # Ignore all warnings import warnings warnings.filterwarnings(\"ignore\")  import torch import yaml In\u00a0[2]: Copied! <pre>from xopt.generators import get_generator_and_defaults\nucb_gen, ucb_options = get_generator_and_defaults(\"upper_confidence_bound\")\nprint(yaml.dump(ucb_options.dict()))\n</pre> from xopt.generators import get_generator_and_defaults ucb_gen, ucb_options = get_generator_and_defaults(\"upper_confidence_bound\") print(yaml.dump(ucb_options.dict())) <pre>acq:\n  beta: 2.0\n  monte_carlo_samples: 128\n  proximal_lengthscales: null\n  use_transformed_proximal_weights: true\nmodel:\n  covar_modules: {}\n  custom_constructor: null\n  mean_modules: {}\n  name: standard\n  use_low_noise_prior: true\nn_initial: 3\noptim:\n  max_travel_distances: null\n  num_restarts: 20\n  raw_samples: 20\n  sequential: true\n  use_turbo: false\nuse_cuda: false\n\n</pre> In\u00a0[3]: Copied! <pre>from xopt.resources.test_functions.rosenbrock import make_rosenbrock_vocs, evaluate_rosenbrock\nfrom xopt import Xopt, Evaluator\nimport pandas as pd\n\nvocs = make_rosenbrock_vocs(2)\n\ngenerator_options = ucb_gen.default_options()\ngenerator_options.optim.num_restarts = 20\ngenerator_options.optim.max_travel_distances = [0.1, 0.1]\ngenerator_options.acq.beta = 2.0\n\nevaluator = Evaluator(function=evaluate_rosenbrock)\ngenerator = ucb_gen(vocs, generator_options)\n</pre> from xopt.resources.test_functions.rosenbrock import make_rosenbrock_vocs, evaluate_rosenbrock from xopt import Xopt, Evaluator import pandas as pd  vocs = make_rosenbrock_vocs(2)  generator_options = ucb_gen.default_options() generator_options.optim.num_restarts = 20 generator_options.optim.max_travel_distances = [0.1, 0.1] generator_options.acq.beta = 2.0  evaluator = Evaluator(function=evaluate_rosenbrock) generator = ucb_gen(vocs, generator_options)  In\u00a0[4]: Copied! <pre>X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)\n\nfor i in range(10):\n    X.step()\n    print(f\"step {i}: best: {X.data['y'].min()}\")\n</pre> X = Xopt(generator=generator, evaluator=evaluator, vocs=vocs)  for i in range(10):     X.step()     print(f\"step {i}: best: {X.data['y'].min()}\")  <pre>step 0: best: 401.5442133928811\nstep 1: best: 401.5442133928811\nstep 2: best: 295.26888725170113\nstep 3: best: 187.6408271500635\nstep 4: best: 187.6408271500635\nstep 5: best: 187.6408271500635\nstep 6: best: 187.6408271500635\nstep 7: best: 187.6408271500635\nstep 8: best: 187.6408271500635\nstep 9: best: 187.6408271500635\n</pre> In\u00a0[5]: Copied! <pre>X.data\n</pre> X.data Out[5]: x0 x1 y xopt_runtime xopt_error 1 -1.862457 0.949574 642.817093 0.000014 False 2 1.916779 0.259886 1166.485071 0.000005 False 3 0.329123 -1.894412 401.544213 0.000004 False 4 0.251674 -1.972939 415.203233 0.000033 False 5 0.229185 -1.664083 295.268887 0.000017 False 6 -0.131578 -1.347826 187.640827 0.000018 False 7 -0.091121 -1.573830 251.505170 0.000017 False 8 0.054362 -1.847217 343.208096 0.000018 False 9 -0.329108 -1.674785 319.709900 0.000016 False 10 0.011739 -1.591166 254.201551 0.000017 False 11 0.219718 -1.410133 213.304540 0.000017 False 12 0.036848 -1.411283 200.483014 0.000017 False In\u00a0[6]: Copied! <pre># plot results\nax = X.data.plot(*vocs.variable_names)\nax.set_aspect(\"equal\")\n</pre> # plot results ax = X.data.plot(*vocs.variable_names) ax.set_aspect(\"equal\") In\u00a0[7]: Copied! <pre>from matplotlib import pyplot as plt  # plot model predictions\n\ndata = X.data\n\nbounds = generator.vocs.bounds\nmodel = generator.train_model(generator.data)\n\n# create mesh\nn = 200\nx = torch.linspace(*bounds.T[0], n)\ny = torch.linspace(*bounds.T[1], n)\nxx, yy = torch.meshgrid(x, y)\npts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()\n\noutputs = generator.vocs.output_names\nwith torch.no_grad():\n    post = model.posterior(pts)\n\n    mean = post.mean\n    std = torch.sqrt(post.variance)\n\n    fig, ax = plt.subplots()\n    ax.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")\n    c = ax.pcolor(xx, yy, mean.reshape(n, n))\n    fig.colorbar(c)\n    ax.set_title(f\"Posterior mean: {outputs[0]}\")\n\n    fig2, ax2 = plt.subplots()\n    ax2.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")\n    c = ax2.pcolor(xx, yy, std.reshape(n, n))\n    fig2.colorbar(c)\n    ax2.set_title(f\"Posterior std: {outputs[0]}\")\n</pre> from matplotlib import pyplot as plt  # plot model predictions  data = X.data  bounds = generator.vocs.bounds model = generator.train_model(generator.data)  # create mesh n = 200 x = torch.linspace(*bounds.T[0], n) y = torch.linspace(*bounds.T[1], n) xx, yy = torch.meshgrid(x, y) pts = torch.hstack([ele.reshape(-1, 1) for ele in (xx, yy)]).double()  outputs = generator.vocs.output_names with torch.no_grad():     post = model.posterior(pts)      mean = post.mean     std = torch.sqrt(post.variance)      fig, ax = plt.subplots()     ax.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")     c = ax.pcolor(xx, yy, mean.reshape(n, n))     fig.colorbar(c)     ax.set_title(f\"Posterior mean: {outputs[0]}\")      fig2, ax2 = plt.subplots()     ax2.plot(*data[vocs.variable_names].to_numpy().T, \"+C1\")     c = ax2.pcolor(xx, yy, std.reshape(n, n))     fig2.colorbar(c)     ax2.set_title(f\"Posterior std: {outputs[0]}\") In\u00a0[8]: Copied! <pre>ax = X.data.plot(y=\"y\", logy=True)\n</pre> ax = X.data.plot(y=\"y\", logy=True) In\u00a0[9]: Copied! <pre># Cleanup\n!rm dump.yaml\n</pre> # Cleanup !rm dump.yaml <pre>rm: cannot remove 'dump.yaml': No such file or directory\r\n</pre>"},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound_rosenbrock/#upper-confidence-bound-bo","title":"Upper Confidence Bound BO\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound_rosenbrock/#customizing-the-upperconfidencebound-generator","title":"Customizing the UpperConfidenceBound Generator\u00b6","text":"<p>First lets examine the possible options that we can specify for the UpperConfidenceBound generator. We can use these keys to customize optimization.</p>"},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound_rosenbrock/#view-output-data","title":"View output data\u00b6","text":""},{"location":"examples/single_objective_bayes_opt/upper_confidence_bound_rosenbrock/#visualize-model-used-by-upper-confidence-bound","title":"Visualize model used by upper confidence bound\u00b6","text":"<p>Models are kept in a list, in this case that list has one element, the model created for the objective <code>y1</code>.</p>"}]}